{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xv8CD4K_qUX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOd8RhyuZzp2"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/\"My Drive\"/kaggle/lish-moa.zip /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YSua2cgfd2hn",
    "outputId": "955555c0-0b36-4348-f88e-34f65d0b7e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  lish-moa.zip\n",
      "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
     ]
    }
   ],
   "source": [
    "!unzip lish-moa.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "CB9siuMCJ5pd",
    "outputId": "69836728-3ef9-4a5d-8491-1e8a77708106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
      "Requirement already satisfied: install in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "dhdrqaSOrppk",
    "outputId": "e3cc19dc-b4fe-42fc-c272-2755ba7917d4"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "zUpXeUlWeXpI",
    "outputId": "69784acf-dc77-402e-8d64-ef317462c8e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import optuna\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d   ## smoother\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 15, 7\n",
    "\n",
    "CGREEN  = '\\33[32m'\n",
    "CBLUE =  '\\033[34m'\n",
    "CRED = '\\033[1;31m'\n",
    "CEND  = '\\33[0m'\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "p0UcuWgBr0Hn",
    "outputId": "c6826f75-df86-441d-fd27-4c62c8f04df9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'\n",
    "    \n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZhAwZ3hr45u"
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_targets = pd.read_csv('train_targets_scored.csv')\n",
    "train_targets_s = train_targets\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "ss = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWLxDlvyucnm"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    return df\n",
    "\n",
    "train = preprocess(train_features)\n",
    "test = preprocess(test_features)\n",
    "\n",
    "del train_targets['sig_id']\n",
    "\n",
    "target = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n",
    "train = train.loc[train['cp_type']==0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QNGkeMpMufs2",
    "outputId": "0e3ca080-57b7-4522-8cd1-9d9db388c80c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 785), (3982, 785))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
    "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
    "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
    "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
    "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
    "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
    "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
    "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
    "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
    "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
    "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
    "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
    "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
    "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
    "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
    "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
    "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
    "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
    "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
    "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
    "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
    "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
    "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
    "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
    "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
    "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
    "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
    "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
    "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
    "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
    "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
    "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
    "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
    "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
    "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
    "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
    "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
    "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
    "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
    "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
    "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
    "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
    "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
    "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
    "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
    "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
    "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
    "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
    "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
    "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
    "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
    "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
    "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
    "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
    "       870, 871, 872, 873, 874]\n",
    "\n",
    "all_columns = train.columns\n",
    "train=train[all_columns[top_features]]\n",
    "test = test[all_columns[top_features]]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_rTT1uYunp_"
   },
   "outputs": [],
   "source": [
    "train = train.values\n",
    "target = target.values\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n3YuEdDt1HFd",
    "outputId": "b6786f74-bc14-4b48-8a0f-36712e4a3972"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 785), (21948, 206))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9N0QYVKDsTm-"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train,targets, noise ):\n",
    "        \n",
    "        self.features  = train\n",
    "        self.targets = targets\n",
    "        self.noise = noise\n",
    "        \n",
    "    def sizes(self):\n",
    "        print(\"features size = \", self.features.shape[1])\n",
    "        print(\"targets size = \", self.targets.shape[1])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx]).float()\n",
    "        target = torch.tensor(self.targets[idx]).float()\n",
    "        return feature, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCn4Ba6EsVyt"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "        \n",
    "def show_lr(learning_rates):\n",
    "    plt.plot(learning_rates, label = \"learning rate\")\n",
    "    plt.ylabel(\"Learning rate\", fontsize = 15)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_step(x, y, model, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x.to(device))\n",
    "    y = y.float()\n",
    "    loss = criterion(pred,y.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZqPFz-CsX4R"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "    super().__init__()\n",
    "    layers = []\n",
    "    for _ in range(nlayers):\n",
    "      if len(layers) == 0:\n",
    "        layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "        layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "      else:\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "\n",
    "    layers.append(nn.Linear(hidden_size, ntargets))\n",
    "\n",
    "    self.model = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwCVq5qyDXao"
   },
   "outputs": [],
   "source": [
    "def run_training( params, save_model=False):\n",
    "  NFOLDS = 1\n",
    "  EPOCHS = 20 ## changes here \n",
    "\n",
    "  fold_val_losses = list()\n",
    "      \n",
    "  full_dataset = TrainDataset(train, target, noise = False)\n",
    "    \n",
    "  train_size = int(0.9 * len(full_dataset))  ## 90/10 split\n",
    "  test_size = len(full_dataset) - train_size\n",
    "  train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "  train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers = 8)\n",
    "  val_loader = DataLoader(dataset=val_dataset, batch_size=256, shuffle = True, num_workers = 8)\n",
    "  \n",
    "\n",
    "  model = Model(nfeatures=train.shape[1], \n",
    "                ntargets= target.shape[1],\n",
    "                nlayers=params[\"num_layers\"], \n",
    "                hidden_size=params[\"hidden_size\"], \n",
    "                dropout=params[\"dropout\"])\n",
    "  \n",
    "  model = model.cuda()\n",
    "  optimizer = optim.Adam(model.parameters(), lr = params[\"learning_rate\"], weight_decay=1e-5)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                  mode='min', \n",
    "                                                  factor=0.5, \n",
    "                                                  patience=3, \n",
    "                                                  eps=1e-4, \n",
    "                                                  verbose=True)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  eng = Engine(model, optimizer, device='cuda')\n",
    "  best_loss = 10000\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    train_loss = eng.train(train_loader)\n",
    "    valid_loss = eng.evaluate(val_loader)\n",
    "    print(\"train_loss:\", train_loss, \"val_loss:\", valid_loss)\n",
    "    if valid_loss<best_loss:\n",
    "      best_loss = valid_loss\n",
    "      if save_model:\n",
    "        torch.save(model.state_dict(), \"model_{fold}.pth\")\n",
    "    \n",
    "  return(best_loss)\n",
    "\n",
    "  print(CBLUE, \"Training complete\", CEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW4k5C9lEovt"
   },
   "outputs": [],
   "source": [
    "class Engine:\n",
    "  def __init__(self, model, optimizer, device):\n",
    "    self.model = model\n",
    "    self.device = device\n",
    "    self.optimizer = optimizer\n",
    "\n",
    "  @staticmethod\n",
    "  def loss_fn(targets, outputs):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "  def train(self, data_loader):\n",
    "    self.model.train()\n",
    "    final_loss=0\n",
    "    for data in data_loader:\n",
    "      self.optimizer.zero_grad()\n",
    "      inputs, targets = data\n",
    "      inputs, targets = inputs.to(device), targets.to(device)\n",
    "      outputs = self.model(inputs)\n",
    "      loss = self.loss_fn(targets, outputs)\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      final_loss += loss.item()\n",
    "    return(final_loss / len(data_loader))\n",
    "\n",
    "\n",
    "  def evaluate(self, data_loader):\n",
    "    self.model.train()\n",
    "    final_loss=0\n",
    "    for data in data_loader:\n",
    "      #self.optimizer.zero_grad()\n",
    "      inputs, targets = data\n",
    "      inputs, targets = inputs.to(device), targets.to(device)\n",
    "      outputs = self.model(inputs)\n",
    "      loss = self.loss_fn(targets, outputs)\n",
    "      #loss.backward()\n",
    "      #self.optimizer.step()\n",
    "      final_loss += loss.item()\n",
    "    return(final_loss / len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umofVWQbII_u"
   },
   "outputs": [],
   "source": [
    "# params = {\n",
    "#       \"num_layers\":trial.suggest_int(\"num_layer\", 1, 8),\n",
    "#       \"hidden_size\":trial.suggest_int(\"hidden_size\", 16, 4096),\n",
    "#       \"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.7),\n",
    "#       \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-2)\n",
    "#   }\n",
    "# run_training(params, save_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note :\n",
    "    Replace trial.suggest part with a constant if optimization in not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBVrRXFjQefX"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  params = {\n",
    "      \"num_layers\":trial.suggest_int(\"num_layer\", 2, 7),\n",
    "      \"hidden_size\":trial.suggest_int(\"hidden_size\", 512, 2048),\n",
    "      \"dropout\": trial.suggest_uniform(\"dropout\", 0.2, 0.7),\n",
    "      \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 5e-3)\n",
    "  }\n",
    "\n",
    "  loss_ = run_training(params, save_model=False)\n",
    "  return(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ul2-AIqOZoMj",
    "outputId": "a4a9be79-531b-4842-dae1-d04faa8622a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-26 08:26:19,219] A new study created in memory with name: no-name-ee40455a-289f-4a45-9165-481b76409190\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "InXGaPpRcVY9",
    "outputId": "249593bf-5eb3-43bd-b6c7-194e7936217e"
   },
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"best_trial:\")\n",
    "trial_ = study.best_trial\n",
    "print(trial_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hyperparameter-search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
