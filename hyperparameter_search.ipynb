{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter-search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mainakdeb/moa-classification/blob/master/hyperparameter_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xv8CD4K_qUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOd8RhyuZzp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/\"My Drive\"/kaggle/lish-moa.zip /content/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSua2cgfd2hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip lish-moa.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB9siuMCJ5pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pip install iterative-stratification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhdrqaSOrppk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUpXeUlWeXpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e4d93a27-d5b5-45d5-9b24-8c9dda87b4dc"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "        \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import optuna\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter1d   ## smoother\n",
        "from tqdm.notebook import tqdm, tnrange\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 15, 7\n",
        "\n",
        "CGREEN  = '\\33[32m'\n",
        "CBLUE =  '\\033[34m'\n",
        "CRED = '\\033[1;31m'\n",
        "CEND  = '\\33[0m'\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=42)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0UcuWgBr0Hn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6838eedf-cbc1-4a17-b2dd-3b9b9acfde66"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "else:\n",
        "    device='cpu'\n",
        "    \n",
        "device\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZhAwZ3hr45u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = pd.read_csv('train_features.csv')\n",
        "train_targets = pd.read_csv('train_targets_scored.csv')\n",
        "train_targets_s = train_targets\n",
        "test_features = pd.read_csv('test_features.csv')\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaAdK1cyrxuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TorchStandardScaler:\n",
        "  def fit(self, x):\n",
        "    self.mean = x.mean(0, keepdim=True)\n",
        "    self.std = x.std(0, unbiased=False, keepdim=True)\n",
        "  def transform(self, x):\n",
        "    x -= self.mean\n",
        "    x /= (self.std + 1e-7)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWLxDlvyucnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    return df\n",
        "\n",
        "train = preprocess(train_features)\n",
        "test = preprocess(test_features)\n",
        "\n",
        "del train_targets['sig_id']\n",
        "\n",
        "target = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n",
        "train = train.loc[train['cp_type']==0].reset_index(drop=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNGkeMpMufs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f7cbedf-3036-453f-d373-272fc85d1d9c"
      },
      "source": [
        "top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "       870, 871, 872, 873, 874]\n",
        "\n",
        "all_columns = train.columns\n",
        "train=train[all_columns[top_features]]\n",
        "test = test[all_columns[top_features]]\n",
        "train.shape, test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21948, 785), (3982, 785))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_rTT1uYunp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.values\n",
        "target = target.values\n",
        "test = test.values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3YuEdDt1HFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14899f83-3b0e-43ee-ebc4-dfb51962860a"
      },
      "source": [
        "train.shape, target.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21948, 785), (21948, 206))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N0QYVKDsTm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train,targets, noise ):\n",
        "        \n",
        "        self.features  = train\n",
        "        self.targets = targets\n",
        "        self.noise = noise\n",
        "        \n",
        "    def sizes(self):\n",
        "        print(\"features size = \", self.features.shape[1])\n",
        "        print(\"targets size = \", self.targets.shape[1])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx]).float()\n",
        "        target = torch.tensor(self.targets[idx]).float()\n",
        "        return feature, target\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCn4Ba6EsVyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "        \n",
        "def show_lr(learning_rates):\n",
        "    plt.plot(learning_rates, label = \"learning rate\")\n",
        "    plt.ylabel(\"Learning rate\", fontsize = 15)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_step(x, y, model, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x.to(device))\n",
        "    y = y.float()\n",
        "    loss = criterion(pred,y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZqPFz-CsX4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    for _ in range(nlayers):\n",
        "      if len(layers) == 0:\n",
        "        layers.append(nn.Linear(nfeatures, hidden_size))\n",
        "        layers.append(nn.BatchNorm1d(hidden_size))\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.LeakyReLU())\n",
        "      else:\n",
        "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        layers.append(nn.BatchNorm1d(hidden_size))\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.LeakyReLU())\n",
        "\n",
        "    layers.append(nn.Linear(hidden_size, ntargets))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwCVq5qyDXao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training( params, save_model=False):\n",
        "  NFOLDS = 5\n",
        "  EPOCHS = 10 ## changes here \n",
        "  mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
        "\n",
        "  fold_val_losses = list()\n",
        "\n",
        "  for k , (train_idx,valid_idx) in enumerate(mskf.split(train,target)):\n",
        "\n",
        "      x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n",
        "      \n",
        "      train_dataset = TrainDataset(x_train, y_train, noise = False)\n",
        "      valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n",
        "      \n",
        "      train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers = 8)\n",
        "      val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True, num_workers = 8)\n",
        "      \n",
        "\n",
        "      model = Model(nfeatures=x_train.shape[1], \n",
        "                    ntargets=y_train.shape[1],\n",
        "                    nlayers=params[\"num_layers\"], \n",
        "                    hidden_size=params[\"hidden_size\"], \n",
        "                    dropout=params[\"dropout\"])\n",
        "      \n",
        "      model = model.cuda()\n",
        "      optimizer = optim.Adam(model.parameters(), lr = params[\"learning_rate\"], weight_decay=1e-5)\n",
        "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                      mode='min', \n",
        "                                                      factor=0.5, \n",
        "                                                      patience=3, \n",
        "                                                      eps=1e-4, \n",
        "                                                      verbose=True)\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "      eng = Engine(model, optimizer, device='cuda')\n",
        "      best_loss = 10000\n",
        "\n",
        "      print(CRED ,\"fold \", str(k+1), CEND)\n",
        "\n",
        "      for epoch in range(EPOCHS):\n",
        "        train_loss = eng.train(train_loader)\n",
        "        valid_loss = eng.evaluate(val_loader)\n",
        "        print(\"train_loss:\", train_loss, \"val_loss:\", valid_loss)\n",
        "        if valid_loss<best_loss:\n",
        "          best_loss = valid_loss\n",
        "          if save_model:\n",
        "            torch.save(model.state_dict(), \"model_{fold}.pth\")\n",
        "      \n",
        "  return(best_loss)\n",
        "\n",
        "  print(CBLUE, \"Training complete\", CEND)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4k5C9lEovt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Engine:\n",
        "  def __init__(self, model, optimizer, device):\n",
        "    self.model = model\n",
        "    self.device = device\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "  @staticmethod\n",
        "  def loss_fn(targets, outputs):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "  def train(self, data_loader):\n",
        "    self.model.train()\n",
        "    final_loss=0\n",
        "    for data in data_loader:\n",
        "      self.optimizer.zero_grad()\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = self.model(inputs)\n",
        "      loss = self.loss_fn(targets, outputs)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      final_loss += loss.item()\n",
        "    return(final_loss / len(data_loader))\n",
        "\n",
        "\n",
        "  def evaluate(self, data_loader):\n",
        "    self.model.train()\n",
        "    final_loss=0\n",
        "    for data in data_loader:\n",
        "      #self.optimizer.zero_grad()\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = self.model(inputs)\n",
        "      loss = self.loss_fn(targets, outputs)\n",
        "      #loss.backward()\n",
        "      #self.optimizer.step()\n",
        "      final_loss += loss.item()\n",
        "    return(final_loss / len(data_loader))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umofVWQbII_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# params = {\n",
        "#       \"num_layers\":trial.suggest_int(\"num_layer\", 1, 8),\n",
        "#       \"hidden_size\":trial.suggest_int(\"hidden_size\", 16, 4096),\n",
        "#       \"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.7),\n",
        "#       \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-2)\n",
        "#   }\n",
        "# run_training(params, save_model=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBVrRXFjQefX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(trial):\n",
        "  params = {\n",
        "      \"num_layers\":trial.suggest_int(\"num_layer\", 1, 8),\n",
        "      \"hidden_size\":trial.suggest_int(\"hidden_size\", 16, 4096),\n",
        "      \"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.7),\n",
        "      \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-2)\n",
        "  }\n",
        "\n",
        "  loss_ = run_training(params, save_model=False)\n",
        "  return(loss_)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InXGaPpRcVY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c9ac924-2e9c-4041-fbed-e22dfb32af9a"
      },
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"best_trial:\")\n",
        "trial_ = study.best_trial\n",
        "print(trial_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 05:54:06,147] A new study created in memory with name: no-name-ee38c3fd-5b1d-436c-aaf4-9469cbda9cf5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.043743044785831284 val_loss: 0.021343196638756327\n",
            "train_loss: 0.02116268497986206 val_loss: 0.020676683220598433\n",
            "train_loss: 0.020207170693986656 val_loss: 0.019720713194045756\n",
            "train_loss: 0.01940260318013421 val_loss: 0.018940692353579733\n",
            "train_loss: 0.01882745702818468 val_loss: 0.018534616981115606\n",
            "train_loss: 0.018510718789437542 val_loss: 0.01847604838096433\n",
            "train_loss: 0.01823858735219076 val_loss: 0.01821031028197871\n",
            "train_loss: 0.01812999578786717 val_loss: 0.01797429658472538\n",
            "train_loss: 0.017969645674515894 val_loss: 0.01779964416184359\n",
            "train_loss: 0.017864463236722826 val_loss: 0.01830590350760354\n",
            " fold  2 \n",
            "train_loss: 0.04391568210785803 val_loss: 0.021582691412833\n",
            "train_loss: 0.021215533716199192 val_loss: 0.020386476380129654\n",
            "train_loss: 0.020119138035005417 val_loss: 0.019981973701053195\n",
            "train_loss: 0.019343324747962364 val_loss: 0.01923203778763612\n",
            "train_loss: 0.018833740344406037 val_loss: 0.01918898171020879\n",
            "train_loss: 0.01846751036203426 val_loss: 0.018719037166900106\n",
            "train_loss: 0.018294428353724274 val_loss: 0.018450797018077638\n",
            "train_loss: 0.017982111305700266 val_loss: 0.01827315479103062\n",
            "train_loss: 0.017948528056613344 val_loss: 0.018381043233805232\n",
            "train_loss: 0.01783090768435943 val_loss: 0.01841071020397875\n",
            " fold  3 \n",
            "train_loss: 0.04327421723122614 val_loss: 0.021846526612838108\n",
            "train_loss: 0.021146917769658394 val_loss: 0.02087931376364496\n",
            "train_loss: 0.02015345451840456 val_loss: 0.019382867496460676\n",
            "train_loss: 0.0192019814233957 val_loss: 0.01913367687828011\n",
            "train_loss: 0.018822635868159326 val_loss: 0.018902375052372616\n",
            "train_loss: 0.018460124292397413 val_loss: 0.018481723653773468\n",
            "train_loss: 0.018169259888700384 val_loss: 0.018377579593410093\n",
            "train_loss: 0.01802536794591857 val_loss: 0.01846384670999315\n",
            "train_loss: 0.01780455516975211 val_loss: 0.01823085815542274\n",
            "train_loss: 0.017755197381357782 val_loss: 0.01828587603651815\n",
            " fold  4 \n",
            "train_loss: 0.04314519886089408 val_loss: 0.02144086640328169\n",
            "train_loss: 0.021105116098255352 val_loss: 0.020531599616838828\n",
            "train_loss: 0.02021998675891023 val_loss: 0.01958850698752536\n",
            "train_loss: 0.01929721905026531 val_loss: 0.019058045724199876\n",
            "train_loss: 0.018833593269675108 val_loss: 0.01883741281926632\n",
            "train_loss: 0.018526375401711117 val_loss: 0.01840849334581031\n",
            "train_loss: 0.01820519275666363 val_loss: 0.018367608077824116\n",
            "train_loss: 0.018089559509594372 val_loss: 0.018281466224127345\n",
            "train_loss: 0.017888519467543 val_loss: 0.017979716261227924\n",
            "train_loss: 0.017782815138175003 val_loss: 0.018218241011102993\n",
            " fold  5 \n",
            "train_loss: 0.04324774583126756 val_loss: 0.02173359671400653\n",
            "train_loss: 0.021109198436033035 val_loss: 0.02059825499438577\n",
            "train_loss: 0.01999667890207923 val_loss: 0.019440643696321383\n",
            "train_loss: 0.01916436018233282 val_loss: 0.019336361127595108\n",
            "train_loss: 0.01871647977310678 val_loss: 0.018546015231145754\n",
            "train_loss: 0.018458368256688118 val_loss: 0.018471479312413268\n",
            "train_loss: 0.01819020643344392 val_loss: 0.01831403923117452\n",
            "train_loss: 0.018005669616378735 val_loss: 0.018167799442178674\n",
            "train_loss: 0.017834035733687706 val_loss: 0.018182397095693484\n",
            "train_loss: 0.017729074343283108 val_loss: 0.01809049728843901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 05:56:41,592] Trial 0 finished with value: 0.01809049728843901 and parameters: {'num_layer': 6, 'hidden_size': 1052, 'dropout': 0.5188347969850853, 'learning_rate': 0.0008831212239556775}. Best is trial 0 with value: 0.01809049728843901.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03650541653505702 val_loss: 0.020441576424572203\n",
            "train_loss: 0.019983187884740208 val_loss: 0.019850569466749828\n",
            "train_loss: 0.019189611880405657 val_loss: 0.01866108277398679\n",
            "train_loss: 0.01876079143308427 val_loss: 0.018565644199649494\n",
            "train_loss: 0.018553689000723156 val_loss: 0.01841845146069924\n",
            "train_loss: 0.018383346437274115 val_loss: 0.01832727508412467\n",
            "train_loss: 0.01838922692273838 val_loss: 0.01821949312256442\n",
            "train_loss: 0.018229413237692654 val_loss: 0.018506406289007928\n",
            "train_loss: 0.018156288657337427 val_loss: 0.018654074002471235\n",
            "train_loss: 0.0180750146670186 val_loss: 0.018389592775040202\n",
            " fold  2 \n",
            "train_loss: 0.03636835280643857 val_loss: 0.0203969848031799\n",
            "train_loss: 0.01993950075753357 val_loss: 0.01944553293287754\n",
            "train_loss: 0.019059326202757118 val_loss: 0.01889761785666148\n",
            "train_loss: 0.018667442406919123 val_loss: 0.018903950850168865\n",
            "train_loss: 0.018492234526607004 val_loss: 0.018950306706958346\n",
            "train_loss: 0.01843927412604292 val_loss: 0.018602972539762657\n",
            "train_loss: 0.01831135194937604 val_loss: 0.01844283307178153\n",
            "train_loss: 0.018126651138553152 val_loss: 0.018342694112410147\n",
            "train_loss: 0.0181856290369794 val_loss: 0.018214535692499742\n",
            "train_loss: 0.018077092663641426 val_loss: 0.018229210128386814\n",
            " fold  3 \n",
            "train_loss: 0.036825250888216324 val_loss: 0.020507866102788184\n",
            "train_loss: 0.01995514669334111 val_loss: 0.01949900161061022\n",
            "train_loss: 0.0191753125358103 val_loss: 0.018934546141988702\n",
            "train_loss: 0.018777753192715456 val_loss: 0.01871645378155841\n",
            "train_loss: 0.01848182995952126 val_loss: 0.018684190491007432\n",
            "train_loss: 0.018342274443610855 val_loss: 0.01851392464919223\n",
            "train_loss: 0.01826646731680502 val_loss: 0.01861085421923134\n",
            "train_loss: 0.018210020496685436 val_loss: 0.018363142593039408\n",
            "train_loss: 0.01821994730203912 val_loss: 0.01836522099458509\n",
            "train_loss: 0.018181527843293938 val_loss: 0.018390424653059907\n",
            " fold  4 \n",
            "train_loss: 0.03658949571621159 val_loss: 0.020413974196546607\n",
            "train_loss: 0.020131395360373932 val_loss: 0.019682856380111642\n",
            "train_loss: 0.019214121213155813 val_loss: 0.018768189164499443\n",
            "train_loss: 0.018836242706933314 val_loss: 0.018668093718588352\n",
            "train_loss: 0.018586107921125233 val_loss: 0.018186597722686\n",
            "train_loss: 0.018415543648913717 val_loss: 0.01821562254594432\n",
            "train_loss: 0.018344690395600122 val_loss: 0.018545317980978224\n",
            "train_loss: 0.018235478820144268 val_loss: 0.018372713381217584\n",
            "train_loss: 0.01820004300173858 val_loss: 0.018081333591706224\n",
            "train_loss: 0.018186252118776672 val_loss: 0.01801907467759318\n",
            " fold  5 \n",
            "train_loss: 0.036629661580250744 val_loss: 0.020493916753265593\n",
            "train_loss: 0.019976620265431164 val_loss: 0.019335511244005628\n",
            "train_loss: 0.019106946710119213 val_loss: 0.019072365533146594\n",
            "train_loss: 0.01869432539746597 val_loss: 0.01854671982841359\n",
            "train_loss: 0.018394901275472796 val_loss: 0.018757121947904427\n",
            "train_loss: 0.01829473585214304 val_loss: 0.018258856816424265\n",
            "train_loss: 0.01816081426416834 val_loss: 0.018772577866911888\n",
            "train_loss: 0.01815930772167833 val_loss: 0.018428715359833505\n",
            "train_loss: 0.01805661007276048 val_loss: 0.018267755707105\n",
            "train_loss: 0.01813954805307414 val_loss: 0.018089856228066817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 05:59:25,976] Trial 1 finished with value: 0.018089856228066817 and parameters: {'num_layer': 7, 'hidden_size': 721, 'dropout': 0.3384970726057638, 'learning_rate': 0.0019213365993020527}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.6642833369365637 val_loss: 0.6139914790789286\n",
            "train_loss: 0.5666958663774573 val_loss: 0.5239511430263519\n",
            "train_loss: 0.48269844033579895 val_loss: 0.44589446153905654\n",
            "train_loss: 0.41087092804735986 val_loss: 0.37997738023598987\n",
            "train_loss: 0.35085854193438654 val_loss: 0.32510429951879716\n",
            "train_loss: 0.3011719262686329 val_loss: 0.2799010607931349\n",
            "train_loss: 0.2603053202231725 val_loss: 0.24229951037300956\n",
            "train_loss: 0.22628068837566653 val_loss: 0.2113594181007809\n",
            "train_loss: 0.19784164677063623 val_loss: 0.1854535150859091\n",
            "train_loss: 0.1739025056578111 val_loss: 0.1632181571589576\n",
            " fold  2 \n",
            "train_loss: 0.6498746742372927 val_loss: 0.5997591416041056\n",
            "train_loss: 0.5535408841527026 val_loss: 0.5108786722024282\n",
            "train_loss: 0.4706258564323619 val_loss: 0.4341221782896254\n",
            "train_loss: 0.4002356421256411 val_loss: 0.3697380969921748\n",
            "train_loss: 0.3416860757962517 val_loss: 0.31638158361117047\n",
            "train_loss: 0.2932517884866051 val_loss: 0.2724996308485667\n",
            "train_loss: 0.25335160923608835 val_loss: 0.2360158008005884\n",
            "train_loss: 0.22016869269419406 val_loss: 0.205934539437294\n",
            "train_loss: 0.19250291369963382 val_loss: 0.18038342975907856\n",
            "train_loss: 0.16903965520685998 val_loss: 0.15867239319615895\n",
            " fold  3 \n",
            "train_loss: 0.6543863005396249 val_loss: 0.6046147478951348\n",
            "train_loss: 0.5578069021736366 val_loss: 0.5152839422225952\n",
            "train_loss: 0.47444835218830383 val_loss: 0.43804537256558734\n",
            "train_loss: 0.4035028625225675 val_loss: 0.37284576892852783\n",
            "train_loss: 0.3442881381598072 val_loss: 0.31911657916174996\n",
            "train_loss: 0.29562753007031867 val_loss: 0.27463823391331565\n",
            "train_loss: 0.25548003916291223 val_loss: 0.2380206137895584\n",
            "train_loss: 0.22207021335328836 val_loss: 0.20749454614188936\n",
            "train_loss: 0.19422884469015012 val_loss: 0.18183742463588715\n",
            "train_loss: 0.17075945642115414 val_loss: 0.16021274195777047\n",
            " fold  4 \n",
            "train_loss: 0.6715615771818852 val_loss: 0.6211598813533783\n",
            "train_loss: 0.573758757200794 val_loss: 0.5312100450197855\n",
            "train_loss: 0.4889676398125248 val_loss: 0.45242930783165825\n",
            "train_loss: 0.4164552073115888 val_loss: 0.3855315198500951\n",
            "train_loss: 0.35565907670103986 val_loss: 0.32960674663384754\n",
            "train_loss: 0.30518489320209063 val_loss: 0.28354912665155196\n",
            "train_loss: 0.26344183162934537 val_loss: 0.2455210785071055\n",
            "train_loss: 0.22880209647658942 val_loss: 0.21400102062357795\n",
            "train_loss: 0.19981492753478064 val_loss: 0.18716498712698618\n",
            "train_loss: 0.17537040382191754 val_loss: 0.16478532056013742\n",
            " fold  5 \n",
            "train_loss: 0.6591323773930038 val_loss: 0.6086782945526971\n",
            "train_loss: 0.5619022954201353 val_loss: 0.5188857217629751\n",
            "train_loss: 0.47795045678166376 val_loss: 0.4412915077474382\n",
            "train_loss: 0.40640941650971124 val_loss: 0.37589239246315426\n",
            "train_loss: 0.3469326714242714 val_loss: 0.3214606112904019\n",
            "train_loss: 0.2977251026077547 val_loss: 0.27666061123212177\n",
            "train_loss: 0.25702719012464303 val_loss: 0.2395137424270312\n",
            "train_loss: 0.22334374670965085 val_loss: 0.20865978052218756\n",
            "train_loss: 0.19520175996897876 val_loss: 0.1828116642104255\n",
            "train_loss: 0.1714800306852313 val_loss: 0.16100065492921406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:02:41,000] Trial 2 finished with value: 0.16100065492921406 and parameters: {'num_layer': 6, 'hidden_size': 2787, 'dropout': 0.14503428979548028, 'learning_rate': 1.5534075287068437e-06}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03088665543043095 val_loss: 0.020884927465683885\n",
            "train_loss: 0.020129336530099743 val_loss: 0.019850535939137142\n",
            "train_loss: 0.019367103086973446 val_loss: 0.01954799010935757\n",
            "train_loss: 0.019105907097674797 val_loss: 0.01890353879166974\n",
            "train_loss: 0.018860390409827232 val_loss: 0.019061053689155314\n",
            "train_loss: 0.018886232174986948 val_loss: 0.018562389744652644\n",
            "train_loss: 0.01898094492060119 val_loss: 0.019019623287022114\n",
            "train_loss: 0.019085173987769995 val_loss: 0.018833105348878436\n",
            "train_loss: 0.01906961255936303 val_loss: 0.01904902524418301\n",
            "train_loss: 0.019086054944689724 val_loss: 0.018990434499250516\n",
            " fold  2 \n",
            "train_loss: 0.030099278693829758 val_loss: 0.02118291664454672\n",
            "train_loss: 0.020382466705758936 val_loss: 0.020171596358219784\n",
            "train_loss: 0.019404628503041855 val_loss: 0.01938761832813422\n",
            "train_loss: 0.019184751139170883 val_loss: 0.01944707954923312\n",
            "train_loss: 0.01901932854367339 val_loss: 0.018900133876336947\n",
            "train_loss: 0.01908402551856378 val_loss: 0.019290151798890695\n",
            "train_loss: 0.019101312087065933 val_loss: 0.019153666372100513\n",
            "train_loss: 0.019077133103881195 val_loss: 0.018872572005622916\n",
            "train_loss: 0.019004045607711094 val_loss: 0.019097272720601823\n",
            "train_loss: 0.019033961308499176 val_loss: 0.01921715069976118\n",
            " fold  3 \n",
            "train_loss: 0.030474596332920635 val_loss: 0.020988952678938706\n",
            "train_loss: 0.020131480205210224 val_loss: 0.02110034144586987\n",
            "train_loss: 0.0194163060463641 val_loss: 0.019517972237533994\n",
            "train_loss: 0.019024809619978718 val_loss: 0.019166530420382816\n",
            "train_loss: 0.018891074417995802 val_loss: 0.019044011934763856\n",
            "train_loss: 0.01894963540784691 val_loss: 0.019306522483627003\n",
            "train_loss: 0.018910486514315657 val_loss: 0.01922456506225798\n",
            "train_loss: 0.019020538642138676 val_loss: 0.019127051966885727\n",
            "train_loss: 0.01899803579663453 val_loss: 0.0198302384879854\n",
            "train_loss: 0.019149522459053474 val_loss: 0.019165310491290357\n",
            " fold  4 \n",
            "train_loss: 0.030795437675239384 val_loss: 0.021673317791687116\n",
            "train_loss: 0.021083976061123867 val_loss: 0.020291942378713027\n",
            "train_loss: 0.019543943481276863 val_loss: 0.019254013792508178\n",
            "train_loss: 0.019116382775963215 val_loss: 0.019297986808750365\n",
            "train_loss: 0.019018939021380916 val_loss: 0.019033348601725366\n",
            "train_loss: 0.018940736917589886 val_loss: 0.018650095133731764\n",
            "train_loss: 0.01904587187817779 val_loss: 0.018914604663021035\n",
            "train_loss: 0.01900957661100488 val_loss: 0.018427652462075155\n",
            "train_loss: 0.01903737195349042 val_loss: 0.019020108713044062\n",
            "train_loss: 0.019024548829411684 val_loss: 0.018836279917094443\n",
            " fold  5 \n",
            "train_loss: 0.030202750292053257 val_loss: 0.02040438147054778\n",
            "train_loss: 0.02000800497236027 val_loss: 0.019023912130958505\n",
            "train_loss: 0.019178393728815128 val_loss: 0.018916681512362428\n",
            "train_loss: 0.01900150077552467 val_loss: 0.01924503418720431\n",
            "train_loss: 0.018842577569834564 val_loss: 0.018944006620181933\n",
            "train_loss: 0.019026612869693316 val_loss: 0.019161157723930147\n",
            "train_loss: 0.018942984563393005 val_loss: 0.01890930864546034\n",
            "train_loss: 0.019013924746895613 val_loss: 0.019191744426886242\n",
            "train_loss: 0.018953003583179005 val_loss: 0.019092012610700395\n",
            "train_loss: 0.019111705815716497 val_loss: 0.019194887226654425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:05:00,603] Trial 3 finished with value: 0.01890930864546034 and parameters: {'num_layer': 4, 'hidden_size': 1663, 'dropout': 0.16307639956155834, 'learning_rate': 0.007604576348209199}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.6968648679878401 val_loss: 0.6563395096196069\n",
            "train_loss: 0.6181175890176193 val_loss: 0.583483037021425\n",
            "train_loss: 0.5472093643485636 val_loss: 0.5165839095910391\n",
            "train_loss: 0.4829794257015422 val_loss: 0.45554548170831466\n",
            "train_loss: 0.425919807691505 val_loss: 0.4017651660574807\n",
            "train_loss: 0.37590533883675287 val_loss: 0.3550354556904899\n",
            "train_loss: 0.33281251593776373 val_loss: 0.3146530075205697\n",
            "train_loss: 0.29551305157550867 val_loss: 0.28008224566777545\n",
            "train_loss: 0.26335948662481445 val_loss: 0.25030087845193016\n",
            "train_loss: 0.2360152890500815 val_loss: 0.22457850890027153\n",
            " fold  2 \n",
            "train_loss: 0.6868013204007909 val_loss: 0.6471011406845517\n",
            "train_loss: 0.6087735904299695 val_loss: 0.5735419922404819\n",
            "train_loss: 0.5382542301347291 val_loss: 0.5064620325962702\n",
            "train_loss: 0.4745602860398915 val_loss: 0.4467582073476579\n",
            "train_loss: 0.4182714182829511 val_loss: 0.39376993974049884\n",
            "train_loss: 0.3689023638549058 val_loss: 0.3479609870248371\n",
            "train_loss: 0.32650096692900726 val_loss: 0.3083595567279392\n",
            "train_loss: 0.2901644672172657 val_loss: 0.27427240709463757\n",
            "train_loss: 0.2587062969155934 val_loss: 0.24536834988329145\n",
            "train_loss: 0.23188411293254382 val_loss: 0.22049939632415771\n",
            " fold  3 \n",
            "train_loss: 0.6687865883543871 val_loss: 0.6300439569685194\n",
            "train_loss: 0.5917400791161302 val_loss: 0.5579135119915009\n",
            "train_loss: 0.5221286113711371 val_loss: 0.49196495446893906\n",
            "train_loss: 0.45966927253681683 val_loss: 0.43349653482437134\n",
            "train_loss: 0.40473705076653027 val_loss: 0.38123493724399143\n",
            "train_loss: 0.356943194416986 val_loss: 0.337154100338618\n",
            "train_loss: 0.3158841955920924 val_loss: 0.2988439980480406\n",
            "train_loss: 0.2805734872817993 val_loss: 0.2665024979246987\n",
            "train_loss: 0.25046249781397806 val_loss: 0.23805921938684252\n",
            "train_loss: 0.224387736208197 val_loss: 0.21393799367878172\n",
            " fold  4 \n",
            "train_loss: 0.7001601487830065 val_loss: 0.6605385277006361\n",
            "train_loss: 0.6213128044121508 val_loss: 0.5871205396122403\n",
            "train_loss: 0.5501967223658077 val_loss: 0.519381990035375\n",
            "train_loss: 0.4854655756034713 val_loss: 0.4581347356239955\n",
            "train_loss: 0.42807675638924475 val_loss: 0.4045337273014916\n",
            "train_loss: 0.377939993272657 val_loss: 0.3573046310080422\n",
            "train_loss: 0.33469422809455707 val_loss: 0.3168623944123586\n",
            "train_loss: 0.2972370673348938 val_loss: 0.282018792298105\n",
            "train_loss: 0.26501241941814835 val_loss: 0.2519482970237732\n",
            "train_loss: 0.2373142180883366 val_loss: 0.22633504701985252\n",
            " fold  5 \n",
            "train_loss: 0.6881114767081495 val_loss: 0.6479158335261874\n",
            "train_loss: 0.6097920105077218 val_loss: 0.5751425094074674\n",
            "train_loss: 0.5390270620152571 val_loss: 0.5083453357219696\n",
            "train_loss: 0.47525186469589453 val_loss: 0.4474502851565679\n",
            "train_loss: 0.41863321502139605 val_loss: 0.3944666071070565\n",
            "train_loss: 0.36944868987885077 val_loss: 0.34831633667151135\n",
            "train_loss: 0.3269078183001366 val_loss: 0.3088579922914505\n",
            "train_loss: 0.29026226504989294 val_loss: 0.27489692800574833\n",
            "train_loss: 0.259053407371908 val_loss: 0.24595875458584893\n",
            "train_loss: 0.2319726926693018 val_loss: 0.2207100209262636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:08:35,614] Trial 4 finished with value: 0.2207100209262636 and parameters: {'num_layer': 6, 'hidden_size': 3184, 'dropout': 0.4377058242487215, 'learning_rate': 1.1138116364225768e-06}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.6151247950999633 val_loss: 0.49207105073663926\n",
            "train_loss: 0.3952506860529167 val_loss: 0.31758784916665816\n",
            "train_loss: 0.26019609104032104 val_loss: 0.2151799574494362\n",
            "train_loss: 0.18158708966296652 val_loss: 0.15522060791651407\n",
            "train_loss: 0.13417684491993725 val_loss: 0.1176775416566266\n",
            "train_loss: 0.10432827029971109 val_loss: 0.093449669993586\n",
            "train_loss: 0.08435163609143617 val_loss: 0.0770620529850324\n",
            "train_loss: 0.07049525844986024 val_loss: 0.06545595534973675\n",
            "train_loss: 0.060508422850482704 val_loss: 0.05669556351171599\n",
            "train_loss: 0.053180794623019036 val_loss: 0.05030591723819574\n",
            " fold  2 \n",
            "train_loss: 0.5969049589357515 val_loss: 0.4753780961036682\n",
            "train_loss: 0.3815236162880193 val_loss: 0.3061240878370073\n",
            "train_loss: 0.25115497647852136 val_loss: 0.20755533377329508\n",
            "train_loss: 0.17547979864521304 val_loss: 0.1500218858321508\n",
            "train_loss: 0.13010804101392842 val_loss: 0.11429813380042712\n",
            "train_loss: 0.10128460160416106 val_loss: 0.09095341505275832\n",
            "train_loss: 0.08212473562014276 val_loss: 0.07488998936282264\n",
            "train_loss: 0.0686524714194778 val_loss: 0.06357599111894767\n",
            "train_loss: 0.05923388884875221 val_loss: 0.055418897006246776\n",
            "train_loss: 0.052121239916785904 val_loss: 0.04914851838515864\n",
            " fold  3 \n",
            "train_loss: 0.6031991437725399 val_loss: 0.48085379269387984\n",
            "train_loss: 0.3857268643551979 val_loss: 0.30984217259618974\n",
            "train_loss: 0.2538132760403813 val_loss: 0.2100575558013386\n",
            "train_loss: 0.17731183886096097 val_loss: 0.1517940941784117\n",
            "train_loss: 0.13143823695355566 val_loss: 0.1156257875263691\n",
            "train_loss: 0.10238090999748396 val_loss: 0.09163696318864822\n",
            "train_loss: 0.08282033267660417 val_loss: 0.07567318611674839\n",
            "train_loss: 0.06929152712657832 val_loss: 0.06438256634606256\n",
            "train_loss: 0.05953826720191949 val_loss: 0.05604193131956789\n",
            "train_loss: 0.052432706255627716 val_loss: 0.04969172635012203\n",
            " fold  4 \n",
            "train_loss: 0.6146590828463652 val_loss: 0.4912262409925461\n",
            "train_loss: 0.39457895034465235 val_loss: 0.31799845066335464\n",
            "train_loss: 0.25991140575944516 val_loss: 0.21481451061036852\n",
            "train_loss: 0.18134582949721295 val_loss: 0.1553709093067381\n",
            "train_loss: 0.13412018584600394 val_loss: 0.11792641836735937\n",
            "train_loss: 0.10427433020178822 val_loss: 0.09373008873727587\n",
            "train_loss: 0.08437326879820962 val_loss: 0.07736106051339044\n",
            "train_loss: 0.07050014929710954 val_loss: 0.06562581203050083\n",
            "train_loss: 0.060620107944460884 val_loss: 0.05675129054321183\n",
            "train_loss: 0.05318943737749604 val_loss: 0.05054165402220355\n",
            " fold  5 \n",
            "train_loss: 0.606262824025707 val_loss: 0.4829784731070201\n",
            "train_loss: 0.3881820813901182 val_loss: 0.31304891407489777\n",
            "train_loss: 0.2558806929467381 val_loss: 0.21190069284703997\n",
            "train_loss: 0.17885654376468796 val_loss: 0.152854575879044\n",
            "train_loss: 0.13247048838630968 val_loss: 0.11627194119824304\n",
            "train_loss: 0.10308862634111142 val_loss: 0.09256737968987888\n",
            "train_loss: 0.08337187194737836 val_loss: 0.07631546010573705\n",
            "train_loss: 0.06981420527765717 val_loss: 0.06480356802543004\n",
            "train_loss: 0.06010844064471514 val_loss: 0.05631710092226664\n",
            "train_loss: 0.052814890104143516 val_loss: 0.05015210476186541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:12:45,024] Trial 5 finished with value: 0.05015210476186541 and parameters: {'num_layer': 7, 'hidden_size': 3233, 'dropout': 0.6193981419816988, 'learning_rate': 4.165938075566149e-06}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03293439647371786 val_loss: 0.020307278881470364\n",
            "train_loss: 0.019030170015774776 val_loss: 0.018621237638096016\n",
            "train_loss: 0.018468527926429026 val_loss: 0.018763719230062433\n",
            "train_loss: 0.01851545374813503 val_loss: 0.018424669539348945\n",
            "train_loss: 0.01860906349738007 val_loss: 0.018551115050084062\n",
            "train_loss: 0.018648617840169565 val_loss: 0.018806213926937845\n",
            "train_loss: 0.018694207537919283 val_loss: 0.018735127937462594\n",
            "train_loss: 0.018703254479644955 val_loss: 0.018353188255180914\n",
            "train_loss: 0.018662483235686155 val_loss: 0.019095926752520934\n",
            "train_loss: 0.018653138446203178 val_loss: 0.018534400086436007\n",
            " fold  2 \n",
            "train_loss: 0.03254761199966289 val_loss: 0.019909600002898112\n",
            "train_loss: 0.019010838526098625 val_loss: 0.018664310582809977\n",
            "train_loss: 0.018608638395865757 val_loss: 0.018532876649664506\n",
            "train_loss: 0.01851814766616925 val_loss: 0.01889954424566693\n",
            "train_loss: 0.018545819338465084 val_loss: 0.018678776402440336\n",
            "train_loss: 0.018526222543332024 val_loss: 0.018692301482790045\n",
            "train_loss: 0.01859688565161997 val_loss: 0.018700781278312206\n",
            "train_loss: 0.018628077864970848 val_loss: 0.018894598715835147\n",
            "train_loss: 0.018603755910273478 val_loss: 0.01922529490871562\n",
            "train_loss: 0.018719090040827144 val_loss: 0.018592732958495617\n",
            " fold  3 \n",
            "train_loss: 0.032421823611240026 val_loss: 0.02075817094494899\n",
            "train_loss: 0.019053216564698494 val_loss: 0.018498758371505473\n",
            "train_loss: 0.018474350669893665 val_loss: 0.018774428715308506\n",
            "train_loss: 0.018510085523830377 val_loss: 0.01860110756630699\n",
            "train_loss: 0.01858656480908394 val_loss: 0.018998337392177846\n",
            "train_loss: 0.018607885880476755 val_loss: 0.01876520241300265\n",
            "train_loss: 0.018587834559435 val_loss: 0.018972086513208017\n",
            "train_loss: 0.018654257585497006 val_loss: 0.018761436868872907\n",
            "train_loss: 0.01863899872219865 val_loss: 0.018958318047225475\n",
            "train_loss: 0.0187208520491486 val_loss: 0.01908263677938117\n",
            " fold  4 \n",
            "train_loss: 0.03289658470970133 val_loss: 0.019888734962377284\n",
            "train_loss: 0.019090971356068832 val_loss: 0.018344864766630862\n",
            "train_loss: 0.018527296736188557 val_loss: 0.018565448363208108\n",
            "train_loss: 0.018520319078495537 val_loss: 0.018655901153882343\n",
            "train_loss: 0.01864839329694708 val_loss: 0.018461469353901014\n",
            "train_loss: 0.018654586794529703 val_loss: 0.01829643485446771\n",
            "train_loss: 0.018633485339797924 val_loss: 0.01880583715521627\n",
            "train_loss: 0.018749781209381595 val_loss: 0.01878941638602151\n",
            "train_loss: 0.018647691315930824 val_loss: 0.0187261826876137\n",
            "train_loss: 0.018653805894048317 val_loss: 0.018626844510436058\n",
            " fold  5 \n",
            "train_loss: 0.03259766626887131 val_loss: 0.019556985029743776\n",
            "train_loss: 0.019026870798805485 val_loss: 0.01881418004631996\n",
            "train_loss: 0.01850490899000695 val_loss: 0.018528120281795662\n",
            "train_loss: 0.0185607959030439 val_loss: 0.01880485305769576\n",
            "train_loss: 0.018594933552262577 val_loss: 0.018470706107715767\n",
            "train_loss: 0.018657977024660162 val_loss: 0.01899532104531924\n",
            "train_loss: 0.018686537624107324 val_loss: 0.01900600962754753\n",
            "train_loss: 0.018759086518885866 val_loss: 0.018624604783124395\n",
            "train_loss: 0.01863316753609241 val_loss: 0.018919314050840005\n",
            "train_loss: 0.018811304020978834 val_loss: 0.018659163162940078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:14:36,718] Trial 6 finished with value: 0.018470706107715767 and parameters: {'num_layer': 1, 'hidden_size': 1295, 'dropout': 0.28016787603649573, 'learning_rate': 0.009502422021513527}. Best is trial 1 with value: 0.018089856228066817.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.028835664651748062 val_loss: 0.01998532170222865\n",
            "train_loss: 0.019417749015965324 val_loss: 0.018648916338053014\n",
            "train_loss: 0.018754574692929567 val_loss: 0.0183451222255826\n",
            "train_loss: 0.018346820087374552 val_loss: 0.018168181129213836\n",
            "train_loss: 0.0182151655882489 val_loss: 0.018609686340722773\n",
            "train_loss: 0.018120806050095438 val_loss: 0.018333837596906558\n",
            "train_loss: 0.018094853916461918 val_loss: 0.018045142396456666\n",
            "train_loss: 0.018096213505697855 val_loss: 0.018030543501178425\n",
            "train_loss: 0.018072235708435375 val_loss: 0.01826270752482944\n",
            "train_loss: 0.017967635306759155 val_loss: 0.017979278850058716\n",
            " fold  2 \n",
            "train_loss: 0.029126453955752262 val_loss: 0.019887456877364054\n",
            "train_loss: 0.0193469582422488 val_loss: 0.019333414629929595\n",
            "train_loss: 0.018623027336392282 val_loss: 0.018979814007050462\n",
            "train_loss: 0.018437012361929468 val_loss: 0.01845993779392706\n",
            "train_loss: 0.018258746996845886 val_loss: 0.01818419403086106\n",
            "train_loss: 0.018052520169674055 val_loss: 0.01817448406169812\n",
            "train_loss: 0.017960972982742216 val_loss: 0.018439544468290277\n",
            "train_loss: 0.018084439833688994 val_loss: 0.017979509352395933\n",
            "train_loss: 0.018024540438816166 val_loss: 0.018195348067416087\n",
            "train_loss: 0.018008699215462675 val_loss: 0.0181063251155946\n",
            " fold  3 \n",
            "train_loss: 0.02920975602245417 val_loss: 0.02001846493739221\n",
            "train_loss: 0.019288887836686943 val_loss: 0.018830302274889417\n",
            "train_loss: 0.01847871702056432 val_loss: 0.018629546277225018\n",
            "train_loss: 0.018370256219329178 val_loss: 0.018780568304161232\n",
            "train_loss: 0.018131440991292828 val_loss: 0.018592533241543505\n",
            "train_loss: 0.018059841620371393 val_loss: 0.01849946690102418\n",
            "train_loss: 0.018022259393625933 val_loss: 0.018394347383744188\n",
            "train_loss: 0.018010712944079136 val_loss: 0.018630197271704674\n",
            "train_loss: 0.018035614255653774 val_loss: 0.017995327814585634\n",
            "train_loss: 0.018120525580277477 val_loss: 0.0182808352013429\n",
            " fold  4 \n",
            "train_loss: 0.029253778182833954 val_loss: 0.019755041744146083\n",
            "train_loss: 0.0195328191705588 val_loss: 0.018856265685624547\n",
            "train_loss: 0.01863927186291287 val_loss: 0.01913363000171052\n",
            "train_loss: 0.018392747787731714 val_loss: 0.01830386949910058\n",
            "train_loss: 0.018165792048355375 val_loss: 0.018260683450433943\n",
            "train_loss: 0.018055675929223282 val_loss: 0.018248422278298274\n",
            "train_loss: 0.01801211382869793 val_loss: 0.01778449723497033\n",
            "train_loss: 0.018009383838785732 val_loss: 0.01806198950443003\n",
            "train_loss: 0.018026089113529608 val_loss: 0.01823597090939681\n",
            "train_loss: 0.01806821592449062 val_loss: 0.018032092187139723\n",
            " fold  5 \n",
            "train_loss: 0.02892296427887851 val_loss: 0.020136326344476804\n",
            "train_loss: 0.0192395923929154 val_loss: 0.018823798435429733\n",
            "train_loss: 0.018528716233761414 val_loss: 0.01845878041866753\n",
            "train_loss: 0.01831026167433331 val_loss: 0.018735630644692317\n",
            "train_loss: 0.018121182493379583 val_loss: 0.018516380339860916\n",
            "train_loss: 0.018060830684945635 val_loss: 0.017976594985359244\n",
            "train_loss: 0.017986798483500446 val_loss: 0.01777702807966206\n",
            "train_loss: 0.017971809008631153 val_loss: 0.01797550595882866\n",
            "train_loss: 0.017985999813654285 val_loss: 0.018108231118983693\n",
            "train_loss: 0.017991359858517197 val_loss: 0.018215154711571004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:16:48,701] Trial 7 finished with value: 0.01777702807966206 and parameters: {'num_layer': 3, 'hidden_size': 2003, 'dropout': 0.3511757636099212, 'learning_rate': 0.0033357927638280713}. Best is trial 7 with value: 0.01777702807966206.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03044311295978833 val_loss: 0.02055494725290272\n",
            "train_loss: 0.0204283957761051 val_loss: 0.02100184549474054\n",
            "train_loss: 0.01972603118991938 val_loss: 0.019154510874715116\n",
            "train_loss: 0.01932605276343183 val_loss: 0.019656938811143238\n",
            "train_loss: 0.019060101191364767 val_loss: 0.0194419389590621\n",
            "train_loss: 0.018982133281457682 val_loss: 0.019226913547350302\n",
            "train_loss: 0.01883515042077372 val_loss: 0.018798722471627925\n",
            "train_loss: 0.018816492850959734 val_loss: 0.01864133237136735\n",
            "train_loss: 0.01876377733424306 val_loss: 0.018741528762297496\n",
            "train_loss: 0.01864190589285631 val_loss: 0.018264416087832715\n",
            " fold  2 \n",
            "train_loss: 0.030130901873327683 val_loss: 0.020505967032578256\n",
            "train_loss: 0.020248925920737827 val_loss: 0.019552553693453472\n",
            "train_loss: 0.019394344589470522 val_loss: 0.019381248495644994\n",
            "train_loss: 0.01886829013760755 val_loss: 0.01893586054858234\n",
            "train_loss: 0.018746134487615116 val_loss: 0.01897217043572002\n",
            "train_loss: 0.01870727130090413 val_loss: 0.01907639391720295\n",
            "train_loss: 0.01874264220342688 val_loss: 0.019297611175311938\n",
            "train_loss: 0.018727077063227047 val_loss: 0.019282970887919266\n",
            "train_loss: 0.01860408842617619 val_loss: 0.01870026470472415\n",
            "train_loss: 0.01851964231742465 val_loss: 0.018696799046463437\n",
            " fold  3 \n",
            "train_loss: 0.02998256158299636 val_loss: 0.021696347329351637\n",
            "train_loss: 0.020319474739548954 val_loss: 0.01974883282350169\n",
            "train_loss: 0.01935483293229903 val_loss: 0.01959267869177792\n",
            "train_loss: 0.01908413550236087 val_loss: 0.019963191925651498\n",
            "train_loss: 0.018883167968496033 val_loss: 0.019423680379986763\n",
            "train_loss: 0.018883899353660534 val_loss: 0.0191234415397048\n",
            "train_loss: 0.018876840184996094 val_loss: 0.01892586869912015\n",
            "train_loss: 0.018674422649369724 val_loss: 0.019164364888436265\n",
            "train_loss: 0.018607791135276573 val_loss: 0.018702544789347384\n",
            "train_loss: 0.01847570515035287 val_loss: 0.018820616313152842\n",
            " fold  4 \n",
            "train_loss: 0.029679951150024284 val_loss: 0.020868568163779046\n",
            "train_loss: 0.020183257880094257 val_loss: 0.019397620887806017\n",
            "train_loss: 0.01959180756998451 val_loss: 0.019437189524372418\n",
            "train_loss: 0.019245667747505333 val_loss: 0.019741639598376222\n",
            "train_loss: 0.01909440776090259 val_loss: 0.0190237309369776\n",
            "train_loss: 0.018831796971136246 val_loss: 0.019207192170951102\n",
            "train_loss: 0.018823853599420494 val_loss: 0.018562422547903325\n",
            "train_loss: 0.01881114280094271 val_loss: 0.019145835501452286\n",
            "train_loss: 0.01863741597322666 val_loss: 0.01836079690191481\n",
            "train_loss: 0.018469536339567192 val_loss: 0.018104448190165892\n",
            " fold  5 \n",
            "train_loss: 0.030890706224717956 val_loss: 0.020717554311785433\n",
            "train_loss: 0.020429302695328774 val_loss: 0.019798699869877763\n",
            "train_loss: 0.019607675342780094 val_loss: 0.01960341880718867\n",
            "train_loss: 0.019355306982238224 val_loss: 0.019067515412138566\n",
            "train_loss: 0.019146672503995724 val_loss: 0.019064941443502903\n",
            "train_loss: 0.01891084838712561 val_loss: 0.018970264225370355\n",
            "train_loss: 0.01889624108519891 val_loss: 0.01893730026980241\n",
            "train_loss: 0.018778076516869274 val_loss: 0.018790043993956514\n",
            "train_loss: 0.018690993698934715 val_loss: 0.018622359157436423\n",
            "train_loss: 0.018483428021326014 val_loss: 0.018144183688693576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:20:33,393] Trial 8 finished with value: 0.018144183688693576 and parameters: {'num_layer': 8, 'hidden_size': 2607, 'dropout': 0.4013797519939254, 'learning_rate': 0.0015363231998321502}. Best is trial 7 with value: 0.01777702807966206.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03149076024799243 val_loss: 0.01868260672522916\n",
            "train_loss: 0.01808860057802952 val_loss: 0.017919432889256213\n",
            "train_loss: 0.017431349385583748 val_loss: 0.01771824186046918\n",
            "train_loss: 0.01702320944868784 val_loss: 0.017361194361001253\n",
            "train_loss: 0.016671937694638105 val_loss: 0.017070844769477844\n",
            "train_loss: 0.01632803887722717 val_loss: 0.017190953250974417\n",
            "train_loss: 0.016105791904788086 val_loss: 0.017002052161842585\n",
            "train_loss: 0.015805258125444685 val_loss: 0.016818586219516065\n",
            "train_loss: 0.015575575723272303 val_loss: 0.017133603752073314\n",
            "train_loss: 0.01540291235120832 val_loss: 0.017300736935188372\n",
            " fold  2 \n",
            "train_loss: 0.031086959943607235 val_loss: 0.01861319991035594\n",
            "train_loss: 0.018058074145591345 val_loss: 0.017751732013291784\n",
            "train_loss: 0.017429058777465336 val_loss: 0.017772296650542155\n",
            "train_loss: 0.017135410103946924 val_loss: 0.017362661763197847\n",
            "train_loss: 0.01676448112677621 val_loss: 0.017654996261828475\n",
            "train_loss: 0.01650780849698661 val_loss: 0.017495042954881985\n",
            "train_loss: 0.01618618706403219 val_loss: 0.017470732538236514\n",
            "train_loss: 0.015943539261386013 val_loss: 0.016920336315201387\n",
            "train_loss: 0.01564559536864576 val_loss: 0.01706840827440222\n",
            "train_loss: 0.015488966374887505 val_loss: 0.017225935227341123\n",
            " fold  3 \n",
            "train_loss: 0.031475879231710795 val_loss: 0.018978231793476477\n",
            "train_loss: 0.018188280903774758 val_loss: 0.017889450304210186\n",
            "train_loss: 0.01748533903931578 val_loss: 0.017875987105071545\n",
            "train_loss: 0.01698639868772116 val_loss: 0.017769729511605367\n",
            "train_loss: 0.016616931043403303 val_loss: 0.01740662318964799\n",
            "train_loss: 0.016280073449825464 val_loss: 0.017263642874442868\n",
            "train_loss: 0.016000124592118074 val_loss: 0.01697246388842662\n",
            "train_loss: 0.01572695359641659 val_loss: 0.01710148983531528\n",
            "train_loss: 0.015560467450785032 val_loss: 0.01708059985604551\n",
            "train_loss: 0.015366086041203875 val_loss: 0.017219581227335665\n",
            " fold  4 \n",
            "train_loss: 0.03134756942914016 val_loss: 0.018661370500922203\n",
            "train_loss: 0.018148617204818605 val_loss: 0.017870495044108894\n",
            "train_loss: 0.017446765365699928 val_loss: 0.017650834181242518\n",
            "train_loss: 0.017001220963193453 val_loss: 0.017732359779377777\n",
            "train_loss: 0.01668482384495977 val_loss: 0.017830463954144053\n",
            "train_loss: 0.016259454838607624 val_loss: 0.017510173117948905\n",
            "train_loss: 0.015985062649554533 val_loss: 0.016810670132852264\n",
            "train_loss: 0.015714381261310285 val_loss: 0.01758049273242553\n",
            "train_loss: 0.015511626418194046 val_loss: 0.017082489509549405\n",
            "train_loss: 0.015302765399109627 val_loss: 0.01698945974931121\n",
            " fold  5 \n",
            "train_loss: 0.031752848795250706 val_loss: 0.018619787982768483\n",
            "train_loss: 0.018142240830575643 val_loss: 0.018116018631392054\n",
            "train_loss: 0.01737177618306832 val_loss: 0.018075240672462516\n",
            "train_loss: 0.016971998152903456 val_loss: 0.01742143991092841\n",
            "train_loss: 0.016553679933312578 val_loss: 0.017740549105736945\n",
            "train_loss: 0.016277474371473425 val_loss: 0.017280531374530658\n",
            "train_loss: 0.016065393709510132 val_loss: 0.01735574436477489\n",
            "train_loss: 0.01577672830013477 val_loss: 0.017357283375329442\n",
            "train_loss: 0.01556650829244999 val_loss: 0.017220640917205148\n",
            "train_loss: 0.015370051288788301 val_loss: 0.016764250118285418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:22:47,319] Trial 9 finished with value: 0.016764250118285418 and parameters: {'num_layer': 3, 'hidden_size': 1894, 'dropout': 0.15739205763019304, 'learning_rate': 0.0011353568729844193}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.06592363691416339 val_loss: 0.021961183701124456\n",
            "train_loss: 0.020150980588210667 val_loss: 0.019206307259284787\n",
            "train_loss: 0.018278266303241253 val_loss: 0.018639640882611275\n",
            "train_loss: 0.01710982191497865 val_loss: 0.017854212472836178\n",
            "train_loss: 0.01617044516150718 val_loss: 0.017641084372169442\n",
            "train_loss: 0.01532377521979852 val_loss: 0.017487618347836867\n",
            "train_loss: 0.014559761067663414 val_loss: 0.01714132136354844\n",
            "train_loss: 0.013738122087079977 val_loss: 0.016895542593879834\n",
            "train_loss: 0.012991225178205017 val_loss: 0.017184819457017712\n",
            "train_loss: 0.012300251072029705 val_loss: 0.01685146428644657\n",
            " fold  2 \n",
            "train_loss: 0.06753828288798315 val_loss: 0.022049437794420455\n",
            "train_loss: 0.020123765560919823 val_loss: 0.019738847700258095\n",
            "train_loss: 0.01822884467875828 val_loss: 0.01856619885398282\n",
            "train_loss: 0.01705795700621346 val_loss: 0.018104274550245866\n",
            "train_loss: 0.016179535160030144 val_loss: 0.017860683509045176\n",
            "train_loss: 0.015316374100528765 val_loss: 0.017309343649281397\n",
            "train_loss: 0.014505050003366627 val_loss: 0.017372796311974525\n",
            "train_loss: 0.013723414964919937 val_loss: 0.01716023704244031\n",
            "train_loss: 0.012978865650307009 val_loss: 0.01705125683090753\n",
            "train_loss: 0.012254503170919159 val_loss: 0.01733400807198551\n",
            " fold  3 \n",
            "train_loss: 0.06603207200279702 val_loss: 0.021989256764451664\n",
            "train_loss: 0.020089630172520443 val_loss: 0.019525522262685828\n",
            "train_loss: 0.018220145849214085 val_loss: 0.018602969745794933\n",
            "train_loss: 0.017036610411183126 val_loss: 0.01812763149953551\n",
            "train_loss: 0.016114601858662092 val_loss: 0.017917154770758417\n",
            "train_loss: 0.015251654389219872 val_loss: 0.017686647259526782\n",
            "train_loss: 0.014472701600280361 val_loss: 0.017279704101383686\n",
            "train_loss: 0.013705507966865232 val_loss: 0.016993041719413467\n",
            "train_loss: 0.013002933171726223 val_loss: 0.017287363970859185\n",
            "train_loss: 0.012198672930210612 val_loss: 0.01698603496576349\n",
            " fold  4 \n",
            "train_loss: 0.06535465908709212 val_loss: 0.021915436825818486\n",
            "train_loss: 0.02004928071645723 val_loss: 0.01943570923888021\n",
            "train_loss: 0.01822532006147979 val_loss: 0.018572152281800907\n",
            "train_loss: 0.017086360954504082 val_loss: 0.018092637985116906\n",
            "train_loss: 0.01612846715969668 val_loss: 0.017666753329750564\n",
            "train_loss: 0.015294528964475014 val_loss: 0.017528217803272936\n",
            "train_loss: 0.014472477387744879 val_loss: 0.01719580844251646\n",
            "train_loss: 0.013735431806602772 val_loss: 0.016943803005334403\n",
            "train_loss: 0.012927954544083796 val_loss: 0.016962846637599997\n",
            "train_loss: 0.012211754843862593 val_loss: 0.016982758624686137\n",
            " fold  5 \n",
            "train_loss: 0.06652906717921513 val_loss: 0.021988347586658265\n",
            "train_loss: 0.020119202012817066 val_loss: 0.019508248505493004\n",
            "train_loss: 0.018222061033104208 val_loss: 0.018709591072466638\n",
            "train_loss: 0.017075476682056553 val_loss: 0.018033879291680124\n",
            "train_loss: 0.016162760697467173 val_loss: 0.017829950381484296\n",
            "train_loss: 0.015295593837357086 val_loss: 0.01747332254631652\n",
            "train_loss: 0.014496647428883158 val_loss: 0.017435517938186724\n",
            "train_loss: 0.013749819991273293 val_loss: 0.01705365405521459\n",
            "train_loss: 0.013024756529678902 val_loss: 0.017031505703926086\n",
            "train_loss: 0.01225064906116197 val_loss: 0.017138808241320982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:24:41,016] Trial 10 finished with value: 0.017031505703926086 and parameters: {'num_layer': 1, 'hidden_size': 3911, 'dropout': 0.22594218092039553, 'learning_rate': 0.00012026704264571361}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.13096850054959455 val_loss: 0.033955153077840805\n",
            "train_loss: 0.026932238020758698 val_loss: 0.023453434618810814\n",
            "train_loss: 0.021504429182496624 val_loss: 0.020902887918055058\n",
            "train_loss: 0.019553066771207512 val_loss: 0.019707091049187712\n",
            "train_loss: 0.018411494467569435 val_loss: 0.019007548068960507\n",
            "train_loss: 0.017678310628980398 val_loss: 0.01863131258222792\n",
            "train_loss: 0.01697709109040274 val_loss: 0.018399859571622476\n",
            "train_loss: 0.01637735674697636 val_loss: 0.018018311510483425\n",
            "train_loss: 0.015879493692646855 val_loss: 0.017867113049659464\n",
            "train_loss: 0.015377524592306303 val_loss: 0.017558164325439267\n",
            " fold  2 \n",
            "train_loss: 0.1303119712603697 val_loss: 0.033516918826434344\n",
            "train_loss: 0.026957316958493946 val_loss: 0.023429817106160853\n",
            "train_loss: 0.021541569246978 val_loss: 0.020518533781998687\n",
            "train_loss: 0.019543701202433178 val_loss: 0.019558530093895063\n",
            "train_loss: 0.018428587179253067 val_loss: 0.019105889317062166\n",
            "train_loss: 0.017709493421126103 val_loss: 0.018543187011447217\n",
            "train_loss: 0.016947190763185852 val_loss: 0.018568058187762897\n",
            "train_loss: 0.016410278250881725 val_loss: 0.018073376268148422\n",
            "train_loss: 0.015857602246915518 val_loss: 0.017700418933398195\n",
            "train_loss: 0.01536772919791764 val_loss: 0.017616503613276616\n",
            " fold  3 \n",
            "train_loss: 0.1317883945353653 val_loss: 0.03389837717016538\n",
            "train_loss: 0.027025215827144573 val_loss: 0.02362022652394242\n",
            "train_loss: 0.02150898775004822 val_loss: 0.020899657987886004\n",
            "train_loss: 0.019582293333782665 val_loss: 0.019565279492073588\n",
            "train_loss: 0.01844631522839916 val_loss: 0.019091513007879257\n",
            "train_loss: 0.017632897376366283 val_loss: 0.01862377269814412\n",
            "train_loss: 0.017003982044432476 val_loss: 0.01811563244296445\n",
            "train_loss: 0.01637152633022355 val_loss: 0.01787069382973843\n",
            "train_loss: 0.015870670295333948 val_loss: 0.01770346679000391\n",
            "train_loss: 0.015365112518918688 val_loss: 0.017629030160605907\n",
            " fold  4 \n",
            "train_loss: 0.13110199953982796 val_loss: 0.033897130439678826\n",
            "train_loss: 0.026887702944594017 val_loss: 0.023430692238940135\n",
            "train_loss: 0.021471182572776856 val_loss: 0.020927635331948597\n",
            "train_loss: 0.01956086285898219 val_loss: 0.01966878864914179\n",
            "train_loss: 0.018453327952411728 val_loss: 0.018839477457933955\n",
            "train_loss: 0.017660672945118902 val_loss: 0.01844770647585392\n",
            "train_loss: 0.01697482663355228 val_loss: 0.017968361576398213\n",
            "train_loss: 0.016416344406974058 val_loss: 0.01800738223310974\n",
            "train_loss: 0.015884435081017622 val_loss: 0.017754499490062397\n",
            "train_loss: 0.01539844802270333 val_loss: 0.01751238676822848\n",
            " fold  5 \n",
            "train_loss: 0.12908346735048984 val_loss: 0.03374065893391768\n",
            "train_loss: 0.026750988097510475 val_loss: 0.023562363555861845\n",
            "train_loss: 0.021406028993373762 val_loss: 0.02082022600289848\n",
            "train_loss: 0.01950761115691368 val_loss: 0.019880853696829744\n",
            "train_loss: 0.01840055248686585 val_loss: 0.01928201297091113\n",
            "train_loss: 0.017593035022255735 val_loss: 0.018501529470086098\n",
            "train_loss: 0.01697296720079106 val_loss: 0.018316998042994075\n",
            "train_loss: 0.016358996397289244 val_loss: 0.01811929036759668\n",
            "train_loss: 0.015858691327435816 val_loss: 0.01787495468225744\n",
            "train_loss: 0.015317106045836556 val_loss: 0.017635313069654837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:26:34,074] Trial 11 finished with value: 0.017635313069654837 and parameters: {'num_layer': 1, 'hidden_size': 2280, 'dropout': 0.10109736699512295, 'learning_rate': 8.364734744218163e-05}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.6279185232908829 val_loss: 0.5459213223722246\n",
            "train_loss: 0.47390033736609033 val_loss: 0.40692011680867934\n",
            "train_loss: 0.35069089631239575 val_loss: 0.2981966783603032\n",
            "train_loss: 0.25643820164428244 val_loss: 0.2182487431499693\n",
            "train_loss: 0.18860155991885974 val_loss: 0.16134761936134762\n",
            "train_loss: 0.14090731340473978 val_loss: 0.12227734757794274\n",
            "train_loss: 0.1090227106343145 val_loss: 0.09584006418784459\n",
            "train_loss: 0.0864042957731779 val_loss: 0.07691787969734934\n",
            "train_loss: 0.0712967588957669 val_loss: 0.06407796943353282\n",
            "train_loss: 0.06012844585854074 val_loss: 0.05571607107089625\n",
            " fold  2 \n",
            "train_loss: 0.6330210527648097 val_loss: 0.5491271283891466\n",
            "train_loss: 0.474320978358172 val_loss: 0.4037303494082557\n",
            "train_loss: 0.34488625630088476 val_loss: 0.29138988422022927\n",
            "train_loss: 0.2467796720456386 val_loss: 0.20893151147498024\n",
            "train_loss: 0.1799274592295937 val_loss: 0.15455272959338295\n",
            "train_loss: 0.13542478823143503 val_loss: 0.11805953333775203\n",
            "train_loss: 0.10549145645421484 val_loss: 0.09335201647546557\n",
            "train_loss: 0.08456349502439084 val_loss: 0.07718438448177443\n",
            "train_loss: 0.0697459096385949 val_loss: 0.06436838561462031\n",
            "train_loss: 0.05919495967311272 val_loss: 0.05511141589118375\n",
            " fold  3 \n",
            "train_loss: 0.6357023150160692 val_loss: 0.5528909265995026\n",
            "train_loss: 0.47885997766169947 val_loss: 0.4104604836967256\n",
            "train_loss: 0.3521194594061893 val_loss: 0.3000023927953508\n",
            "train_loss: 0.25712433424980746 val_loss: 0.21740185138252047\n",
            "train_loss: 0.1888822921808215 val_loss: 0.1608553785416815\n",
            "train_loss: 0.14098066841994505 val_loss: 0.1228150948882103\n",
            "train_loss: 0.10879128280541171 val_loss: 0.09603789986835586\n",
            "train_loss: 0.08680603642394577 val_loss: 0.07850619778037071\n",
            "train_loss: 0.07111942935465039 val_loss: 0.06479575898912218\n",
            "train_loss: 0.059873571295453155 val_loss: 0.05511736890508069\n",
            " fold  4 \n",
            "train_loss: 0.6217273456462915 val_loss: 0.5402951339880625\n",
            "train_loss: 0.4687893757785576 val_loss: 0.4026816685994466\n",
            "train_loss: 0.3472695534212002 val_loss: 0.2977810783518685\n",
            "train_loss: 0.25566418857678125 val_loss: 0.21830371850066715\n",
            "train_loss: 0.189115304985772 val_loss: 0.1632196290625466\n",
            "train_loss: 0.14227334696097652 val_loss: 0.12405157089233398\n",
            "train_loss: 0.11026489761644516 val_loss: 0.09648147887653774\n",
            "train_loss: 0.0877784616921259 val_loss: 0.07902015993992488\n",
            "train_loss: 0.07186242190284142 val_loss: 0.06515072513785627\n",
            "train_loss: 0.060375820548422096 val_loss: 0.05556240160432127\n",
            " fold  5 \n",
            "train_loss: 0.6192454518615336 val_loss: 0.5382502675056458\n",
            "train_loss: 0.46719037233919336 val_loss: 0.3998345848586824\n",
            "train_loss: 0.34395388740560284 val_loss: 0.29239484833346474\n",
            "train_loss: 0.24908153205246164 val_loss: 0.21072136527962154\n",
            "train_loss: 0.18218312714842783 val_loss: 0.15717093812094796\n",
            "train_loss: 0.1363395498930544 val_loss: 0.12078484189179209\n",
            "train_loss: 0.10598005273419878 val_loss: 0.09479885093039936\n",
            "train_loss: 0.08500615426379701 val_loss: 0.07665079914861256\n",
            "train_loss: 0.07016461777190368 val_loss: 0.06432245692445172\n",
            "train_loss: 0.05963206472064274 val_loss: 0.055266672331425876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:28:37,055] Trial 12 finished with value: 0.055266672331425876 and parameters: {'num_layer': 2, 'hidden_size': 62, 'dropout': 0.23186919064045042, 'learning_rate': 0.00010489938265935068}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.045315457359496235 val_loss: 0.01894083846774366\n",
            "train_loss: 0.018046756956618334 val_loss: 0.017831797815031476\n",
            "train_loss: 0.01658264511818255 val_loss: 0.01703664743238025\n",
            "train_loss: 0.015105906867192707 val_loss: 0.017388468660000298\n",
            "train_loss: 0.013338054051163836 val_loss: 0.017163944223688707\n",
            "train_loss: 0.01149927474477369 val_loss: 0.017660554808874924\n",
            "train_loss: 0.009640844724397513 val_loss: 0.018001668258673616\n",
            "train_loss: 0.007855806614447763 val_loss: 0.01808151633789142\n",
            "train_loss: 0.006039677673707838 val_loss: 0.018684030510485172\n",
            "train_loss: 0.004668105574176256 val_loss: 0.019147346934510603\n",
            " fold  2 \n",
            "train_loss: 0.04668865747668821 val_loss: 0.019011763752334647\n",
            "train_loss: 0.01798967663468658 val_loss: 0.01782354112300608\n",
            "train_loss: 0.016562136571746374 val_loss: 0.017704059680302937\n",
            "train_loss: 0.015091097792205603 val_loss: 0.01736797361324231\n",
            "train_loss: 0.013366774190217257 val_loss: 0.01726991119277146\n",
            "train_loss: 0.01149615621113259 val_loss: 0.01803654059767723\n",
            "train_loss: 0.00960933719424234 val_loss: 0.018318714367018804\n",
            "train_loss: 0.007707235416856365 val_loss: 0.018452907705472574\n",
            "train_loss: 0.006069201787096867 val_loss: 0.01931520675619443\n",
            "train_loss: 0.0048284702849965815 val_loss: 0.020001781586971547\n",
            " fold  3 \n",
            "train_loss: 0.045696358587862786 val_loss: 0.019029684054354828\n",
            "train_loss: 0.01795575679347351 val_loss: 0.018014286127355363\n",
            "train_loss: 0.016434113987708006 val_loss: 0.01737667702966266\n",
            "train_loss: 0.014936846741677626 val_loss: 0.017229263412041798\n",
            "train_loss: 0.013198580383660568 val_loss: 0.01727839145395491\n",
            "train_loss: 0.011263722094936647 val_loss: 0.017531156436436705\n",
            "train_loss: 0.009345533910031983 val_loss: 0.018036340777244832\n",
            "train_loss: 0.00752854631687312 val_loss: 0.0186255458328459\n",
            "train_loss: 0.005920246830615012 val_loss: 0.01887707132846117\n",
            "train_loss: 0.00461182827838575 val_loss: 0.01930245493228237\n",
            " fold  4 \n",
            "train_loss: 0.04611442813082882 val_loss: 0.019207450457745127\n",
            "train_loss: 0.018136419605571722 val_loss: 0.017959689411024254\n",
            "train_loss: 0.016618436971760315 val_loss: 0.01743353923989667\n",
            "train_loss: 0.015169222184551367 val_loss: 0.017315980098727677\n",
            "train_loss: 0.013482640558124884 val_loss: 0.017496184549397893\n",
            "train_loss: 0.011592738181892513 val_loss: 0.017711296522368986\n",
            "train_loss: 0.009721349668351637 val_loss: 0.018117567834754784\n",
            "train_loss: 0.007871071700060713 val_loss: 0.018567607634597354\n",
            "train_loss: 0.006096608386766436 val_loss: 0.01929847326957517\n",
            "train_loss: 0.0047095503820025406 val_loss: 0.019430062733590603\n",
            " fold  5 \n",
            "train_loss: 0.046977116793826004 val_loss: 0.0191749090121852\n",
            "train_loss: 0.01801660780430488 val_loss: 0.017654211208638217\n",
            "train_loss: 0.01649347905987415 val_loss: 0.017662781808111403\n",
            "train_loss: 0.015073603336307882 val_loss: 0.01723725027922127\n",
            "train_loss: 0.013349182417859201 val_loss: 0.01732987372411622\n",
            "train_loss: 0.011468187939591598 val_loss: 0.017652857324315444\n",
            "train_loss: 0.009564503581951494 val_loss: 0.018190421888397798\n",
            "train_loss: 0.007709577898967309 val_loss: 0.018158927663332887\n",
            "train_loss: 0.006027022926676748 val_loss: 0.01916732783946726\n",
            "train_loss: 0.004800514898438385 val_loss: 0.019540818201171026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:31:10,826] Trial 13 finished with value: 0.01723725027922127 and parameters: {'num_layer': 3, 'hidden_size': 3548, 'dropout': 0.23406821664145047, 'learning_rate': 0.00019710567763290364}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.2392981930271439 val_loss: 0.06974356952640745\n",
            "train_loss: 0.04670461240238038 val_loss: 0.03345166105363104\n",
            "train_loss: 0.028836241491354893 val_loss: 0.025387560638288658\n",
            "train_loss: 0.023551473879943722 val_loss: 0.022245855381091435\n",
            "train_loss: 0.021173234813023304 val_loss: 0.020673017534944747\n",
            "train_loss: 0.019780814202259415 val_loss: 0.019969121139082644\n",
            "train_loss: 0.01881348934240531 val_loss: 0.01894149153182904\n",
            "train_loss: 0.018123443724344605 val_loss: 0.018556712091796927\n",
            "train_loss: 0.017510909452170566 val_loss: 0.018258322237266436\n",
            "train_loss: 0.016963921602059534 val_loss: 0.017932950105104182\n",
            " fold  2 \n",
            "train_loss: 0.2383767082963301 val_loss: 0.07052961240212123\n",
            "train_loss: 0.04661883764724801 val_loss: 0.03379653187261687\n",
            "train_loss: 0.02867980139411014 val_loss: 0.025696646215187177\n",
            "train_loss: 0.023481524873362934 val_loss: 0.022561362530622218\n",
            "train_loss: 0.02113289971822414 val_loss: 0.02095648729138904\n",
            "train_loss: 0.019718730336298115 val_loss: 0.019904210335678525\n",
            "train_loss: 0.018747645619230858 val_loss: 0.019381785144408543\n",
            "train_loss: 0.01812275186639981 val_loss: 0.018815375347104337\n",
            "train_loss: 0.017431785696712526 val_loss: 0.018511043654547796\n",
            "train_loss: 0.016905214947958786 val_loss: 0.01847168772170941\n",
            " fold  3 \n",
            "train_loss: 0.23169580085769945 val_loss: 0.0680190631084972\n",
            "train_loss: 0.045761520501928055 val_loss: 0.03309082633091344\n",
            "train_loss: 0.02843482321317213 val_loss: 0.025336378667917516\n",
            "train_loss: 0.023353669549459995 val_loss: 0.022249125979012914\n",
            "train_loss: 0.021004204935245754 val_loss: 0.020915854308340285\n",
            "train_loss: 0.01964768311143785 val_loss: 0.019792433103753462\n",
            "train_loss: 0.018702113926680624 val_loss: 0.01917938060230679\n",
            "train_loss: 0.017965047168073015 val_loss: 0.018752955831587315\n",
            "train_loss: 0.01732056489403265 val_loss: 0.018492003488871787\n",
            "train_loss: 0.01682740828508268 val_loss: 0.01815421858595477\n",
            " fold  4 \n",
            "train_loss: 0.23827471665066222 val_loss: 0.07045212470822865\n",
            "train_loss: 0.04676740438393925 val_loss: 0.033996205776929855\n",
            "train_loss: 0.028872685371965603 val_loss: 0.025774253118369315\n",
            "train_loss: 0.0235392181067795 val_loss: 0.02234482237448295\n",
            "train_loss: 0.021161914512892996 val_loss: 0.020667745421330135\n",
            "train_loss: 0.019843562951554424 val_loss: 0.019762169983651903\n",
            "train_loss: 0.018870516551955454 val_loss: 0.018977152183651924\n",
            "train_loss: 0.0181418533531436 val_loss: 0.01861218311306503\n",
            "train_loss: 0.01751166167061614 val_loss: 0.018340136855840683\n",
            "train_loss: 0.01691526839968519 val_loss: 0.017990259143213432\n",
            " fold  5 \n",
            "train_loss: 0.23606740326985068 val_loss: 0.06957054345144166\n",
            "train_loss: 0.04618367944182693 val_loss: 0.03357544417182604\n",
            "train_loss: 0.028592296714044136 val_loss: 0.02548872110330396\n",
            "train_loss: 0.023400409511573936 val_loss: 0.022441488173272874\n",
            "train_loss: 0.02106697476752426 val_loss: 0.02094946408437358\n",
            "train_loss: 0.019684145793966625 val_loss: 0.019768536918693118\n",
            "train_loss: 0.018778665477167004 val_loss: 0.01930427240828673\n",
            "train_loss: 0.018054629797520844 val_loss: 0.0188976533503996\n",
            "train_loss: 0.017412869516166225 val_loss: 0.01852958421740267\n",
            "train_loss: 0.016859779708033453 val_loss: 0.018142098871370155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:33:25,293] Trial 14 finished with value: 0.018142098871370155 and parameters: {'num_layer': 2, 'hidden_size': 4038, 'dropout': 0.10770604563853593, 'learning_rate': 1.993304030365116e-05}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.03363849580341923 val_loss: 0.01868396707706981\n",
            "train_loss: 0.018061741086505892 val_loss: 0.017704335920926597\n",
            "train_loss: 0.01729258114768975 val_loss: 0.017338577140536573\n",
            "train_loss: 0.016537667441087357 val_loss: 0.016973094083368778\n",
            "train_loss: 0.01591484758840955 val_loss: 0.016973312478512526\n",
            "train_loss: 0.015233160212528015 val_loss: 0.017051603852046862\n",
            "train_loss: 0.014755471211358688 val_loss: 0.017114581695447367\n",
            "train_loss: 0.01413895965864261 val_loss: 0.017032019845727418\n",
            "train_loss: 0.01363834096253782 val_loss: 0.017672182785140142\n",
            "train_loss: 0.013123964519658382 val_loss: 0.017202204196817346\n",
            " fold  2 \n",
            "train_loss: 0.034106590852573296 val_loss: 0.018979036973582372\n",
            "train_loss: 0.018105636863712814 val_loss: 0.01771507727810078\n",
            "train_loss: 0.017212521170090506 val_loss: 0.01809958792808983\n",
            "train_loss: 0.01671092696757852 val_loss: 0.017814649165504508\n",
            "train_loss: 0.016070322133600712 val_loss: 0.0174249450986584\n",
            "train_loss: 0.015450955996208864 val_loss: 0.017538754735141993\n",
            "train_loss: 0.014854229271303917 val_loss: 0.017202383786853816\n",
            "train_loss: 0.014383125726295553 val_loss: 0.017973162854711216\n",
            "train_loss: 0.013890890647535738 val_loss: 0.01762444308648507\n",
            "train_loss: 0.013418017947317465 val_loss: 0.01785002017600669\n",
            " fold  3 \n",
            "train_loss: 0.033440758109740586 val_loss: 0.01897986067665948\n",
            "train_loss: 0.018089578547717436 val_loss: 0.017965705133974552\n",
            "train_loss: 0.01727081620660813 val_loss: 0.017576344932119053\n",
            "train_loss: 0.01659159553979618 val_loss: 0.017508528609242704\n",
            "train_loss: 0.015992023772897497 val_loss: 0.01762730007370313\n",
            "train_loss: 0.015399031489547611 val_loss: 0.017183447670605447\n",
            "train_loss: 0.01490708739510265 val_loss: 0.017157888660828274\n",
            "train_loss: 0.014383231964556204 val_loss: 0.017475018381244607\n",
            "train_loss: 0.013869549876645855 val_loss: 0.01749826605535216\n",
            "train_loss: 0.013525405898690224 val_loss: 0.017896900677846536\n",
            " fold  4 \n",
            "train_loss: 0.03407454852392708 val_loss: 0.019041614710456796\n",
            "train_loss: 0.01824018855889638 val_loss: 0.018106390722095966\n",
            "train_loss: 0.01730298657861093 val_loss: 0.01756265800860193\n",
            "train_loss: 0.016549308008203905 val_loss: 0.01717372445596589\n",
            "train_loss: 0.01590381444607308 val_loss: 0.01724804989579651\n",
            "train_loss: 0.01527605828680638 val_loss: 0.01717184830663933\n",
            "train_loss: 0.014738474554125813 val_loss: 0.017551693134009838\n",
            "train_loss: 0.01428077042615716 val_loss: 0.01730655288944642\n",
            "train_loss: 0.01370647329621125 val_loss: 0.017392120013634365\n",
            "train_loss: 0.01314543983966544 val_loss: 0.01754626797305213\n",
            " fold  5 \n",
            "train_loss: 0.03369945604219169 val_loss: 0.018884584307670593\n",
            "train_loss: 0.018183133963063574 val_loss: 0.01778092302588953\n",
            "train_loss: 0.01727212448536918 val_loss: 0.017421883737875357\n",
            "train_loss: 0.016695219861424488 val_loss: 0.017612632364034653\n",
            "train_loss: 0.01611042476893551 val_loss: 0.017448675042639177\n",
            "train_loss: 0.015563742838044098 val_loss: 0.01725746298001872\n",
            "train_loss: 0.015005356004542631 val_loss: 0.01704975486629539\n",
            "train_loss: 0.014552937496615492 val_loss: 0.01734970748010609\n",
            "train_loss: 0.014011481965797535 val_loss: 0.017119832906044193\n",
            "train_loss: 0.013682022258855295 val_loss: 0.01739466583563222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:36:45,333] Trial 15 finished with value: 0.01704975486629539 and parameters: {'num_layer': 4, 'hidden_size': 3914, 'dropout': 0.22633017344380432, 'learning_rate': 0.0004001925820350279}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.5856528204420338 val_loss: 0.4657350894477632\n",
            "train_loss: 0.37716285042140796 val_loss: 0.30139420098728603\n",
            "train_loss: 0.24826741715272269 val_loss: 0.2019280335969395\n",
            "train_loss: 0.16986286337824835 val_loss: 0.14157222625282076\n",
            "train_loss: 0.1221928118687609 val_loss: 0.10468049595753352\n",
            "train_loss: 0.09244336317414822 val_loss: 0.08096822392609385\n",
            "train_loss: 0.07292198109022086 val_loss: 0.06525559764769343\n",
            "train_loss: 0.06009867325749086 val_loss: 0.0542386161784331\n",
            "train_loss: 0.051051011929909386 val_loss: 0.046749494348963104\n",
            "train_loss: 0.04446731858711312 val_loss: 0.04131027455959055\n",
            " fold  2 \n",
            "train_loss: 0.5942910477734994 val_loss: 0.4737155950731701\n",
            "train_loss: 0.3834346401086752 val_loss: 0.30645838379859924\n",
            "train_loss: 0.2512385795513789 val_loss: 0.20481947892242008\n",
            "train_loss: 0.1709410108733868 val_loss: 0.14299633850653967\n",
            "train_loss: 0.12243706711392471 val_loss: 0.1055089785820908\n",
            "train_loss: 0.09228028707962105 val_loss: 0.0816181202729543\n",
            "train_loss: 0.07288664136675821 val_loss: 0.06579092393318813\n",
            "train_loss: 0.060041805297352265 val_loss: 0.0555544083731042\n",
            "train_loss: 0.05091637364872124 val_loss: 0.047390260423223175\n",
            "train_loss: 0.04458066669927127 val_loss: 0.0421506700416406\n",
            " fold  3 \n",
            "train_loss: 0.5803943300160809 val_loss: 0.4608575883838866\n",
            "train_loss: 0.3738978898179704 val_loss: 0.29826432632075417\n",
            "train_loss: 0.2464773978876031 val_loss: 0.20064763890372384\n",
            "train_loss: 0.16898870230584906 val_loss: 0.14104417049222523\n",
            "train_loss: 0.1218138941820117 val_loss: 0.10359166852302021\n",
            "train_loss: 0.09195149511746738 val_loss: 0.08051126615868674\n",
            "train_loss: 0.07263174917602884 val_loss: 0.0652147386636999\n",
            "train_loss: 0.059853881543529205 val_loss: 0.054140022231472865\n",
            "train_loss: 0.05077446465366992 val_loss: 0.0466591107348601\n",
            "train_loss: 0.04436342420893303 val_loss: 0.04114645202126768\n",
            " fold  4 \n",
            "train_loss: 0.5768719151400138 val_loss: 0.45819708704948425\n",
            "train_loss: 0.37151778500149213 val_loss: 0.29813255700800156\n",
            "train_loss: 0.24511940464161444 val_loss: 0.20046191165844598\n",
            "train_loss: 0.16823925509832907 val_loss: 0.14082814918624031\n",
            "train_loss: 0.12110943044873251 val_loss: 0.10421486405862702\n",
            "train_loss: 0.09182080000207044 val_loss: 0.08117109454340404\n",
            "train_loss: 0.07255729209577692 val_loss: 0.06529848732882076\n",
            "train_loss: 0.059774457868458565 val_loss: 0.054692628896898694\n",
            "train_loss: 0.05084422924488351 val_loss: 0.047205697952045336\n",
            "train_loss: 0.04441349945314552 val_loss: 0.041529987628261246\n",
            " fold  5 \n",
            "train_loss: 0.5735330972550572 val_loss: 0.4552491058905919\n",
            "train_loss: 0.3681894899278447 val_loss: 0.2947415891620848\n",
            "train_loss: 0.2418422754044118 val_loss: 0.19779448045624626\n",
            "train_loss: 0.16553901600233023 val_loss: 0.13905255082580778\n",
            "train_loss: 0.11915807156027228 val_loss: 0.102882393118408\n",
            "train_loss: 0.09020155544082324 val_loss: 0.07983623113897112\n",
            "train_loss: 0.07148106976587704 val_loss: 0.06504979812436634\n",
            "train_loss: 0.05899892099525617 val_loss: 0.05465813995235496\n",
            "train_loss: 0.05016501579919587 val_loss: 0.046679160454206996\n",
            "train_loss: 0.043913517471240913 val_loss: 0.041426479195555053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:38:48,571] Trial 16 finished with value: 0.041426479195555053 and parameters: {'num_layer': 2, 'hidden_size': 423, 'dropout': 0.18690111600519338, 'learning_rate': 2.678986599916121e-05}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.04333103114766055 val_loss: 0.01917903259810474\n",
            "train_loss: 0.018054589750650135 val_loss: 0.018129721180432372\n",
            "train_loss: 0.016449657228329907 val_loss: 0.01735695746416847\n",
            "train_loss: 0.015207872739520626 val_loss: 0.017150089455147583\n",
            "train_loss: 0.014099098276346922 val_loss: 0.017242641602125432\n",
            "train_loss: 0.013208319129341322 val_loss: 0.017023402731865644\n",
            "train_loss: 0.012389272113965042 val_loss: 0.017214753665030003\n",
            "train_loss: 0.01163320648956342 val_loss: 0.01731832677291499\n",
            "train_loss: 0.010947373283999985 val_loss: 0.01693940002264248\n",
            "train_loss: 0.010595767380858677 val_loss: 0.016962301244752273\n",
            " fold  2 \n",
            "train_loss: 0.04358168546974227 val_loss: 0.01950059396525224\n",
            "train_loss: 0.018078128442816113 val_loss: 0.018264322852094967\n",
            "train_loss: 0.016440208245446716 val_loss: 0.017857529533406098\n",
            "train_loss: 0.015178256998837425 val_loss: 0.01752246507546968\n",
            "train_loss: 0.014144606604848219 val_loss: 0.017267107342680294\n",
            "train_loss: 0.013267787038416102 val_loss: 0.017214639836715326\n",
            "train_loss: 0.012332035451317612 val_loss: 0.01707078930404451\n",
            "train_loss: 0.011734711465196333 val_loss: 0.017099242760903306\n",
            "train_loss: 0.010975066471197035 val_loss: 0.01738843445976575\n",
            "train_loss: 0.010480590974507124 val_loss: 0.017215884238895442\n",
            " fold  3 \n",
            "train_loss: 0.04352041589015204 val_loss: 0.01940247333712048\n",
            "train_loss: 0.018059648735367733 val_loss: 0.018237069559593994\n",
            "train_loss: 0.016531657272328932 val_loss: 0.017533443195538387\n",
            "train_loss: 0.015172253477562597 val_loss: 0.01732481565947334\n",
            "train_loss: 0.01414956209803189 val_loss: 0.017069707210693095\n",
            "train_loss: 0.013235383012426504 val_loss: 0.0171747501525614\n",
            "train_loss: 0.012374869608522757 val_loss: 0.017218685243278742\n",
            "train_loss: 0.011580725045253834 val_loss: 0.017115928025709257\n",
            "train_loss: 0.010872885497097952 val_loss: 0.017349649738106463\n",
            "train_loss: 0.010502406672669062 val_loss: 0.01730857530815734\n",
            " fold  4 \n",
            "train_loss: 0.0432901613994677 val_loss: 0.01925830574085315\n",
            "train_loss: 0.018070623157141003 val_loss: 0.01808273947487275\n",
            "train_loss: 0.016487763573726017 val_loss: 0.017422335274103615\n",
            "train_loss: 0.015178985461808634 val_loss: 0.01717796316370368\n",
            "train_loss: 0.014115297863178927 val_loss: 0.017148638971977763\n",
            "train_loss: 0.013152816022435823 val_loss: 0.017290944854418438\n",
            "train_loss: 0.012340717064891604 val_loss: 0.017094615898612473\n",
            "train_loss: 0.01154094574082157 val_loss: 0.01701154549502664\n",
            "train_loss: 0.011015680463363728 val_loss: 0.017152782012191083\n",
            "train_loss: 0.010282158048526533 val_loss: 0.017308443474272888\n",
            " fold  5 \n",
            "train_loss: 0.04280921604916237 val_loss: 0.01939738045136134\n",
            "train_loss: 0.018078150355891474 val_loss: 0.01817405741247866\n",
            "train_loss: 0.016442818445679935 val_loss: 0.01788929756730795\n",
            "train_loss: 0.01517590362092723 val_loss: 0.01737089900092946\n",
            "train_loss: 0.014123014228391474 val_loss: 0.017423898809485965\n",
            "train_loss: 0.013194964739723482 val_loss: 0.016971057332638238\n",
            "train_loss: 0.012331634429215954 val_loss: 0.017069665973799095\n",
            "train_loss: 0.011685590441946102 val_loss: 0.01691754798715313\n",
            "train_loss: 0.01091322325763927 val_loss: 0.017171067496140797\n",
            "train_loss: 0.010419828931976488 val_loss: 0.0172064958864616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:40:42,284] Trial 17 finished with value: 0.01691754798715313 and parameters: {'num_layer': 1, 'hidden_size': 1990, 'dropout': 0.30236857647578136, 'learning_rate': 0.0004529221755793316}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.04480706496785084 val_loss: 0.01965240202844143\n",
            "train_loss: 0.018951686267889498 val_loss: 0.018580824240214296\n",
            "train_loss: 0.017851890361719372 val_loss: 0.01774555134276549\n",
            "train_loss: 0.01723722595235576 val_loss: 0.017622331985168986\n",
            "train_loss: 0.016857454155072355 val_loss: 0.017353243608441617\n",
            "train_loss: 0.016451478348639997 val_loss: 0.017123380882872477\n",
            "train_loss: 0.01602077712237403 val_loss: 0.017353584420763783\n",
            "train_loss: 0.015656640908370417 val_loss: 0.01718384638014767\n",
            "train_loss: 0.015278107790357393 val_loss: 0.017141662538051605\n",
            "train_loss: 0.014787716075670029 val_loss: 0.017187018775277667\n",
            " fold  2 \n",
            "train_loss: 0.04436535760760307 val_loss: 0.019786341529753473\n",
            "train_loss: 0.01886998097398791 val_loss: 0.018504344547788303\n",
            "train_loss: 0.01784546678021982 val_loss: 0.01807844576736291\n",
            "train_loss: 0.017158294278804376 val_loss: 0.018108613478640716\n",
            "train_loss: 0.016781424313945616 val_loss: 0.017725515593257215\n",
            "train_loss: 0.016295311648560608 val_loss: 0.017632598678270977\n",
            "train_loss: 0.015828855308717575 val_loss: 0.01733517274260521\n",
            "train_loss: 0.015404817789954983 val_loss: 0.017551095328397222\n",
            "train_loss: 0.015022043874352306 val_loss: 0.017256370642118983\n",
            "train_loss: 0.014589806674453228 val_loss: 0.017454685022433598\n",
            " fold  3 \n",
            "train_loss: 0.04504017291617566 val_loss: 0.019910566922691133\n",
            "train_loss: 0.019012668358998886 val_loss: 0.018360218084934685\n",
            "train_loss: 0.017892477554741545 val_loss: 0.01780898863863614\n",
            "train_loss: 0.017279103735758774 val_loss: 0.01819289092802339\n",
            "train_loss: 0.016837840683866238 val_loss: 0.01735860114503238\n",
            "train_loss: 0.016374184702779505 val_loss: 0.017304982834806044\n",
            "train_loss: 0.015974807924172586 val_loss: 0.017584829694694944\n",
            "train_loss: 0.015602897812166939 val_loss: 0.0172170867315597\n",
            "train_loss: 0.01521555779069878 val_loss: 0.017559267166588042\n",
            "train_loss: 0.014759723083588524 val_loss: 0.017395897199296288\n",
            " fold  4 \n",
            "train_loss: 0.04442299490767545 val_loss: 0.01959479372534487\n",
            "train_loss: 0.019060480806544638 val_loss: 0.01841486297133896\n",
            "train_loss: 0.017925147000916193 val_loss: 0.01767954556271434\n",
            "train_loss: 0.017319420690013878 val_loss: 0.0178616504288382\n",
            "train_loss: 0.01679242761346741 val_loss: 0.017423539008531306\n",
            "train_loss: 0.016412272205765265 val_loss: 0.01724426593217585\n",
            "train_loss: 0.016114570132956123 val_loss: 0.017379128529379766\n",
            "train_loss: 0.015594581364775482 val_loss: 0.017284977146320872\n",
            "train_loss: 0.015154444146901369 val_loss: 0.017498994039164648\n",
            "train_loss: 0.014846047319039919 val_loss: 0.017396807049711544\n",
            " fold  5 \n",
            "train_loss: 0.04599877604809792 val_loss: 0.020097187927199736\n",
            "train_loss: 0.01905852016331493 val_loss: 0.018408896608485117\n",
            "train_loss: 0.01794830317595515 val_loss: 0.018008829922311835\n",
            "train_loss: 0.017336134069963642 val_loss: 0.017758578889899783\n",
            "train_loss: 0.01692189853908359 val_loss: 0.017704065578679245\n",
            "train_loss: 0.016448911009491352 val_loss: 0.01728978670305676\n",
            "train_loss: 0.016062631440497396 val_loss: 0.0172602325781352\n",
            "train_loss: 0.015639963607047346 val_loss: 0.01749600677026643\n",
            "train_loss: 0.015282457215252562 val_loss: 0.017234986130562093\n",
            "train_loss: 0.015018454773108597 val_loss: 0.0173868742874927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:43:14,867] Trial 18 finished with value: 0.017234986130562093 and parameters: {'num_layer': 5, 'hidden_size': 1787, 'dropout': 0.316428024102369, 'learning_rate': 0.00043538971847463726}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " fold  1 \n",
            "train_loss: 0.02978992919721033 val_loss: 0.02009709044877026\n",
            "train_loss: 0.019737743887294462 val_loss: 0.019398805271420214\n",
            "train_loss: 0.018983170687072518 val_loss: 0.01851717082576619\n",
            "train_loss: 0.01863463698764858 val_loss: 0.01818245856298341\n",
            "train_loss: 0.018496409706447437 val_loss: 0.018281709868460894\n",
            "train_loss: 0.01837314075479905 val_loss: 0.018558475499351818\n",
            "train_loss: 0.018412175365602194 val_loss: 0.017958281096071005\n",
            "train_loss: 0.018441458941315828 val_loss: 0.018352523342602782\n",
            "train_loss: 0.01846834774925441 val_loss: 0.01837344912605153\n",
            "train_loss: 0.01839180171246762 val_loss: 0.018632179747025173\n",
            " fold  2 \n",
            "train_loss: 0.029388575517720936 val_loss: 0.020360642733673256\n",
            "train_loss: 0.01947236443073421 val_loss: 0.01900698451532258\n",
            "train_loss: 0.018816998464635748 val_loss: 0.01906106931467851\n",
            "train_loss: 0.018716178024592606 val_loss: 0.018713584169745445\n",
            "train_loss: 0.018366377152826473 val_loss: 0.01864211613105403\n",
            "train_loss: 0.01831304626809298 val_loss: 0.018412478630327515\n",
            "train_loss: 0.018305601498139076 val_loss: 0.018624289996094175\n",
            "train_loss: 0.018365964590423348 val_loss: 0.01849149540066719\n",
            "train_loss: 0.01837218844372293 val_loss: 0.018393453469292984\n",
            "train_loss: 0.018383772024695856 val_loss: 0.018660490194128618\n",
            " fold  3 \n",
            "train_loss: 0.029621365331653236 val_loss: 0.020342435480819807\n",
            "train_loss: 0.019476453213534063 val_loss: 0.01868739046363367\n",
            "train_loss: 0.01876740905361763 val_loss: 0.018682425427767966\n",
            "train_loss: 0.018518963504744614 val_loss: 0.01854457137071424\n",
            "train_loss: 0.018316322525936193 val_loss: 0.018417199245757528\n",
            "train_loss: 0.018320787650789472 val_loss: 0.018594129321475823\n",
            "train_loss: 0.018390838178279606 val_loss: 0.018512166519131925\n",
            "train_loss: 0.018383885301865528 val_loss: 0.01844758116122749\n",
            "train_loss: 0.01843518848814394 val_loss: 0.018664497675167188\n",
            "train_loss: 0.018461735385969496 val_loss: 0.018716644392245345\n",
            " fold  4 \n",
            "train_loss: 0.0292312399605694 val_loss: 0.019970736569828458\n",
            "train_loss: 0.019417225403468245 val_loss: 0.019063895671731897\n",
            "train_loss: 0.018892179438979296 val_loss: 0.018518354950679675\n",
            "train_loss: 0.018709043789978907 val_loss: 0.01865101678089963\n",
            "train_loss: 0.018438157490522102 val_loss: 0.01827313740634256\n",
            "train_loss: 0.01834842235362832 val_loss: 0.01868258747789595\n",
            "train_loss: 0.018369169527853745 val_loss: 0.018468849050501984\n",
            "train_loss: 0.018393646574754646 val_loss: 0.017980073164734576\n",
            "train_loss: 0.01846393482570631 val_loss: 0.018534309644665983\n",
            "train_loss: 0.018462462377720985 val_loss: 0.01848747653679715\n",
            " fold  5 \n",
            "train_loss: 0.02967932640804329 val_loss: 0.02056443349768718\n",
            "train_loss: 0.019466109735810238 val_loss: 0.019161286246445444\n",
            "train_loss: 0.018771131752409798 val_loss: 0.01868084900909\n",
            "train_loss: 0.018581317143811695 val_loss: 0.018711246757043734\n",
            "train_loss: 0.018494624563533325 val_loss: 0.018637252143687673\n",
            "train_loss: 0.018314857933454325 val_loss: 0.018270036826531093\n",
            "train_loss: 0.018352351806032053 val_loss: 0.018390089583893616\n",
            "train_loss: 0.018323492306028154 val_loss: 0.018319936883118417\n",
            "train_loss: 0.018411006441042908 val_loss: 0.01857259486698442\n",
            "train_loss: 0.01839165722924298 val_loss: 0.018560038569072883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 06:45:28,621] Trial 19 finished with value: 0.018270036826531093 and parameters: {'num_layer': 3, 'hidden_size': 1419, 'dropout': 0.4523398838430158, 'learning_rate': 0.00401867937543168}. Best is trial 9 with value: 0.016764250118285418.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best_trial:\n",
            "FrozenTrial(number=9, value=0.016764250118285418, datetime_start=datetime.datetime(2020, 9, 26, 6, 20, 33, 402214), datetime_complete=datetime.datetime(2020, 9, 26, 6, 22, 47, 319401), params={'num_layer': 3, 'hidden_size': 1894, 'dropout': 0.15739205763019304, 'learning_rate': 0.0011353568729844193}, distributions={'num_layer': IntUniformDistribution(high=8, low=1, step=1), 'hidden_size': IntUniformDistribution(high=4096, low=16, step=1), 'dropout': UniformDistribution(high=0.7, low=0.1), 'learning_rate': LogUniformDistribution(high=0.01, low=1e-06)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=9, state=TrialState.COMPLETE)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}