{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moa_two.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_zNSR68YTYs",
        "outputId": "c05cef1c-5c74-4b00-c6f9-d74a6884bf3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOd8RhyuZzp2"
      },
      "source": [
        "!cp /content/drive/\"My Drive\"/kaggle/moa/lish-moa.zip /content/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSua2cgfd2hn",
        "outputId": "034aadcf-56a9-464a-f60b-89fa0b84aaef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!unzip lish-moa.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  lish-moa.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_features.csv       \n",
            "  inflating: train_features.csv      \n",
            "  inflating: train_targets_nonscored.csv  \n",
            "  inflating: train_targets_scored.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB9siuMCJ5pd",
        "outputId": "d7810b94-0634-4b2c-9d97-85b2dc386df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install pip install iterative-stratification"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Collecting install\n",
            "  Downloading https://files.pythonhosted.org/packages/41/cf/e3e6b4d494051c07261cae8c403f0f0d0cedad43d980e5255f2c88fd5edf/install-1.3.3-py3-none-any.whl\n",
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.16.0)\n",
            "Installing collected packages: install, iterative-stratification\n",
            "Successfully installed install-1.3.3 iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUpXeUlWeXpI"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "        \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from IPython.display import clear_output\n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter1d   ## smoother\n",
        "from tqdm.notebook import tqdm, tnrange\n",
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 15, 7\n",
        "\n",
        "CGREEN  = '\\33[32m'\n",
        "CBLUE =  '\\033[34m'\n",
        "CRED = '\\033[1;31m'\n",
        "CEND  = '\\33[0m'\n",
        "\n",
        "def seed_everything(seed=1903):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    print(\"seed kar diya\")\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0UcuWgBr0Hn",
        "outputId": "ce383aea-b8ff-4bb5-976f-dd6f944ad0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "else:\n",
        "    device='cpu'\n",
        "    \n",
        "device\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZhAwZ3hr45u"
      },
      "source": [
        "train_features = pd.read_csv('train_features.csv')\n",
        "train_targets = pd.read_csv('train_targets_scored.csv')\n",
        "test_features = pd.read_csv('test_features.csv')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWH98yYwp-w"
      },
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    return df\n",
        "\n",
        "train = preprocess(train_features)\n",
        "test = preprocess(test_features)\n",
        "\n",
        "del train_targets['sig_id']\n",
        "\n",
        "target = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n",
        "train = train.loc[train['cp_type']==0].reset_index(drop=True)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0AtYxrP0kpp"
      },
      "source": [
        "# top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "#         18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "#         32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "#         47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "#         61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "#         74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "#         89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "#        102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "#        115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "#        129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "#        144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "#        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "#        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "#        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "#        198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "#        213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "#        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "#        240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "#        254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "#        267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "#        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "#        295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "#        310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "#        324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "#        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "#        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "#        363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "#        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "#        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "#        405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "#        419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "#        432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "#        447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "#        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "#        476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "#        490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "#        506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "#        522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "#        538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "#        552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "#        571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "#        586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "#        600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "#        618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "#        631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "#        645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "#        660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "#        673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "#        686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "#        701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "#        718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "#        733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "#        748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "#        762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "#        775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "#        789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "#        804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "#        821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "#        837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "#        854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "#        870, 871, 872, 873, 874]\n",
        "\n",
        "# all_columns = train.columns\n",
        "# train=train[all_columns[top_features]]\n",
        "# test = test[all_columns[top_features]]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBFXoVhT5X4",
        "outputId": "2b570c6f-2c0f-4223-f76d-4e222caadcc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21948, 876), (3982, 876))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O52yoAOM14k4"
      },
      "source": [
        "train = train.values[:,1:].astype(np.float)\n",
        "target = target.values\n",
        "test = test.values[:,1:].astype(np.float)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIzwrLTUll_",
        "outputId": "ed4274b1-0dd9-4564-f238-bf65c50b50a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test[0]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.000e+00,  2.400e+01,  0.000e+00, -5.458e-01,  1.306e-01,\n",
              "       -5.135e-01,  4.408e-01,  1.550e+00, -1.644e-01, -2.140e-01,\n",
              "        2.221e-01, -3.260e-01,  1.939e+00, -2.305e-01, -3.670e-01,\n",
              "        1.304e+00,  1.461e+00,  4.300e-03,  6.816e-01, -2.304e-01,\n",
              "       -6.350e-02, -2.030e-01, -6.821e-01, -6.242e-01,  1.297e-01,\n",
              "       -3.380e-02,  3.372e-01,  2.254e-01,  4.795e-01,  7.642e-01,\n",
              "        6.638e-01, -2.480e-01, -1.183e-01, -4.847e-01, -1.790e-02,\n",
              "       -8.204e-01, -5.296e-01, -1.507e+00, -1.440e-02,  4.448e-01,\n",
              "        5.753e-01, -2.993e-01, -9.532e-01, -1.443e-01, -1.088e+00,\n",
              "        2.080e-01,  5.670e-01,  1.119e+00, -4.550e-02,  2.789e-01,\n",
              "       -1.023e+00, -6.231e-01, -6.682e-01, -7.180e-02, -1.524e+00,\n",
              "        9.117e-01,  6.883e-01, -1.073e-01, -1.134e-01,  4.536e-01,\n",
              "        5.514e+00,  3.271e-01,  1.388e-01, -2.321e-01,  1.517e+00,\n",
              "        5.460e-02,  9.043e-01, -3.088e-01,  1.536e+00,  1.511e-01,\n",
              "       -3.711e-01, -1.950e-02, -4.992e-01,  1.208e+00, -3.614e-01,\n",
              "        7.978e-01,  3.076e-01,  5.958e-01,  4.006e-01,  3.546e-01,\n",
              "       -4.230e-02,  3.159e-01,  6.640e-02,  2.195e-01, -5.896e-01,\n",
              "       -4.285e-01,  1.485e-01, -8.791e-01,  1.925e-01,  6.859e-01,\n",
              "       -2.048e-01, -3.180e-02, -4.575e-01, -6.830e-02, -5.259e-01,\n",
              "        1.055e-01,  1.432e+00, -2.144e-01,  8.011e-01, -1.894e-01,\n",
              "       -2.883e-01, -1.824e-01, -2.639e-01, -2.179e-01,  2.714e-01,\n",
              "        4.930e-02, -6.086e-01,  1.989e-01, -3.487e-01, -5.663e-01,\n",
              "       -5.690e-02, -6.374e-01,  2.824e-01,  6.126e-01, -5.231e-01,\n",
              "       -1.132e-01, -1.326e+00,  3.753e-01, -2.650e-02, -4.480e-01,\n",
              "       -6.728e-01, -1.847e-01, -7.800e-02,  7.650e-02, -8.591e-01,\n",
              "       -1.643e-01, -6.818e-01, -5.565e-01,  4.931e-01,  3.570e-02,\n",
              "        2.697e-01, -2.734e-01, -1.996e-01,  1.150e-01, -7.040e-01,\n",
              "       -1.198e-01,  1.137e+00, -2.400e-02, -1.716e-01,  6.704e-01,\n",
              "        1.040e-01, -2.755e-01,  1.232e-01, -5.820e-02, -1.439e+00,\n",
              "        1.845e-01, -6.218e-01,  1.508e-01, -1.811e-01, -6.889e-01,\n",
              "       -8.700e-02, -3.517e-01, -6.284e-01, -5.490e-02, -1.289e+00,\n",
              "        2.110e-02,  4.212e-01,  3.191e-01,  6.190e-02,  7.241e-01,\n",
              "       -3.875e-01, -1.693e-01,  3.896e+00, -9.690e-02, -6.108e-01,\n",
              "        1.826e+00,  4.870e-01,  0.000e+00, -9.273e-01, -1.015e-01,\n",
              "       -4.507e-01,  3.072e-01, -6.619e-01, -4.487e-01, -6.100e-01,\n",
              "        6.191e-01, -1.309e+00, -2.377e-01, -2.694e-01, -1.178e+00,\n",
              "        8.030e-01,  1.462e-01, -7.103e-01,  6.697e-01,  1.098e+00,\n",
              "        4.867e-01,  3.775e-01,  3.072e-01, -1.895e+00,  4.178e-01,\n",
              "       -9.293e-01, -3.731e-01, -6.134e-01,  7.074e-01,  2.200e-03,\n",
              "        3.032e-01, -2.980e-01, -4.609e-01, -5.940e-02,  4.191e-01,\n",
              "       -4.828e-01, -6.677e-01, -1.641e-01, -4.933e-01, -9.820e-02,\n",
              "       -3.372e-01,  4.280e-02, -4.688e-01,  1.760e-02, -7.300e-02,\n",
              "        9.710e-01,  5.980e-02, -1.264e-01,  3.100e-03, -1.062e+00,\n",
              "       -3.490e-02, -5.612e-01,  1.487e-01, -8.714e-01,  6.910e-02,\n",
              "        7.826e-01,  3.700e-03, -3.187e-01,  1.458e-01, -3.310e-01,\n",
              "        3.849e-01,  7.420e-02, -5.776e-01, -1.279e-01,  3.360e-01,\n",
              "       -5.700e-02, -1.019e-01,  7.250e-02, -5.949e-01, -8.090e-02,\n",
              "       -1.403e-01,  2.490e-02, -1.198e+00,  2.334e-01,  4.490e-02,\n",
              "       -3.738e-01, -5.586e-01,  3.611e-01, -3.377e-01, -4.988e-01,\n",
              "       -2.340e-02,  2.862e-01, -5.045e-01, -1.536e-01,  4.571e-01,\n",
              "       -6.564e-01, -4.593e-01,  6.262e-01,  8.680e-01,  1.495e-01,\n",
              "        2.756e-01, -5.701e-01,  4.737e-01, -2.250e-02, -3.587e-01,\n",
              "        1.099e+00,  4.025e-01, -3.774e-01,  4.148e-01, -3.530e-02,\n",
              "        8.050e-01,  6.220e-02, -2.243e-01,  5.510e-02,  3.546e-01,\n",
              "        3.073e-01, -1.272e+00, -7.601e-01,  6.381e-01,  2.111e-01,\n",
              "        3.080e-01,  1.390e-01,  3.459e-01,  1.771e-01, -6.110e-02,\n",
              "       -4.939e-01, -5.676e-01,  7.296e-01, -6.017e-01,  8.580e-02,\n",
              "        7.311e-01, -1.132e-01, -2.629e-01, -9.445e-01,  3.413e-01,\n",
              "        4.060e-02,  3.288e-01, -1.028e-01, -7.589e-01, -1.435e-01,\n",
              "        7.704e-01,  8.142e-01,  2.594e-01, -7.858e-01, -4.320e-01,\n",
              "        3.547e-01, -7.380e-02, -1.220e-01,  3.460e-01, -7.129e-01,\n",
              "        1.482e+00, -1.176e+00,  1.499e-01,  3.605e-01, -5.021e-01,\n",
              "       -7.050e-02,  6.973e-01, -1.797e-01,  6.021e-01, -1.776e-01,\n",
              "       -6.140e-01,  1.094e-01,  2.094e-01, -5.581e-01,  2.024e-01,\n",
              "        1.203e+00, -7.400e-03,  2.025e-01,  6.084e-01,  3.835e-01,\n",
              "        4.560e-02, -1.881e-01, -6.447e-01, -1.200e-03, -7.594e-01,\n",
              "       -2.600e-01, -6.551e-01, -8.438e-01, -6.810e-01, -3.330e-01,\n",
              "       -3.900e-02,  6.525e-01, -1.123e-01, -3.525e-01, -1.835e-01,\n",
              "        2.920e-02, -6.036e-01,  2.990e-02,  8.380e-02, -1.367e+00,\n",
              "        7.818e-01, -3.083e-01, -3.264e-01,  9.198e-01,  4.456e-01,\n",
              "       -7.657e-01,  4.219e-01,  6.210e-02, -6.513e-01,  4.303e-01,\n",
              "        2.050e-01, -4.201e-01, -7.182e-01, -5.390e-01,  6.156e-01,\n",
              "        2.650e-01, -5.990e-01,  2.606e-01,  5.690e-02,  2.566e-01,\n",
              "        1.705e-01, -3.812e-01, -4.153e-01,  3.900e-02,  4.209e-01,\n",
              "        7.495e-01, -3.348e-01, -1.594e-01, -1.357e+00, -7.585e-01,\n",
              "       -1.150e-02, -4.360e-02,  3.285e-01, -1.394e+00, -3.873e-01,\n",
              "       -1.266e+00, -5.460e-01, -4.930e-02,  3.700e-02,  3.473e-01,\n",
              "       -7.707e-01, -4.632e-01, -1.295e+00,  3.700e-03,  1.054e-01,\n",
              "       -2.929e-01, -4.631e-01,  6.376e-01, -2.396e-01,  5.041e-01,\n",
              "       -7.027e-01, -5.525e-01, -3.828e-01,  4.479e-01,  2.796e-01,\n",
              "       -5.900e-03,  5.008e-01, -2.780e-02,  6.600e-02,  1.993e-01,\n",
              "        9.150e-02, -4.406e-01,  4.287e-01,  1.473e-01,  1.116e+00,\n",
              "       -1.548e-01, -2.740e-02, -2.880e-01, -1.079e-01,  2.947e-01,\n",
              "        1.005e-01, -4.530e-01,  9.243e-01,  1.713e-01, -3.052e-01,\n",
              "       -8.384e-01, -1.243e+00, -3.386e-01,  5.253e-01,  6.258e-01,\n",
              "        3.446e-01,  2.425e-01,  1.177e-01, -4.574e-01, -9.623e-01,\n",
              "        3.540e-02,  1.591e-01, -2.087e-01,  1.024e+00,  8.676e-01,\n",
              "       -1.207e-01, -5.978e-01, -5.738e-01,  5.760e-02,  6.148e-01,\n",
              "       -8.994e-01, -3.811e+00,  7.020e-02,  5.266e-01, -4.220e-01,\n",
              "        5.094e-01,  9.360e-02, -5.240e-02, -5.585e-01, -3.120e-02,\n",
              "        2.531e-01, -6.576e-01,  1.479e-01,  4.633e-01, -3.459e-01,\n",
              "        1.292e-01,  1.252e-01,  1.436e-01,  8.985e-01,  4.895e-01,\n",
              "        1.965e-01, -4.674e-01, -2.858e-01,  1.140e-02, -1.309e-01,\n",
              "       -2.780e-02,  3.749e-01, -7.244e-01,  2.483e+00,  7.560e-02,\n",
              "       -7.980e-01, -9.390e-02,  5.954e-01,  8.513e-01,  6.700e-03,\n",
              "       -9.830e-01,  4.002e-01, -6.682e-01, -4.560e-01, -3.695e-01,\n",
              "        9.696e-01, -1.305e+00, -5.911e-01, -5.468e-01, -4.890e-02,\n",
              "        1.141e-01,  2.952e-01,  1.519e-01,  1.026e+00, -3.965e-01,\n",
              "       -1.523e-01,  1.040e-01,  3.784e-01, -3.869e+00, -2.128e-01,\n",
              "        6.976e-01, -8.031e-01,  8.640e-02, -1.995e-01,  4.001e-01,\n",
              "        7.109e-01, -1.039e-01, -1.159e-01, -5.651e-01,  5.466e-01,\n",
              "       -4.047e-01, -9.160e-02,  2.615e+00, -1.118e+00, -8.826e-01,\n",
              "       -6.650e-01, -5.513e-01, -4.564e-01,  1.682e-01,  3.419e-01,\n",
              "       -1.278e+00,  4.136e-01,  1.536e+00,  5.137e-01, -1.096e-01,\n",
              "       -3.788e-01, -2.440e-02,  5.912e-01,  4.960e-02,  1.046e+00,\n",
              "        7.510e-02, -1.003e-01,  3.494e-01, -3.380e-02,  2.378e-01,\n",
              "       -2.900e-03, -2.160e-01, -8.479e-01, -2.562e-01,  1.232e+00,\n",
              "        3.017e+00,  1.027e-01, -2.678e-01,  7.378e-01,  2.993e-01,\n",
              "        4.371e-01, -1.031e+00,  1.260e-02,  4.790e-01, -2.875e-01,\n",
              "        9.420e-02,  5.730e-02,  8.800e-03,  1.014e-01, -2.154e-01,\n",
              "       -7.220e-02,  3.908e-01,  5.898e-01,  0.000e+00,  3.960e-02,\n",
              "       -3.784e-01,  4.570e-02, -1.074e-01, -7.520e-02,  6.403e-01,\n",
              "       -2.266e-01, -5.572e-01, -1.935e-01, -7.045e-01, -1.198e+00,\n",
              "       -2.057e-01,  5.917e-01, -2.139e-01,  9.300e-02, -3.970e-01,\n",
              "        9.619e-01,  6.912e-01,  4.967e-01,  2.612e-01,  4.376e-01,\n",
              "       -7.566e-01,  5.510e-02,  7.150e-02, -3.162e-01,  3.900e-03,\n",
              "       -1.909e-01, -1.037e-01,  1.821e-01, -1.362e-01, -2.779e-01,\n",
              "       -7.083e-01,  2.219e-01,  5.008e-01, -1.013e+00,  2.132e-01,\n",
              "       -5.939e-01,  5.770e-01, -1.580e-02,  5.214e-01,  3.904e-01,\n",
              "       -6.543e-01, -3.280e-01, -2.463e-01, -3.349e-01,  3.322e-01,\n",
              "        1.926e-01,  4.784e-01, -5.664e-01, -8.162e-01, -2.730e-01,\n",
              "        1.227e+00, -4.260e-01,  4.403e-01, -3.140e-02, -3.309e-01,\n",
              "        2.159e-01, -3.400e-02, -4.436e-01, -2.808e-01, -4.310e-01,\n",
              "        9.472e-01,  1.939e-01,  9.126e-01, -3.923e-01,  3.388e-01,\n",
              "       -5.845e-01,  8.708e-01, -2.157e-01, -8.292e-01,  3.505e-01,\n",
              "       -9.320e-02, -9.391e-01, -5.305e-01,  2.982e-01, -7.674e-01,\n",
              "        3.561e-01, -1.349e-01, -8.630e-01, -4.088e-01, -3.082e-01,\n",
              "       -1.748e-01,  4.721e-01, -1.761e+00, -2.192e-01, -1.031e+00,\n",
              "       -3.871e+00,  4.570e-01,  6.811e-01,  2.690e-01, -2.164e-01,\n",
              "       -3.763e-01,  2.743e-01,  5.501e-01,  8.568e-01, -1.401e-01,\n",
              "        2.484e-01, -2.020e-01, -4.311e-01, -5.374e-01, -2.312e-01,\n",
              "       -3.450e-02,  5.360e-02, -3.088e-01,  1.382e-01, -3.359e-01,\n",
              "       -1.939e-01,  6.000e-01, -5.151e-01, -1.644e-01, -4.798e-01,\n",
              "        6.302e-01,  1.566e-01,  6.392e-01, -4.902e-01,  4.500e-02,\n",
              "        4.982e-01, -2.007e-01, -8.471e-01,  4.715e-01,  8.430e-02,\n",
              "       -2.759e-01,  1.332e+00,  7.990e-01, -3.526e-01, -5.630e-02,\n",
              "        4.823e-01, -6.840e-02,  4.468e-01,  6.035e-01, -2.222e-01,\n",
              "        1.082e+00,  9.628e-01, -1.111e-01, -7.472e-01, -1.787e-01,\n",
              "        3.266e-01,  3.140e-02, -6.898e-01, -6.540e-02,  1.999e-01,\n",
              "        4.851e-01,  7.606e-01, -3.829e-01,  1.008e+00, -7.958e-01,\n",
              "        3.377e-01, -4.797e-01,  1.527e+00,  3.701e-01,  2.519e-01,\n",
              "        3.638e-01,  1.273e+00,  2.820e-02, -1.108e+00, -3.268e-01,\n",
              "       -1.288e-01, -1.033e+00,  1.301e+00, -5.638e-01,  8.870e-01,\n",
              "       -5.232e-01,  5.211e-01,  2.921e-01, -1.637e-01, -2.540e-01,\n",
              "       -7.310e-02, -1.133e-01,  4.527e-01,  4.927e-01,  2.469e-01,\n",
              "       -4.983e-01,  2.334e-01, -8.228e-01, -4.151e-01,  1.677e-01,\n",
              "       -8.603e-01, -2.748e-01,  2.446e+00,  1.269e+00,  4.310e-01,\n",
              "        1.000e-04,  6.490e-02, -5.460e-02, -3.350e-02, -7.971e-01,\n",
              "       -3.877e-01, -4.143e-01, -6.280e-02, -9.469e-01,  7.043e-01,\n",
              "       -1.803e+00, -4.952e-01, -7.419e-01, -5.549e-01,  6.375e-01,\n",
              "        5.490e-01,  4.600e-03,  9.110e-02,  5.982e-01,  6.430e-01,\n",
              "       -1.081e+00, -1.463e-01, -1.399e-01,  2.390e-02,  5.713e-01,\n",
              "       -7.452e-01,  3.070e-02, -4.138e-01, -6.141e-01, -2.724e-01,\n",
              "        1.390e+00, -1.214e+00, -2.894e-01,  5.936e-01, -3.929e-01,\n",
              "        5.822e-01, -1.234e-01, -9.469e-01,  4.470e-01,  8.311e-01,\n",
              "       -1.013e+00, -4.012e-01, -1.121e+00, -4.584e-01, -2.166e-01,\n",
              "        1.331e-01,  4.109e-01, -9.940e-01, -1.066e-01, -3.474e-01,\n",
              "        3.331e-01,  4.086e-01, -2.665e-01, -1.174e-01,  1.553e-01,\n",
              "       -1.295e-01, -2.760e-02,  5.067e-01,  1.066e-01,  3.034e-01,\n",
              "        4.576e-01, -1.113e-01,  7.005e-01, -8.044e-01, -3.054e-01,\n",
              "       -4.800e-02, -1.911e-01, -3.621e-01, -7.112e-01,  2.406e-01,\n",
              "       -7.326e-01, -2.073e-01, -3.736e-01, -1.077e-01,  7.139e-01,\n",
              "       -6.950e-01, -5.780e-01, -6.300e-02, -3.713e-01,  5.102e-01,\n",
              "        7.970e-02, -1.460e+00, -1.702e-01, -7.258e-01, -5.979e-01,\n",
              "        8.245e-01,  2.411e-01,  5.092e-01,  1.331e-01,  6.866e-01,\n",
              "       -3.456e-01, -6.688e-01,  6.439e-01, -1.065e+00,  1.098e+00,\n",
              "        3.390e-02, -1.341e+00, -2.861e-01,  3.991e-01,  8.242e-01,\n",
              "       -1.353e-01,  4.940e-02,  8.939e-01,  2.270e-01,  2.876e-01,\n",
              "       -3.065e-01,  6.519e-01, -8.156e-01, -1.496e+00,  3.796e-01,\n",
              "        8.770e-02, -1.023e+00, -2.060e-02, -4.149e-01, -6.258e-01,\n",
              "       -2.688e-01,  4.403e-01, -4.900e-01,  2.910e-01,  4.730e-02,\n",
              "       -9.140e-02,  3.087e-01, -6.120e-02, -9.128e-01, -9.399e-01,\n",
              "        1.730e-02,  5.190e-02, -3.500e-03, -5.184e-01, -3.485e-01,\n",
              "        9.810e-02,  7.978e-01, -1.430e-01, -2.067e-01, -2.303e-01,\n",
              "       -1.193e-01,  2.100e-02, -5.020e-02,  1.510e-01, -7.750e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N0QYVKDsTm-"
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train,targets, noise ):\n",
        "        \n",
        "        self.features  = train\n",
        "        self.targets = targets\n",
        "        self.noise = noise\n",
        "        \n",
        "    def sizes(self):\n",
        "        print(\"features size = \", self.features.shape[1])\n",
        "        print(\"targets size = \", self.targets.shape[1])\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx]).float()\n",
        "        \n",
        "#         if self.noise == True:\n",
        "# #             print(\"noisy boi\")\n",
        "#             feature  = feature + torch.randn_like(feature)/150\n",
        "            \n",
        "        target = torch.tensor(self.targets[idx]).float()\n",
        "        \n",
        "        return feature, target\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCn4Ba6EsVyt"
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "        \n",
        "def show_lr(learning_rates):\n",
        "    plt.plot(learning_rates, label = \"learning rate\")\n",
        "    plt.ylabel(\"Learning rate\", fontsize = 15)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_step(x, y, model, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x.to(device))\n",
        "    y = y.float()\n",
        "    loss = criterion(pred,y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZqPFz-CsX4R"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(875)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(875, 2048))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 2048))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(2048)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(2048, 206))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6q-PUbsezZ"
      },
      "source": [
        "\n",
        "def train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, test = test,  fold_number = 1, show_plots = False, train = True, validate = True):\n",
        "    \n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    learning_rates = []    \n",
        "    best_loss = 1000000\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "     \n",
        "        if train == True:\n",
        "            model.train()\n",
        "            losses_temp = []\n",
        "            for batch in train_loader:\n",
        "                (x_batch, y_batch) = batch\n",
        "                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n",
        "                losses_temp.append(loss)\n",
        "            losses.append(torch.mean(torch.tensor(losses_temp)))\n",
        "            \n",
        "            scheduler.step(1.)   ## lr decay caller \n",
        "\n",
        "            learning_rates.append(get_lr(optimizer))\n",
        "            \n",
        "        if validate == True:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_losses_temp = []\n",
        "                for x_val, y_val in val_loader:\n",
        "                    yhat =model(x_val.to(device))  # pred \n",
        "                    val_loss = criterion(yhat.to(device), y_val.to(device))\n",
        "                    val_losses_temp.append(val_loss.item())  ## metrics \n",
        "                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n",
        "\n",
        "        if train == True:\n",
        "            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n",
        "\n",
        "            if val_losses[-1] <= best_loss:\n",
        "\n",
        "                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n",
        "                best_loss = val_losses[-1]\n",
        "                name = \"./model_\" + str(fold_number)+\".pth\"\n",
        "                print(\"saving model as: \", name)\n",
        "                torch.save(model.state_dict(), name)\n",
        "\n",
        "            else: \n",
        "                print(\"showing no improvements, best loss yet:\", best_loss)\n",
        "\n",
        "        if show_plots == True:\n",
        "\n",
        "            show_lr(learning_rates)\n",
        "            plt.plot(val_losses, label = \"val\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "            plt.plot(losses, label = \"train\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "        \n",
        "    test_dataset = TrainDataset(test, target, noise = False)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = Model()\n",
        "    model.load_state_dict(torch.load(name))\n",
        "    model.to(device)\n",
        "    preds = inference_fn(model, test_loader, device)\n",
        "\n",
        "    return val_losses, name, preds\n",
        "\n",
        "def inference_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs.to(device))\n",
        "        \n",
        "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    preds = np.concatenate(preds)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdN_UgGAskZK"
      },
      "source": [
        "def train_model_and_return_test_preds(num_folds, num_epochs, lr, batch_size, decay_factor, decay_patience, train, target, test):\n",
        "\n",
        "    mskf = MultilabelStratifiedKFold(n_splits= num_folds, shuffle=True, random_state=0)\n",
        "\n",
        "    fold_val_losses = list()\n",
        "    \n",
        "    predictions = np.zeros((len(test), 206))\n",
        "\n",
        "    for k , (train_idx,valid_idx) in enumerate(mskf.split(train,target)):\n",
        "\n",
        "        x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n",
        "\n",
        "        input_size = x_train.shape[1]\n",
        "        output_size = target.shape[1]\n",
        "        \n",
        "        \n",
        "        train_dataset = TrainDataset(x_train, y_train, noise = False)\n",
        "        valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n",
        "        \n",
        "        train_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True)\n",
        "        \n",
        "        model = Model()\n",
        "\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-5)\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                        mode='min', \n",
        "                                                        factor= decay_factor, ## wooo hoo\n",
        "                                                        patience=decay_patience, ## was 3 for 158 \n",
        "                                                        eps=1e-4, \n",
        "                                                        verbose=True)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        if k > 1:\n",
        "            print(CRED ,\"fold \", str(k+1), \"  :: mean loss on all folds: \", np.array([min(l) for l in fold_val_losses]).mean(), CEND)\n",
        "\n",
        "        val_losses, filename , preds = train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+1, test = test)\n",
        "        fold_val_losses.append(val_losses)\n",
        "\n",
        "        predictions += preds / num_folds\n",
        "        \n",
        "    return predictions, fold_val_losses\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DApGEjVRZygd",
        "outputId": "d906eac4-13b7-4e69-ab3f-48acbaee12b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions = np.zeros((len(test), 206))\n",
        "seeds =  [1903, 1881]\n",
        "\n",
        "for seed in seeds:\n",
        "    seed_everything(seed = seed)\n",
        "    preds, fold_val_losses = train_model_and_return_test_preds(num_folds = 10, \n",
        "                                    num_epochs = 45, \n",
        "                                    lr = 0.004299882049752947, \n",
        "                                    batch_size = 128,\n",
        "                                    decay_factor = 0.1, \n",
        "                                    decay_patience = 8, \n",
        "                                    train = train, \n",
        "                                    target = target, \n",
        "                                    test = test\n",
        "                                    )\n",
        "    predictions += preds / len(seeds)\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed kar diya\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020441744476556778 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020441744476556778  to  0.018870705738663673 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018870705738663673  to  0.0180932879447937 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  4  out of  45      >showing no improvements, best loss yet: 0.0180932879447937\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.0180932879447937  to  0.01717430166900158 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01717430166900158  to  0.017045237123966217 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017045237123966217  to  0.016998153179883957 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.016998153179883957\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.016998153179883957\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.016998153179883957\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016998153179883957  to  0.01637773960828781 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01637773960828781  to  0.01615399308502674 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.01615399308502674\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.01615399308502674\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.01615399308502674  to  0.01605268195271492 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01605268195271492\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01605268195271492  to  0.015969213098287582 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015969213098287582\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015969213098287582\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015969213098287582\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.015969213098287582  to  0.015923568978905678 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015923568978905678\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.015923568978905678  to  0.01588624157011509 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.01588624157011509  to  0.01584087684750557 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01584087684750557\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01584087684750557\n",
            "epoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.01584087684750557  to  0.015822138637304306 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015822138637304306\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015822138637304306  to  0.015804393216967583 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015804393216967583\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015804393216967583\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015804393216967583\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015804393216967583\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015804393216967583\n",
            "epoch  40  out of  45      >\u001b[32m Val loss decreased from: 0.015804393216967583  to  0.01575116254389286 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01575116254389286\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01575116254389286\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01575116254389286\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01575116254389286\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01575116254389286\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020238080993294716 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020238080993294716  to  0.018687641248106956 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018687641248106956  to  0.017881933599710464 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017881933599710464  to  0.017496924847364426 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017496924847364426  to  0.01728520356118679 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01728520356118679  to  0.016988256946206093 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.016988256946206093  to  0.01689770072698593 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01689770072698593\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01689770072698593\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01689770072698593\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01689770072698593  to  0.01645045541226864 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01645045541226864  to  0.016334785148501396 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016334785148501396  to  0.016153642907738686 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016153642907738686\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016153642907738686  to  0.016133395954966545 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016133395954966545  to  0.016070950776338577 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.016070950776338577  to  0.01601412333548069 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.01601412333548069\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.01601412333548069\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.01601412333548069  to  0.01598380133509636 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.01598380133509636  to  0.01589825563132763 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01589825563132763\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.01589825563132763\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.01589825563132763  to  0.015842268243432045 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015842268243432045\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015842268243432045  to  0.015806930139660835 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015806930139660835\n",
            "epoch  42  out of  45      >\u001b[32m Val loss decreased from: 0.015806930139660835  to  0.015798544511198997 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015798544511198997\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015798544511198997\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015798544511198997\n",
            "\u001b[1;31m fold  3   :: mean loss on all folds:  0.01577485352754593 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020500460639595985 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020500460639595985  to  0.0194113627076149 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.0194113627076149  to  0.01852019876241684 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01852019876241684  to  0.017810281366109848 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017810281366109848  to  0.017802968621253967 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017802968621253967  to  0.017437539994716644 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017437539994716644  to  0.017288343980908394 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017288343980908394\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017288343980908394  to  0.017218492925167084 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.017218492925167084\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017218492925167084  to  0.01669171079993248 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01669171079993248  to  0.016596071422100067 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016596071422100067  to  0.016414333134889603 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016414333134889603  to  0.016325466334819794 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016325466334819794  to  0.016189193353056908 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016189193353056908  to  0.016103524714708328 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016103524714708328\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016103524714708328\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016103524714708328\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016103524714708328  to  0.016029708087444305 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016029708087444305  to  0.01599166728556156 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01599166728556156\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.01599166728556156\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.01599166728556156\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.01599166728556156  to  0.01593375764787197 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01593375764787197\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01593375764787197\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01593375764787197\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01593375764787197\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.01593375764787197\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.01593375764787197  to  0.015928193926811218 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015928193926811218\n",
            "epoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.015928193926811218  to  0.015898555517196655 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  39  out of  45      >\u001b[32m Val loss decreased from: 0.015898555517196655  to  0.015894681215286255 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  40  out of  45      >\u001b[32m Val loss decreased from: 0.015894681215286255  to  0.015856735408306122 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015856735408306122\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015856735408306122\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015856735408306122\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015856735408306122\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015856735408306122\n",
            "\u001b[1;31m fold  4   :: mean loss on all folds:  0.015802147487799328 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02062668278813362 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02062668278813362  to  0.018998779356479645 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018998779356479645  to  0.017853282392024994 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017853282392024994  to  0.017566844820976257 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017566844820976257  to  0.017205355688929558 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017205355688929558  to  0.017138976603746414 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017138976603746414  to  0.016908597201108932 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.016908597201108932\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.016908597201108932\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.016908597201108932\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016908597201108932  to  0.016382984817028046 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016382984817028046  to  0.016245540231466293 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016245540231466293  to  0.016173064708709717 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016173064708709717\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016173064708709717  to  0.01609528809785843 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01609528809785843  to  0.01609041355550289 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01609041355550289  to  0.016059510409832 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.016059510409832  to  0.01598997227847576 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.01598997227847576  to  0.015954410657286644 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015954410657286644  to  0.01582074724137783 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.01582074724137783  to  0.015748148784041405 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015748148784041405\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015748148784041405\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015748148784041405\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015748148784041405  to  0.015707245096564293 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015707245096564293\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015707245096564293\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015707245096564293\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015707245096564293\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015707245096564293\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.015707245096564293  to  0.015641240403056145 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015641240403056145\n",
            "\u001b[1;31m fold  5   :: mean loss on all folds:  0.01576192071661353 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020905930548906326 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020905930548906326  to  0.019155316054821014 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019155316054821014  to  0.018409453332424164 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018409453332424164  to  0.017838703468441963 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017838703468441963  to  0.017577767372131348 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017577767372131348  to  0.0172598734498024 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.0172598734498024\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.0172598734498024\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.0172598734498024\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.0172598734498024\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.0172598734498024  to  0.01667649671435356 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01667649671435356  to  0.01647300459444523 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.01647300459444523\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01647300459444523  to  0.016366658732295036 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016366658732295036  to  0.016350090503692627 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016350090503692627  to  0.016187552362680435 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016187552362680435\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016187552362680435\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016187552362680435\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016187552362680435  to  0.016086755320429802 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016086755320429802  to  0.01594657078385353 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.01594657078385353  to  0.015919703990221024 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015919703990221024\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.015919703990221024  to  0.01591379940509796 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01591379940509796\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.01591379940509796  to  0.015826044604182243 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015826044604182243\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015826044604182243  to  0.015746379271149635 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015746379271149635\n",
            "\u001b[1;31m fold  6   :: mean loss on all folds:  0.015758812427520752 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.021211866289377213 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.021211866289377213  to  0.018826467916369438 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018826467916369438  to  0.018017850816249847 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018017850816249847  to  0.01751791127026081 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.01751791127026081  to  0.017140530049800873 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017140530049800873  to  0.016949152573943138 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.016949152573943138  to  0.01694677770137787 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01694677770137787\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01694677770137787\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01694677770137787\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01694677770137787  to  0.01636546105146408 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01636546105146408  to  0.016157759353518486 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.016157759353518486\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016157759353518486  to  0.01599501632153988 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.01599501632153988\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01599501632153988  to  0.01592002436518669 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01592002436518669\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.01592002436518669  to  0.015909312292933464 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015909312292933464\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015909312292933464  to  0.015888724476099014 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.015888724476099014  to  0.01585591584444046 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01585591584444046\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.01585591584444046  to  0.015761975198984146 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.015761975198984146  to  0.015700507909059525 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015700507909059525\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015700507909059525\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015700507909059525\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015700507909059525\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015700507909059525\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.015700507909059525  to  0.015638623386621475 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015638623386621475\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015638623386621475\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015638623386621475\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015638623386621475\n",
            "epoch  35  out of  45      >\u001b[32m Val loss decreased from: 0.015638623386621475  to  0.01563401333987713 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  36  out of  45      >\u001b[32m Val loss decreased from: 0.01563401333987713  to  0.01558382622897625 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01558382622897625\n",
            "\u001b[1;31m fold  7   :: mean loss on all folds:  0.015729648061096668 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020760923624038696 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020760923624038696  to  0.018766211345791817 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018766211345791817  to  0.01796453259885311 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01796453259885311  to  0.017312876880168915 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017312876880168915  to  0.01727575808763504 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01727575808763504  to  0.017122872173786163 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017122872173786163  to  0.016910023987293243 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.016910023987293243\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.016910023987293243\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.016910023987293243\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016910023987293243  to  0.016336776316165924 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016336776316165924  to  0.016334282234311104 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016334282234311104  to  0.01619584858417511 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01619584858417511  to  0.01604096032679081 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.01604096032679081  to  0.01600179448723793 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01600179448723793  to  0.015978191047906876 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.015978191047906876  to  0.015961846336722374 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.015961846336722374  to  0.015888569876551628 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015888569876551628\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015888569876551628  to  0.015778806060552597 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.015778806060552597  to  0.015752866864204407 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015752866864204407\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.015752866864204407  to  0.01571710593998432 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.01571710593998432\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01571710593998432\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01571710593998432\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01571710593998432\n",
            "epoch  28  out of  45      >\u001b[32m Val loss decreased from: 0.01571710593998432  to  0.015640607103705406 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015640607103705406\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015640607103705406  to  0.015613704919815063 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015613704919815063\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015613704919815063\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015613704919815063\n",
            "epoch  41  out of  45      >\u001b[32m Val loss decreased from: 0.015613704919815063  to  0.015517843887209892 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015517843887209892\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015517843887209892\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015517843887209892\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015517843887209892\n",
            "\u001b[1;31m fold  8   :: mean loss on all folds:  0.015699390321969986 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02073584496974945 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02073584496974945  to  0.019130799919366837 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019130799919366837  to  0.0183657705783844 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.0183657705783844  to  0.017729707062244415 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017729707062244415  to  0.0175637099891901 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  6  out of  45      >showing no improvements, best loss yet: 0.0175637099891901\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.0175637099891901  to  0.01704775169491768 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01704775169491768\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.01704775169491768  to  0.01703554019331932 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01703554019331932\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01703554019331932  to  0.016388295218348503 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016388295218348503  to  0.016341837123036385 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016341837123036385  to  0.016145626083016396 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016145626083016396\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016145626083016396  to  0.01606527529656887 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01606527529656887  to  0.015957767143845558 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.015957767143845558\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015957767143845558\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015957767143845558\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015957767143845558  to  0.015886075794696808 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015886075794696808\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015886075794696808\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015886075794696808\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015886075794696808\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015886075794696808  to  0.01583367958664894 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01583367958664894\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01583367958664894\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01583367958664894\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01583367958664894\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.01583367958664894  to  0.01582706905901432 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.01582706905901432  to  0.015768710523843765 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015768710523843765\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015768710523843765  to  0.015750033780932426 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015750033780932426\n",
            "epoch  41  out of  45      >\u001b[32m Val loss decreased from: 0.015750033780932426  to  0.015747282654047012 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015747282654047012\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015747282654047012\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015747282654047012\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015747282654047012\n",
            "\u001b[1;31m fold  9   :: mean loss on all folds:  0.015705376863479614 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020556384697556496 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020556384697556496  to  0.0190111231058836 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.0190111231058836  to  0.017985258251428604 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017985258251428604  to  0.017593206837773323 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  5  out of  45      >showing no improvements, best loss yet: 0.017593206837773323\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017593206837773323  to  0.01710350252687931 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.01710350252687931\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01710350252687931\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01710350252687931\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01710350252687931\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01710350252687931  to  0.016582166776061058 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016582166776061058  to  0.0164537001401186 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.0164537001401186  to  0.01638929173350334 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01638929173350334  to  0.016289692372083664 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016289692372083664  to  0.016217222437262535 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016217222437262535  to  0.016164502128958702 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.016164502128958702  to  0.0160859152674675 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.0160859152674675\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.0160859152674675\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.0160859152674675  to  0.016005177050828934 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.016005177050828934\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.016005177050828934\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.016005177050828934\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.016005177050828934  to  0.015949314460158348 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015949314460158348\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015949314460158348\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015949314460158348\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015949314460158348\n",
            "epoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.015949314460158348  to  0.015937183052301407 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.015937183052301407  to  0.015888379886746407 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015888379886746407\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015888379886746407  to  0.015801789239048958 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015801789239048958\n",
            "\u001b[1;31m fold  10   :: mean loss on all folds:  0.015716089349653985 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020497996360063553 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020497996360063553  to  0.018955428153276443 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  3  out of  45      >showing no improvements, best loss yet: 0.018955428153276443\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018955428153276443  to  0.017637770622968674 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017637770622968674  to  0.01744716800749302 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01744716800749302  to  0.017227470874786377 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017227470874786377  to  0.017028512433171272 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017028512433171272  to  0.0170142725110054 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.0170142725110054\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.0170142725110054\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.0170142725110054  to  0.016457444056868553 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016457444056868553  to  0.016370141878724098 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016370141878724098  to  0.01626860909163952 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01626860909163952  to  0.016180554404854774 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016180554404854774\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016180554404854774  to  0.016155164688825607 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.016155164688825607  to  0.016005121171474457 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016005121171474457\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016005121171474457\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.016005121171474457\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016005121171474457  to  0.015978943556547165 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015978943556547165\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.015978943556547165  to  0.01596643589437008 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.01596643589437008  to  0.015877055004239082 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015877055004239082\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.015877055004239082  to  0.01587013155221939 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01587013155221939\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01587013155221939\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01587013155221939\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.01587013155221939  to  0.01584613136947155 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.01584613136947155\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.01584613136947155\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.01584613136947155\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.01584613136947155\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.01584613136947155\n",
            "epoch  36  out of  45      >\u001b[32m Val loss decreased from: 0.01584613136947155  to  0.015844659879803658 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015844659879803658  to  0.015828246250748634 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.015828246250748634  to  0.015817692503333092 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015817692503333092\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015817692503333092\n",
            "epoch  41  out of  45      >\u001b[32m Val loss decreased from: 0.015817692503333092  to  0.01581367291510105 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01581367291510105\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01581367291510105\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01581367291510105\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01581367291510105\n",
            "seed kar diya\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020466037094593048 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020466037094593048  to  0.018922708928585052 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018922708928585052  to  0.017977040261030197 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017977040261030197  to  0.01759725622832775 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.01759725622832775  to  0.017394017428159714 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017394017428159714  to  0.01735408790409565 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01735408790409565  to  0.017133396118879318 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017133396118879318  to  0.017027173191308975 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.017027173191308975\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.017027173191308975\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017027173191308975  to  0.016483351588249207 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016483351588249207  to  0.01630638726055622 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.01630638726055622  to  0.016224825754761696 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016224825754761696\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016224825754761696  to  0.01603521779179573 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01603521779179573\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01603521779179573\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.01603521779179573\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.01603521779179573\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.01603521779179573  to  0.015911433845758438 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015911433845758438\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015911433845758438\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015911433845758438\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015911433845758438\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015911433845758438  to  0.01586025580763817 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01586025580763817\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01586025580763817\n",
            "epoch  28  out of  45      >\u001b[32m Val loss decreased from: 0.01586025580763817  to  0.015819409862160683 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.015819409862160683  to  0.015800585970282555 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015800585970282555\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.015800585970282555  to  0.015796013176441193 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015796013176441193\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015796013176441193  to  0.015780461952090263 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015780461952090263\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02101290225982666 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02101290225982666  to  0.01868894323706627 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.01868894323706627  to  0.017869165167212486 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017869165167212486  to  0.01742120087146759 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  5  out of  45      >showing no improvements, best loss yet: 0.01742120087146759\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01742120087146759  to  0.017117705196142197 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017117705196142197  to  0.017086926847696304 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017086926847696304\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017086926847696304  to  0.017017893493175507 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017017893493175507  to  0.016864584758877754 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016864584758877754  to  0.016402103006839752 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016402103006839752  to  0.01617724820971489 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.01617724820971489  to  0.01601603999733925 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.01601603999733925\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.01601603999733925\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01601603999733925\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01601603999733925  to  0.015921229496598244 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015921229496598244\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015921229496598244\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015921229496598244  to  0.015878021717071533 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015878021717071533\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015878021717071533\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015878021717071533\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.015878021717071533  to  0.01584814116358757 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01584814116358757\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.01584814116358757  to  0.015712514519691467 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015712514519691467\n",
            "epoch  40  out of  45      >\u001b[32m Val loss decreased from: 0.015712514519691467  to  0.015686070546507835 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015686070546507835\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015686070546507835\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015686070546507835\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015686070546507835\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015686070546507835\n",
            "\u001b[1;31m fold  3   :: mean loss on all folds:  0.01573326624929905 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.0210653617978096 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.0210653617978096  to  0.019525352865457535 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019525352865457535  to  0.018327465280890465 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018327465280890465  to  0.01791936717927456 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.01791936717927456  to  0.01753072440624237 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  6  out of  45      >showing no improvements, best loss yet: 0.01753072440624237\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01753072440624237  to  0.017446033656597137 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017446033656597137  to  0.017295729368925095 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.017295729368925095\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017295729368925095  to  0.017267918214201927 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017267918214201927  to  0.016641827300190926 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016641827300190926  to  0.01649344526231289 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.01649344526231289  to  0.016371993348002434 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016371993348002434  to  0.016326121985912323 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016326121985912323\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016326121985912323  to  0.01623181253671646 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01623181253671646\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.01623181253671646  to  0.016159920021891594 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016159920021891594\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016159920021891594  to  0.016125719994306564 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016125719994306564  to  0.01605416275560856 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01605416275560856\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.01605416275560856\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.01605416275560856\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01605416275560856\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01605416275560856\n",
            "epoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.01605416275560856  to  0.01596563309431076 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01596563309431076\n",
            "epoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.01596563309431076  to  0.015931569039821625 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015931569039821625\n",
            "epoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.015931569039821625  to  0.015921158716082573 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  39  out of  45      >\u001b[32m Val loss decreased from: 0.015921158716082573  to  0.01586936041712761 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01586936041712761\n",
            "\u001b[1;31m fold  4   :: mean loss on all folds:  0.01577863097190857 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020414311438798904 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020414311438798904  to  0.019048413261771202 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019048413261771202  to  0.01811286062002182 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01811286062002182  to  0.017524728551506996 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017524728551506996  to  0.01735832169651985 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01735832169651985  to  0.01735556498169899 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01735556498169899  to  0.017006047070026398 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017006047070026398\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017006047070026398  to  0.01693940907716751 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01693940907716751\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01693940907716751  to  0.01658039353787899 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01658039353787899  to  0.016366882249712944 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016366882249712944  to  0.01633242331445217 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01633242331445217  to  0.016052257269620895 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016052257269620895\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016052257269620895  to  0.016034333035349846 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016034333035349846\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.016034333035349846  to  0.015966786071658134 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.015966786071658134  to  0.015901906415820122 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015901906415820122\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015901906415820122\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.015901906415820122  to  0.01583917811512947 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.01583917811512947  to  0.015770159661769867 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015770159661769867\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015770159661769867  to  0.015763230621814728 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015763230621814728\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015763230621814728\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015763230621814728\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015763230621814728\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015763230621814728\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.015763230621814728  to  0.015748046338558197 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  32  out of  45      >\u001b[32m Val loss decreased from: 0.015748046338558197  to  0.015708835795521736 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015708835795521736\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015708835795521736  to  0.01563854143023491 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01563854143023491\n",
            "\u001b[1;31m fold  5   :: mean loss on all folds:  0.015743608586490154 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020679527893662453 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020679527893662453  to  0.019304785877466202 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019304785877466202  to  0.018065063282847404 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  4  out of  45      >showing no improvements, best loss yet: 0.018065063282847404\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.018065063282847404  to  0.01761036366224289 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01761036366224289  to  0.01755952648818493 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01755952648818493  to  0.017355618998408318 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017355618998408318  to  0.01733461581170559 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.01733461581170559  to  0.017300356179475784 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.017300356179475784\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017300356179475784  to  0.0166440699249506 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.0166440699249506  to  0.01657935231924057 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.01657935231924057  to  0.016425959765911102 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016425959765911102  to  0.01632104068994522 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.01632104068994522  to  0.016214169561862946 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016214169561862946  to  0.01613031141459942 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01613031141459942\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.01613031141459942\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.01613031141459942\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.01613031141459942  to  0.01607425883412361 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.01607425883412361  to  0.016040541231632233 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.016040541231632233  to  0.01603282243013382 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.01603282243013382  to  0.015868479385972023 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015868479385972023\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015868479385972023  to  0.015834657475352287 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.015834657475352287  to  0.015828801319003105 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015828801319003105\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015828801319003105\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015828801319003105\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015828801319003105\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.015828801319003105  to  0.015817396342754364 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015817396342754364\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015817396342754364\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015817396342754364  to  0.01581043191254139 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01581043191254139\n",
            "epoch  42  out of  45      >\u001b[32m Val loss decreased from: 0.01581043191254139  to  0.015746191143989563 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015746191143989563\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015746191143989563\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015746191143989563\n",
            "\u001b[1;31m fold  6   :: mean loss on all folds:  0.015744125097990037 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02065134048461914 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02065134048461914  to  0.018862318247556686 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018862318247556686  to  0.01797308586537838 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01797308586537838  to  0.017472142353653908 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017472142353653908  to  0.01723668910562992 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01723668910562992  to  0.01695217937231064 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.01695217937231064\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.01695217937231064  to  0.016924887895584106 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.016924887895584106\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.016924887895584106\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016924887895584106  to  0.016303813084959984 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016303813084959984  to  0.016225874423980713 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016225874423980713  to  0.016037307679653168 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016037307679653168  to  0.015972130000591278 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.015972130000591278  to  0.015930110588669777 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.015930110588669777  to  0.015917278826236725 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.015917278826236725\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015917278826236725\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015917278826236725\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015917278826236725  to  0.015748340636491776 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015748340636491776\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.015748340636491776  to  0.015713704749941826 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015713704749941826\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015713704749941826\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015713704749941826  to  0.01567148230969906 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.01567148230969906  to  0.015662556514143944 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015662556514143944\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015662556514143944\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015662556514143944\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.015662556514143944  to  0.015604848973453045 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015604848973453045\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015604848973453045\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015604848973453045  to  0.015573067590594292 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015573067590594292  to  0.015506363473832607 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015506363473832607\n",
            "\u001b[1;31m fold  7   :: mean loss on all folds:  0.015704498160630465 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020440200343728065 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020440200343728065  to  0.019001353532075882 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019001353532075882  to  0.017860811203718185 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017860811203718185  to  0.017583036795258522 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017583036795258522  to  0.017105795443058014 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  6  out of  45      >showing no improvements, best loss yet: 0.017105795443058014\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017105795443058014  to  0.01693175733089447 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01693175733089447\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01693175733089447\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.01693175733089447\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01693175733089447  to  0.016359038650989532 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016359038650989532  to  0.016213512048125267 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016213512048125267  to  0.01611420325934887 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01611420325934887  to  0.016050254926085472 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016050254926085472  to  0.015960656106472015 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.015960656106472015\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.015960656106472015\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015960656106472015\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015960656106472015\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015960656106472015  to  0.01584741473197937 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.01584741473197937  to  0.01577458158135414 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01577458158135414\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.01577458158135414\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.01577458158135414  to  0.015746306627988815 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015746306627988815\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.015746306627988815  to  0.015704602003097534 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015704602003097534\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015704602003097534\n",
            "epoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.015704602003097534  to  0.015696512535214424 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015696512535214424\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.015696512535214424  to  0.01568244956433773 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  32  out of  45      >\u001b[32m Val loss decreased from: 0.01568244956433773  to  0.015629567205905914 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015629567205905914\n",
            "\u001b[1;31m fold  8   :: mean loss on all folds:  0.015693793738526956 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020637448877096176 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020637448877096176  to  0.018913768231868744 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018913768231868744  to  0.01815730333328247 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01815730333328247  to  0.017978714779019356 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017978714779019356  to  0.017392851412296295 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017392851412296295  to  0.01718388870358467 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01718388870358467  to  0.017056480050086975 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017056480050086975\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.017056480050086975\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >showing no improvements, best loss yet: 0.017056480050086975\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017056480050086975  to  0.016545496881008148 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016545496881008148  to  0.016305778175592422 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016305778175592422  to  0.01621307246387005 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.01621307246387005\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.01621307246387005  to  0.016009621322155 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016009621322155  to  0.016001228243112564 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016001228243112564\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016001228243112564\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.016001228243112564  to  0.015942329540848732 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015942329540848732\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.015942329540848732  to  0.015844909474253654 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015844909474253654\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.015844909474253654  to  0.015807952731847763 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015807952731847763\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015807952731847763\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015807952731847763\n",
            "epoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.015807952731847763  to  0.015785707160830498 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015785707160830498\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015785707160830498\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015785707160830498\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015785707160830498\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015785707160830498\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015785707160830498  to  0.01574869640171528 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.01574869640171528\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.01574869640171528\n",
            "epoch  36  out of  45      >\u001b[32m Val loss decreased from: 0.01574869640171528  to  0.015723826363682747 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015723826363682747  to  0.01566167362034321 \u001b[0m   >saving model as:  ./model_8.pth\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01566167362034321\n",
            "\u001b[1;31m fold  9   :: mean loss on all folds:  0.01568977872375399 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020636914297938347 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020636914297938347  to  0.019143957644701004 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019143957644701004  to  0.018313933163881302 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018313933163881302  to  0.017665967345237732 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017665967345237732  to  0.017337437719106674 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017337437719106674  to  0.01721491664648056 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.01721491664648056\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01721491664648056\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01721491664648056\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.01721491664648056  to  0.017097285017371178 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017097285017371178  to  0.016538765281438828 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016538765281438828  to  0.016307733952999115 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016307733952999115  to  0.016244372352957726 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016244372352957726\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016244372352957726\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016244372352957726  to  0.016211392357945442 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.016211392357945442  to  0.016190476715564728 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016190476715564728\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.016190476715564728  to  0.016184503212571144 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016184503212571144  to  0.016095474362373352 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016095474362373352  to  0.016066033393144608 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.016066033393144608  to  0.0160489734262228 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.0160489734262228  to  0.01588338613510132 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.01588338613510132\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01588338613510132\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01588338613510132\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01588338613510132\n",
            "epoch  28  out of  45      >\u001b[32m Val loss decreased from: 0.01588338613510132  to  0.0158770140260458 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.0158770140260458\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.0158770140260458\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.0158770140260458\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.0158770140260458\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.0158770140260458  to  0.015865735709667206 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  34  out of  45      >\u001b[32m Val loss decreased from: 0.015865735709667206  to  0.015864023938775063 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  35  out of  45      >\u001b[32m Val loss decreased from: 0.015864023938775063  to  0.015854161232709885 \u001b[0m   >saving model as:  ./model_9.pth\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015854161232709885\n",
            "\u001b[1;31m fold  10   :: mean loss on all folds:  0.015708043446971312 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020309537649154663 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020309537649154663  to  0.018961796537041664 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018961796537041664  to  0.017936570569872856 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.017936570569872856  to  0.017626306042075157 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017626306042075157  to  0.01730727218091488 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.01730727218091488  to  0.01722618192434311 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01722618192434311  to  0.01713109388947487 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01713109388947487\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01713109388947487\n",
            "Epoch    10: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.01713109388947487  to  0.017065076157450676 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.017065076157450676  to  0.0165525171905756 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.0165525171905756  to  0.01629973202943802 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.01629973202943802\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01629973202943802  to  0.016229519620537758 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016229519620537758  to  0.01609172485768795 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01609172485768795\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01609172485768795\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.01609172485768795\n",
            "Epoch    19: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.01609172485768795\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.01609172485768795  to  0.016046376898884773 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.016046376898884773  to  0.015876784920692444 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.015876784920692444  to  0.015858421102166176 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015858421102166176\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015858421102166176\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015858421102166176\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015858421102166176\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015858421102166176\n",
            "epoch  28  out of  45      >\u001b[32m Val loss decreased from: 0.015858421102166176  to  0.01583155244588852 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01583155244588852\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.01583155244588852\n",
            "epoch  31  out of  45      >\u001b[32m Val loss decreased from: 0.01583155244588852  to  0.015791157260537148 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015791157260537148\n",
            "epoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.015791157260537148  to  0.01576068252325058 \u001b[0m   >saving model as:  ./model_10.pth\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01576068252325058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjn513VZmldY",
        "outputId": "ae19e57f-c1bd-498f-a44a-540756f2fef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions[0][:5]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00076782, 0.00084333, 0.00230426, 0.01547371, 0.02112664])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fXAx7B38ouG",
        "outputId": "2250a03a-e616-42a9-b8e3-4276db68ba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "plt.plot(predictions[0])\n",
        "plt.grid()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M9kLBIhnfys",
        "outputId": "17b116b4-5975-477e-813a-54accc28602a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "for l in fold_val_losses:\n",
        "    plt.plot(l)\n",
        "    plt.scatter([l.index(min(l))], [min(l)],s = 105, label = str(min(l)))\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "minimas = []\n",
        "for i in fold_val_losses:\n",
        "    minimas.append(min(i))\n",
        "\n",
        "np.array(minimas).mean()\n",
        "## best yet\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.015713307354599238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxB4LVuT0Qn6",
        "outputId": "ad3eb55e-72e3-4df0-cd9c-e25423d75d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00076782, 0.00084333, 0.00230426, ..., 0.00179961, 0.0043172 ,\n",
              "        0.00120759],\n",
              "       [0.00027013, 0.0009506 , 0.00181589, ..., 0.00081296, 0.00128002,\n",
              "        0.00342525],\n",
              "       [0.00047722, 0.0003703 , 0.00246729, ..., 0.00453238, 0.0004479 ,\n",
              "        0.00236072],\n",
              "       ...,\n",
              "       [0.00064053, 0.00048456, 0.0007446 , ..., 0.00099891, 0.00041231,\n",
              "        0.00102656],\n",
              "       [0.0015757 , 0.00096861, 0.0011021 , ..., 0.00140459, 0.00047403,\n",
              "        0.00288218],\n",
              "       [0.00042428, 0.00065828, 0.0010458 , ..., 0.00172391, 0.00028746,\n",
              "        0.00103138]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNHdq359gObR"
      },
      "source": [
        "submission = pd.read_csv('sample_submission.csv')\n",
        "sub_cp = submission\n",
        "sub_cp.to_csv('./submission_cp.csv', index=None, header=True)\n",
        "\n",
        "import csv \n",
        "a = predictions  \n",
        "with open('./submission_cp.csv', \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(a)\n",
        "\n",
        "final_sub = pd.read_csv('./submission_cp.csv', header = None)\n",
        "\n",
        "final_sub.columns = submission.columns[1:]\n",
        "final_sub[\"sig_id\"] = submission[\"sig_id\"]\n",
        "\n",
        "good_cols = np.roll(final_sub.columns.values, 1)\n",
        "final_sub = final_sub[good_cols]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtN2F5ib61s"
      },
      "source": [
        "targets = [col for col in final_sub.columns]\n",
        "final_sub.loc[test_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\n",
        "final_sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca3bRLQmlcNG",
        "outputId": "7f7d23a3-e4e9-4df1-f9cf-6e25115b1f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "final_sub\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.015474</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.006143</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.011404</td>\n",
              "      <td>0.018193</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.002011</td>\n",
              "      <td>0.006703</td>\n",
              "      <td>0.012887</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>0.001606</td>\n",
              "      <td>0.004682</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>0.004359</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.003051</td>\n",
              "      <td>0.003558</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.005611</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.001586</td>\n",
              "      <td>0.015238</td>\n",
              "      <td>0.008764</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>0.004475</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.025843</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000908</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.002499</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.002352</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.004317</td>\n",
              "      <td>0.001208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001816</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>0.002077</td>\n",
              "      <td>0.001819</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>0.007774</td>\n",
              "      <td>0.004767</td>\n",
              "      <td>0.011620</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.002393</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.010478</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.002556</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>0.003893</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.000564</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000925</td>\n",
              "      <td>0.002553</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.001554</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.003847</td>\n",
              "      <td>0.003263</td>\n",
              "      <td>0.004317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.002737</td>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>0.003814</td>\n",
              "      <td>0.008534</td>\n",
              "      <td>0.003807</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>0.006877</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.021829</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>0.002158</td>\n",
              "      <td>0.004049</td>\n",
              "      <td>0.001189</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.001927</td>\n",
              "      <td>0.006306</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>0.005292</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.009214</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>0.001280</td>\n",
              "      <td>0.003425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.010828</td>\n",
              "      <td>0.015564</td>\n",
              "      <td>0.004751</td>\n",
              "      <td>0.003174</td>\n",
              "      <td>0.004545</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.014084</td>\n",
              "      <td>0.032797</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.004949</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.001990</td>\n",
              "      <td>0.002639</td>\n",
              "      <td>0.004888</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.002611</td>\n",
              "      <td>0.002496</td>\n",
              "      <td>0.002143</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.001118</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.007712</td>\n",
              "      <td>0.002279</td>\n",
              "      <td>0.002125</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.002215</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.002485</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003523</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.002695</td>\n",
              "      <td>0.001409</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.001146</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.016275</td>\n",
              "      <td>0.037156</td>\n",
              "      <td>0.002903</td>\n",
              "      <td>0.002114</td>\n",
              "      <td>0.004725</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.009554</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.003612</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.005551</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.001712</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.002790</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.001594</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.001811</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>0.008732</td>\n",
              "      <td>0.005317</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.001893</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.003446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.001920</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.002255</td>\n",
              "      <td>0.012448</td>\n",
              "      <td>0.022481</td>\n",
              "      <td>0.004450</td>\n",
              "      <td>0.004681</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.011694</td>\n",
              "      <td>0.017026</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>0.001584</td>\n",
              "      <td>0.003904</td>\n",
              "      <td>0.004976</td>\n",
              "      <td>0.002481</td>\n",
              "      <td>0.001274</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>0.005118</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.003487</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.004542</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>0.006074</td>\n",
              "      <td>0.004716</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.001917</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005417</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>0.007152</td>\n",
              "      <td>0.004304</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.000769</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.001662</td>\n",
              "      <td>0.001957</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.010150</td>\n",
              "      <td>0.011196</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.015624</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.002101</td>\n",
              "      <td>0.002815</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.002391</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.006441</td>\n",
              "      <td>0.002388</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.002270</td>\n",
              "      <td>0.001948</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.001401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.007348</td>\n",
              "      <td>0.002403</td>\n",
              "      <td>0.000629</td>\n",
              "      <td>0.003768</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.004021</td>\n",
              "      <td>0.008522</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.001911</td>\n",
              "      <td>0.003105</td>\n",
              "      <td>0.005078</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001697</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.002606</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000852</td>\n",
              "      <td>0.003426</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.004342</td>\n",
              "      <td>0.002386</td>\n",
              "      <td>0.006295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003391</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>0.000706</td>\n",
              "      <td>0.008840</td>\n",
              "      <td>0.016620</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>0.001297</td>\n",
              "      <td>0.002887</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.011736</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.007769</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.001699</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.003511</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>0.045888</td>\n",
              "      <td>0.004188</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.002170</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.000952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.004042</td>\n",
              "      <td>0.002169</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.006917</td>\n",
              "      <td>0.024278</td>\n",
              "      <td>0.008167</td>\n",
              "      <td>0.005852</td>\n",
              "      <td>0.003167</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.028222</td>\n",
              "      <td>0.029519</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.005732</td>\n",
              "      <td>0.004753</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.003358</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.001791</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>0.003263</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.004874</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.003267</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.007213</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.010560</td>\n",
              "      <td>0.021628</td>\n",
              "      <td>0.002722</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.021774</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.004220</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.001890</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.002352</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.002050</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.001866</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.001634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.013891</td>\n",
              "      <td>0.028577</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.007929</td>\n",
              "      <td>0.028605</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.001857</td>\n",
              "      <td>0.005220</td>\n",
              "      <td>0.003760</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.004409</td>\n",
              "      <td>0.002074</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.000997</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000359</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.010423</td>\n",
              "      <td>0.025650</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.015612</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.003246</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000901</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.001937</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.001027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.001576</td>\n",
              "      <td>0.000969</td>\n",
              "      <td>0.001102</td>\n",
              "      <td>0.019998</td>\n",
              "      <td>0.024383</td>\n",
              "      <td>0.006008</td>\n",
              "      <td>0.004295</td>\n",
              "      <td>0.003501</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>0.023249</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.001436</td>\n",
              "      <td>0.002411</td>\n",
              "      <td>0.007166</td>\n",
              "      <td>0.003547</td>\n",
              "      <td>0.001781</td>\n",
              "      <td>0.002630</td>\n",
              "      <td>0.003686</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.003978</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.002460</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003411</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.006820</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.010236</td>\n",
              "      <td>0.010128</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.013138</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.005822</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.003665</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.001377</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.002882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.001046</td>\n",
              "      <td>0.012453</td>\n",
              "      <td>0.018073</td>\n",
              "      <td>0.003826</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.004391</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.003791</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>0.001272</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.003178</td>\n",
              "      <td>0.007134</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.005567</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.001047</td>\n",
              "      <td>0.002505</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004626</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.002295</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.001229</td>\n",
              "      <td>0.012732</td>\n",
              "      <td>0.012488</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.017189</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000780</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.001724</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.001031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows  207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.001208\n",
              "1     id_001897cda  ...       0.003425\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.003446\n",
              "4     id_0027f1083  ...       0.001401\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.000952\n",
              "3978  id_ff925dd0d  ...       0.001634\n",
              "3979  id_ffb710450  ...       0.001027\n",
              "3980  id_ffbb869f2  ...       0.002882\n",
              "3981  id_ffd5800b6  ...       0.001031\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xidrALu5Xj",
        "outputId": "426e948b-0f52-4fc4-e236-7a4fe342d39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "final_sub\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.013079</td>\n",
              "      <td>0.020879</td>\n",
              "      <td>0.004594</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.010284</td>\n",
              "      <td>0.017457</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.001999</td>\n",
              "      <td>0.005281</td>\n",
              "      <td>0.010209</td>\n",
              "      <td>0.002171</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.002781</td>\n",
              "      <td>0.002880</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002455</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.004390</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.001731</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.001513</td>\n",
              "      <td>0.011037</td>\n",
              "      <td>0.009106</td>\n",
              "      <td>0.003734</td>\n",
              "      <td>0.003816</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.022784</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.003952</td>\n",
              "      <td>0.000998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.003099</td>\n",
              "      <td>0.001941</td>\n",
              "      <td>0.001825</td>\n",
              "      <td>0.004720</td>\n",
              "      <td>0.008049</td>\n",
              "      <td>0.006754</td>\n",
              "      <td>0.015794</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.010562</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>0.002272</td>\n",
              "      <td>0.003526</td>\n",
              "      <td>0.002915</td>\n",
              "      <td>0.001272</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.001688</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.002943</td>\n",
              "      <td>0.003547</td>\n",
              "      <td>0.006169</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.002530</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>0.019115</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>0.002941</td>\n",
              "      <td>0.008045</td>\n",
              "      <td>0.002810</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.020774</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.001991</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.001541</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.001918</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.002064</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.005034</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.011475</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.010895</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.001495</td>\n",
              "      <td>0.003812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.008715</td>\n",
              "      <td>0.015561</td>\n",
              "      <td>0.004835</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.005242</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.008431</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.003444</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.002164</td>\n",
              "      <td>0.002343</td>\n",
              "      <td>0.005335</td>\n",
              "      <td>0.002735</td>\n",
              "      <td>0.001708</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.002142</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.002069</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.005846</td>\n",
              "      <td>0.001834</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.002040</td>\n",
              "      <td>0.002329</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.002613</td>\n",
              "      <td>0.000585</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002695</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.014134</td>\n",
              "      <td>0.034934</td>\n",
              "      <td>0.002470</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.004378</td>\n",
              "      <td>0.001343</td>\n",
              "      <td>0.008189</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>0.002776</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.001489</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.002817</td>\n",
              "      <td>0.016333</td>\n",
              "      <td>0.004058</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001383</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.002262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.012904</td>\n",
              "      <td>0.021184</td>\n",
              "      <td>0.003925</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.010687</td>\n",
              "      <td>0.016837</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>0.002985</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>0.002172</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.003732</td>\n",
              "      <td>0.004787</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.003113</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.005768</td>\n",
              "      <td>0.002917</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.001717</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004537</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.004773</td>\n",
              "      <td>0.003916</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.002080</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.012758</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.018343</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.001111</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.002546</td>\n",
              "      <td>0.001047</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.001929</td>\n",
              "      <td>0.002136</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.001438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.001385</td>\n",
              "      <td>0.003255</td>\n",
              "      <td>0.007609</td>\n",
              "      <td>0.002351</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.003535</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.004995</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.001343</td>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.007354</td>\n",
              "      <td>0.000925</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.001796</td>\n",
              "      <td>0.003293</td>\n",
              "      <td>0.004741</td>\n",
              "      <td>0.001537</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>0.001637</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.001507</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001559</td>\n",
              "      <td>0.002362</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.004573</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>0.006272</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>0.007652</td>\n",
              "      <td>0.014677</td>\n",
              "      <td>0.002489</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.002807</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.010085</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.008119</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001532</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.004655</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.058207</td>\n",
              "      <td>0.004806</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.000975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.008202</td>\n",
              "      <td>0.027207</td>\n",
              "      <td>0.007810</td>\n",
              "      <td>0.005028</td>\n",
              "      <td>0.003977</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>0.034039</td>\n",
              "      <td>0.033154</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.007107</td>\n",
              "      <td>0.005448</td>\n",
              "      <td>0.002566</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.003420</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.001467</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.003989</td>\n",
              "      <td>0.003913</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.004844</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.002888</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>0.001220</td>\n",
              "      <td>0.006886</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.001462</td>\n",
              "      <td>0.000954</td>\n",
              "      <td>0.001139</td>\n",
              "      <td>0.012481</td>\n",
              "      <td>0.025317</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>0.003146</td>\n",
              "      <td>0.001572</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.024339</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.004326</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.002177</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.002091</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.002439</td>\n",
              "      <td>0.000564</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.002485</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.001614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.013951</td>\n",
              "      <td>0.029812</td>\n",
              "      <td>0.004865</td>\n",
              "      <td>0.002610</td>\n",
              "      <td>0.003679</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.011208</td>\n",
              "      <td>0.027684</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.005643</td>\n",
              "      <td>0.003877</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>0.005514</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.004592</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001111</td>\n",
              "      <td>0.002175</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000780</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.004082</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.009747</td>\n",
              "      <td>0.022234</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>0.001643</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.015556</td>\n",
              "      <td>0.001519</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.003154</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.001155</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.002407</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.001971</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>0.001210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.019289</td>\n",
              "      <td>0.026286</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>0.004155</td>\n",
              "      <td>0.003775</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.019966</td>\n",
              "      <td>0.021110</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.007612</td>\n",
              "      <td>0.003176</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.002212</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.004646</td>\n",
              "      <td>0.003566</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>0.004770</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.002215</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.005902</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.001018</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.009421</td>\n",
              "      <td>0.010955</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>0.001174</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.013638</td>\n",
              "      <td>0.001447</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.005615</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.002983</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.002391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.012169</td>\n",
              "      <td>0.018496</td>\n",
              "      <td>0.003812</td>\n",
              "      <td>0.001510</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.006642</td>\n",
              "      <td>0.018984</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.002898</td>\n",
              "      <td>0.008452</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>0.003014</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004653</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.012375</td>\n",
              "      <td>0.014699</td>\n",
              "      <td>0.001863</td>\n",
              "      <td>0.001468</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.017939</td>\n",
              "      <td>0.001297</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>0.002529</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.001775</td>\n",
              "      <td>0.001505</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.001068</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.001715</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.001065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows  207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.000998\n",
              "1     id_001897cda  ...       0.003812\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.002262\n",
              "4     id_0027f1083  ...       0.001438\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.000975\n",
              "3978  id_ff925dd0d  ...       0.001614\n",
              "3979  id_ffb710450  ...       0.001210\n",
              "3980  id_ffbb869f2  ...       0.002391\n",
              "3981  id_ffd5800b6  ...       0.001065\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYY8Avuau3wI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}