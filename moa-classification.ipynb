{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d   ## smoother\nfrom tqdm.notebook import tqdm\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.preprocessing import MinMaxScaler\n        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\n\n\n\ndef seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'\ndevice","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"'cuda'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.cp_time.unique(), train_features.cp_type.unique(),  train_features.cp_dose.unique()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(array([24, 72, 48]),\n array(['trt_cp', 'ctl_vehicle'], dtype=object),\n array(['D1', 'D2'], dtype=object))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"((23814, 876), (3982, 876))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore_columns = ['sig_id', \"cp_type\"]\n\ntrain_columns = [x for x in train_features.columns if x not in ignore_columns]\n\ntrain = train_features[train_columns]\ntest = test_features[train_columns]\ntarget = train_targets_scored.iloc[:,1:].values","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, target.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"((23814, 874), (3982, 874), (23814, 206))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = ColumnTransformer([\n                            ('o',OneHotEncoder(),[0,1]),\n                            ('s',Normalizer(),list(range(3,train.shape[1])))\n                        ])\n\n\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, train,targets, noise ):\n        \n        self.features  = train\n        self.targets = targets\n        self.noise = noise\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float() \n        \n#         if self.noise == True:\n# #             print(\"noisy boi\")\n#             feature  = feature + torch.randn_like(feature)/150\n            \n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature,target","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef train_step(x, y, model, optimizer, criterion):\n    optimizer.zero_grad()\n    pred = model(x.to(device))\n    y = y.float()\n    loss = criterion(pred,y.to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        \n        \nclass Model(nn.Module):\n    def __init__(self,input_size,output_size,hidden_size):\n        super(Model,self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(input_size)\n        self.dropout1 = nn.Dropout(0.5)\n        self.linear1 = nn.utils.weight_norm(nn.Linear(input_size,hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.6)\n        self.linear2 = nn.utils.weight_norm(nn.Linear(hidden_size,hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.6)\n        self.linear3 = nn.utils.weight_norm(nn.Linear(hidden_size,output_size))\n        \n    def forward(self,xb):\n        x = self.batch_norm1(xb)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.linear1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.linear2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        return self.linear3(x)\n\n    \n    \n\n\n\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n    \n    losses = []\n    val_losses = []\n    learning_rates = []    \n    best_loss = 1000000\n\n    for epoch in range(num_epochs):\n\n        if validate == True:\n            with torch.no_grad():\n                model.eval()\n                val_losses_temp = []\n                for x_val, y_val in val_loader:\n                    yhat =model(x_val.to(device))  # pred \n                    val_loss = criterion(yhat.to(device), y_val.to(device))\n                    val_losses_temp.append(val_loss.item())  ## metrics \n                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n\n            \n        if train == True:\n            model.train()\n            losses_temp = []\n            for batch in train_loader:\n                (x_batch, y_batch) = batch\n                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n                losses_temp.append(loss)\n            losses.append(torch.mean(torch.tensor(losses_temp)))\n            scheduler.step(1.)   ## lr decay caller \n            learning_rates.append(get_lr(optimizer))\n\n\n\n        \n        if train == True:\n            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n\n            if val_losses[-1] <= best_loss:\n\n                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n                best_loss = val_losses[-1]\n\n                name = \"./model_\" + str(fold_number)+\".pth\"\n\n                print(\"saving model as: \", name)\n\n                torch.save(model.state_dict(), name)\n\n            else: \n                print(\"showing no improvements, best loss yet:\", best_loss)\n\n        if show_plots == True:\n\n            show_lr(learning_rates)\n            plt.plot(val_losses, label = \"val\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(losses, label = \"train\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n        \n    return losses, val_losses","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building folds \n\nCGREEN  = '\\33[32m'\nCBLUE =  '\\033[34m'\nCEND  = '\\33[0m'\n\n\nNFOLDS =5\nkfold = KFold(NFOLDS,shuffle=True,random_state=42)\nfold_train_losses = list()\nfold_valid_losses = list()\n\nnum_epochs = 3\n\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(train)):\n\n    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n\n    input_size = x_train.shape[1]\n    output_size = target.shape[1]\n    \n    \n    train_dataset = TrainDataset(x_train, y_train, noise = False)\n    valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n    \n    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers = 8)\n\n    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True, num_workers = 8)\n    \n    model = Model(input_size, output_size, hidden_size = 512)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.8e-3)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     mode='min', \n                                                     factor=0.5, \n                                                     patience=5, \n                                                     eps=1e-5, \n                                                     verbose=True)\n    criterion = nn.BCEWithLogitsLoss()\n    print(\"fold \", str(k+1))\n\n    train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+1)\n    \nprint(CBLUE, \"Training complete\", CEND)","execution_count":13,"outputs":[{"output_type":"stream","text":"fold  1\nepoch  1  out of  3      >\u001b[32m Val loss decreased from: 1000000  to  0.6920276284217834 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  2  out of  3      >\u001b[32m Val loss decreased from: 0.6920276284217834  to  0.042478304356336594 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  3  out of  3      >\u001b[32m Val loss decreased from: 0.042478304356336594  to  0.022082189098000526 \u001b[0m   >saving model as:  ./model_1.pth\nfold  2\nepoch  1  out of  3      >\u001b[32m Val loss decreased from: 1000000  to  0.6931414604187012 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  2  out of  3      >\u001b[32m Val loss decreased from: 0.6931414604187012  to  0.04332802817225456 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  3  out of  3      >\u001b[32m Val loss decreased from: 0.04332802817225456  to  0.021770842373371124 \u001b[0m   >saving model as:  ./model_2.pth\nfold  3\nepoch  1  out of  3      >\u001b[32m Val loss decreased from: 1000000  to  0.6947375535964966 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  2  out of  3      >\u001b[32m Val loss decreased from: 0.6947375535964966  to  0.04136008769273758 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  3  out of  3      >\u001b[32m Val loss decreased from: 0.04136008769273758  to  0.021727461367845535 \u001b[0m   >saving model as:  ./model_3.pth\nfold  4\nepoch  1  out of  3      >\u001b[32m Val loss decreased from: 1000000  to  0.69425368309021 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  2  out of  3      >\u001b[32m Val loss decreased from: 0.69425368309021  to  0.03785991296172142 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  3  out of  3      >\u001b[32m Val loss decreased from: 0.03785991296172142  to  0.0212875809520483 \u001b[0m   >saving model as:  ./model_4.pth\nfold  5\nepoch  1  out of  3      >\u001b[32m Val loss decreased from: 1000000  to  0.6927652359008789 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  2  out of  3      >\u001b[32m Val loss decreased from: 0.6927652359008789  to  0.04109072685241699 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  3  out of  3      >\u001b[32m Val loss decreased from: 0.04109072685241699  to  0.021778972819447517 \u001b[0m   >saving model as:  ./model_5.pth\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [Model(input_size, output_size , 512) for i in range (NFOLDS)]\n\nfor i in range (len(all_models)):\n    \n    name = \"./model_\" + str(i + 1) + \".pth\"\n    all_models[i].load_state_dict(torch.load(name))\n    all_models[i].to(device)\n    print(\"Loaded: \", name)\n\n","execution_count":25,"outputs":[{"output_type":"stream","text":"Loaded:  ./model_1.pth\nLoaded:  ./model_2.pth\nLoaded:  ./model_3.pth\nLoaded:  ./model_4.pth\nLoaded:  ./model_5.pth\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_val_losses = []\nfor i in range(NFOLDS):\n    losses, val_losses = train_one_fold(all_models[i],5 , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 0, train = False, validate = True)\n    all_val_losses.append(np.mean(np.array(val_losses)))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_val_losses)\n# plt.ylim(0.013,0.02)","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ff4bfdf3890>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class model_jury(object):   ## only works for dataloaders for batch size 1 \n    def __init__(self, all_models):\n        self.all_models = all_models\n        \n        \n        \n    def predict(self, x, plot = False):\n        \n        with torch.no_grad():\n            preds = [self.all_models[i](x.to(device)).view(-1).cpu().tolist() for i in range(len(self.all_models))]\n        \n        if plot == True:\n            for pred in preds:\n                plt.plot(pred)\n            plt.show()\n            \n        preds = np.array(preds)\n        mean = np.mean(preds, axis = 0)\n        return mean.flatten()","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jury = model_jury(all_models)","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TrainDataset(test, target, noise = False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers = 8)","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_preds = []\nfor batch in tqdm(test_loader):\n    x, y = batch\n    \n    foo = jury.predict(x, plot = False)\n    list_of_preds.append(foo)\n","execution_count":109,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3982.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e480c911bc3a42ad81aa1a4f6f01b6a1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub_cp = submission\nsub_cp.to_csv('./submission_cp.csv', index=None, header=True)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv \na = list_of_preds  \nwith open('./submission_cp.csv', \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerows(a)","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub = pd.read_csv('./submission_cp.csv', header = None)\nfinal_sub.head()","execution_count":121,"outputs":[{"output_type":"execute_result","execution_count":121,"data":{"text/plain":"        0         1         2         3         4         5         6    \\\n0 -5.679960 -5.628830 -5.634180 -4.666551 -4.121871 -5.306041 -5.552208   \n1 -5.727033 -5.621867 -5.590736 -5.025705 -4.662688 -5.222306 -5.558617   \n2 -6.046939 -6.217653 -5.983761 -5.020549 -4.778184 -5.734206 -5.707179   \n3 -5.862432 -5.826532 -5.779856 -4.960424 -4.677885 -5.357684 -5.693943   \n4 -5.980521 -6.269011 -6.043952 -5.057303 -4.746848 -5.932042 -5.787375   \n\n        7         8         9    ...       196       197       198       199  \\\n0 -4.974575 -5.668497 -4.092396  ... -5.765751 -5.528400 -5.410370 -4.321980   \n1 -5.027584 -5.553468 -4.396370  ... -5.573226 -5.398426 -5.494564 -3.813975   \n2 -5.624209 -6.067981 -4.872532  ... -6.055518 -6.119366 -5.866686 -5.235461   \n3 -5.158955 -5.753490 -4.549016  ... -5.826924 -5.693624 -5.607389 -4.077273   \n4 -5.742365 -6.221155 -4.850810  ... -6.212705 -6.233780 -5.904562 -5.864781   \n\n        200       201       202       203       204       205  \n0 -5.317147 -5.721891 -4.458188 -5.583466 -5.584235 -5.631935  \n1 -5.040711 -5.606177 -4.321869 -5.634780 -5.442592 -5.482084  \n2 -5.903933 -6.121309 -5.717096 -5.902942 -5.789285 -5.825858  \n3 -5.302030 -5.828968 -4.485175 -5.755053 -5.591357 -5.612889  \n4 -6.126331 -6.203797 -6.207918 -5.976230 -5.991645 -6.031420  \n\n[5 rows x 206 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>196</th>\n      <th>197</th>\n      <th>198</th>\n      <th>199</th>\n      <th>200</th>\n      <th>201</th>\n      <th>202</th>\n      <th>203</th>\n      <th>204</th>\n      <th>205</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-5.679960</td>\n      <td>-5.628830</td>\n      <td>-5.634180</td>\n      <td>-4.666551</td>\n      <td>-4.121871</td>\n      <td>-5.306041</td>\n      <td>-5.552208</td>\n      <td>-4.974575</td>\n      <td>-5.668497</td>\n      <td>-4.092396</td>\n      <td>...</td>\n      <td>-5.765751</td>\n      <td>-5.528400</td>\n      <td>-5.410370</td>\n      <td>-4.321980</td>\n      <td>-5.317147</td>\n      <td>-5.721891</td>\n      <td>-4.458188</td>\n      <td>-5.583466</td>\n      <td>-5.584235</td>\n      <td>-5.631935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-5.727033</td>\n      <td>-5.621867</td>\n      <td>-5.590736</td>\n      <td>-5.025705</td>\n      <td>-4.662688</td>\n      <td>-5.222306</td>\n      <td>-5.558617</td>\n      <td>-5.027584</td>\n      <td>-5.553468</td>\n      <td>-4.396370</td>\n      <td>...</td>\n      <td>-5.573226</td>\n      <td>-5.398426</td>\n      <td>-5.494564</td>\n      <td>-3.813975</td>\n      <td>-5.040711</td>\n      <td>-5.606177</td>\n      <td>-4.321869</td>\n      <td>-5.634780</td>\n      <td>-5.442592</td>\n      <td>-5.482084</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.046939</td>\n      <td>-6.217653</td>\n      <td>-5.983761</td>\n      <td>-5.020549</td>\n      <td>-4.778184</td>\n      <td>-5.734206</td>\n      <td>-5.707179</td>\n      <td>-5.624209</td>\n      <td>-6.067981</td>\n      <td>-4.872532</td>\n      <td>...</td>\n      <td>-6.055518</td>\n      <td>-6.119366</td>\n      <td>-5.866686</td>\n      <td>-5.235461</td>\n      <td>-5.903933</td>\n      <td>-6.121309</td>\n      <td>-5.717096</td>\n      <td>-5.902942</td>\n      <td>-5.789285</td>\n      <td>-5.825858</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-5.862432</td>\n      <td>-5.826532</td>\n      <td>-5.779856</td>\n      <td>-4.960424</td>\n      <td>-4.677885</td>\n      <td>-5.357684</td>\n      <td>-5.693943</td>\n      <td>-5.158955</td>\n      <td>-5.753490</td>\n      <td>-4.549016</td>\n      <td>...</td>\n      <td>-5.826924</td>\n      <td>-5.693624</td>\n      <td>-5.607389</td>\n      <td>-4.077273</td>\n      <td>-5.302030</td>\n      <td>-5.828968</td>\n      <td>-4.485175</td>\n      <td>-5.755053</td>\n      <td>-5.591357</td>\n      <td>-5.612889</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-5.980521</td>\n      <td>-6.269011</td>\n      <td>-6.043952</td>\n      <td>-5.057303</td>\n      <td>-4.746848</td>\n      <td>-5.932042</td>\n      <td>-5.787375</td>\n      <td>-5.742365</td>\n      <td>-6.221155</td>\n      <td>-4.850810</td>\n      <td>...</td>\n      <td>-6.212705</td>\n      <td>-6.233780</td>\n      <td>-5.904562</td>\n      <td>-5.864781</td>\n      <td>-6.126331</td>\n      <td>-6.203797</td>\n      <td>-6.207918</td>\n      <td>-5.976230</td>\n      <td>-5.991645</td>\n      <td>-6.031420</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 206 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(final_sub.values[0], \".\")","execution_count":122,"outputs":[{"output_type":"execute_result","execution_count":122,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ff4bff3c6d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdXklEQVR4nO3dbYwd1XkH8P+za4xKGiUuDhBjr40LQQ1N49obxxJqFVfgUisNBYpKQpNIaeIogqpREkUpSBQlUhWlcWk/uGkMRcmHEERiKCi1EkPkBvWDwbtWKAbiZut6zbKEty4pEpHt3X364d6xZ8fzfs7MOTPn//uyu/fenZl75sxzXueMqCqIiKj/RlwfABERtYMBn4goEAz4RESBYMAnIgoEAz4RUSCWuT6APCtXrtR169a5Pgwios6YnJx8VVXfkfae1wF/3bp1mJiYcH0YRESdISLTWe+xS4eIKBAM+EREgWDAJyIKBAM+EVEgGPCJiALBgE9EFAgGfOqEyek57No/hcnpOdeHQtRZXs/DJwIGwf7mew7g5Pwili8bwXc+uQWb1q5wfVhEncMaPnnvwNHXcHJ+EYsKnJpfxIGjr7k+JKJOYsAn721Zfz6WLxvBqADnLBvBlvXnuz4kok5ilw55b9PaFfjOJ7fgwNHXsGX9+ezOodomp+eCzkcM+NQJm9auCPICJXs4FsQuHSIKBMeCGPCJKBAcC2KXDhEFgmNBDPhEFJDQx4LYpUNEFAgGfCKiQDDgExEFggGfiCgQDPhERIFgwCciCgQDPhFRIBjwiYgCwYBPRBQIBnxygo8sJGofl1ag1nGZWiI3WMOn1nVpmVq2RKhPWMOn1kXL1J6aX/R6mVq2RKhvGPCpdV1ZpjatJeLrsRKVwYBPTpRZptb180e70hIhKosBn7zkQ3dKV1oiRGUx4Ce4rlXSgC/dKaE/MIP6xcosHRH5goioiKzMeP8aETkiIlMi8iUb+2xCVKvcue8Ibr7nAGdmOMTnjxLZZ1zDF5E1AK4GcDzj/VEAu4afmQFwUEQeUdVnTfdtmy+1SmJ3ClETbHTp3AXgiwAeznh/M4ApVT0KACJyP4BrAXgX8DlI5xd2pxDZZRTwReRDAF5Q1adEJOtjFwN4Pvb3DID352xzB4AdADA2NmZyeJWxVklEfVYY8EXkMQAXpbx1O4DbAGwr2kTKa5r1YVXdDWA3AIyPj2d+rimsVRJRXxUGfFW9Ku11EXkPgEsARLX71QAOichmVf1F7KMzANbE/l4NYLb2ERMRUS21u3RU9WkAF0R/i8gxAOOq+mriowcBXCYilwB4AcBNAD5Sd79ERFRPI4unicgqEdkLAKo6D+BWAD8C8ByAB1T1mSb2S0RE2azdeKWq62K/zwLYHvt7L4C9tvZFRETVcXnkIS6DS0R9x6UV4Me6LURETWMNH916IAcRUV1BBfysbhuu20IAu/Wo/4Lp0snrtuEdtsRuPQpBMAG/aGE03mEbNi6cRyEIpkuH3TaUh/mDQiCqrS9XU9r4+LhOTExY2x4fbkJ5mD+oD0RkUlXH094LpksHYLcN5WP+oL4LpkuHiCh0DPhERIFgwCciCgQDPhFRIBjwiYgCwYBPRBQIBnw6C9eUIeqnoObhUzGuKUPUX6zh0xJcKpqovxjwaQmuKUPUX+zSoSV8Xyqa690Q1ceAT2fxdU0Zji9Q17musDDgU2dwzfpucB3UfOVDhYUBnzojGl84Nb/I8QVP+RDUfOVDhYUBnzrD9/EF8iOo1dV0y8SHCgsDPnWKr+MLNOBDUKujjZaJDxUWBnwissaHoFZHWy0T1xUWBnwissp1UKujqy2TqhjwiSh4XW2ZVMWAT0SEbrZMquLSCkREgWDAJyIKBAM+EVEgGPCJqPf4UJ8BDtoSUa/5vtxDm2sPMeATGeJiYX7zebmHtgsjKwFfRL4A4O8AvENVX015/xiANwAsAJhX1XEb+yVyzffaI/l9U1XbhZFxwBeRNQCuBnC84KNb0woDoi7zufZIAz7fVNV2YWSjhn8XgC8CeNjCtog6xefaI53h601VbRdGRgFfRD4E4AVVfUpE8j6qAPaJiAL4pqruztnmDgA7AGBsbMzk8Iga53PtkbqhzcKoMOCLyGMALkp563YAtwHYVmI/V6rqrIhcAOBREfmZqj6e9sFhYbAbAMbHx7XEtomc8rX2SJRUGPBV9aq010XkPQAuARDV7lcDOCQim1X1F4ltzA5/viwiDwHYDCA14BMRUTNq33ilqk+r6gWquk5V1wGYAbAxGexF5C0i8tbodwxaBIcNjpmIiGpo5E5bEVklInuHf14I4D9E5CkATwL4N1X9YRP7JSKibNZuvBrW8qPfZwFsH/5+FMB7be2HiIjq4Vo6RESBYMAnIgoEAz4RUSAY8Imos7jscTVcLZOIOokL11XHGn4PsdZDIUhbuI7ysYbfM6z1VMO17LuLC9dVF0TAD+mi5nK95bFw7DYuXFdd7wN+aBc1az3lsXDsPi5cV03vA35oFzVrPeWxcKTQ9D7gh3hRs9ZTDgtHCo2o+rvk/Pj4uE5MTBhvJ6Q+fCIKm4hMZj03vPc1fIA1XiIigPPwiYiCwYBPRBQIBnyygnf3EvkviD58alZo9zoQdRVr+GSMa5oQdQMDPhmL7nUYFQRzrwNRF7FLh4y5vIGJ91j4geehGxjwyQoX9zpw7MAPPA/dwS4d6iyOHfiB56E7GPCpszh24Aeeh+4IYi0d6i/2HfuB58Efwa+lQ/3FdZL8wPPQDezSISIKBAM+EVEgGPCJiALBgE9EFAgGfCKiQDDgExEFggGfiCgQDPg9xweTEFGEN171GBe1IqI41vB7jItadRNbZdSUoGv4fV//I1rU6tT8Ihe16gi2yqhJRgFfRO4E8CkArwxfuk1V96Z87hoA/whgFMA9qvpVk/3aEMKF5fLBJFRPWquM541ssVHDv0tVv571poiMAtgF4GoAMwAOisgjqvqshX3XFsqF1YdFrfreEosLsVUW0vl1rY0unc0AplT1KACIyP0ArgXgNOCHeGF1UQgtsbjQWmWhnV/XbAT8W0XkYwAmAHxeVZMjTRcDeD729wyA92dtTER2ANgBAGNjYxYOL11oF1ZX2WiJda0G2YdWWVmhtLR9URjwReQxABelvHU7gG8A+AoAHf7cCeATyU2k/G/mU1dUdTeA3cDgAShFx2eiqxdW1wJYnqLvYtoSYw3Sb2xpt6sw4KvqVWU2JCJ3A/hBylszANbE/l4NYLbU0dFZ+hTAynwX05YYa5B+Y0u7XaazdN6pqi8O/7wOwOGUjx0EcJmIXALgBQA3AfiIyX5D1qcAVva7mLTEWIP0X1db2l1k2of/NRHZgEEXzTEAnwYAEVmFwfTL7ao6LyK3AvgRBtMy71XVZwz3G6w+BbA2vgtrkERn8CHmHRRSH35omB5kig8x75k+NYH79F1M9Wl8hvzEtXSIPMG1j6hpDPhEnojGNEYFnR+fIT+xS4fIEyENMHOswg0GfEPMuJSlTt7o+5jG5PQc9hyawfcnZzC/wLGKtjHgG+AgW7iKgjnzxtmiNDlxavH0rfZdv5ekaxjwDfTpJigqr0wwZ944W5QmUbAXcKyibQz4Bvp0ExSVVyaYM28MxFtC8TQZHRHcOL4G129cHXxB2CYGfAMhDbLRGWWCeR/zRtUxibSWUN/SpGsY8FOUydjxz9yy9dKWj5BcKhvM+zQAW2dMIq0ldMvWS3uTJl3EgJ9QJmNzQI76FMzLqDMmwW4t/zDgJ5TJ2ByQo9DUCd597NbqOgb8hDIZmzUXCk3d4F2nJcR7W5rD1TJTVO3DZ6akkNm8Fthdao6rZVZUplbSdh+u7QLGxvZY6JHtAM3u0mYx4DtSJVjavqhsbK8PNTEWWOZsB2h2lzaLAd+BqsGyzEVVJXjZuEi7XhPrQ4HlA9sBug8DvT5XJBjwHagaLIsuqqrBK7m9Fectx679U5UyaNdrYslzsOfQjLcXaVKdgNJUEGoiQHd5yqvvFQkGfAeqBsuii6pqARLf3orzluPLP3imcgbtek0seZt/V1ZvrBNQmg5CXQ7Qtvne8mXAd6BOsMy7qOrOkd60dgV27Z+qnUG7fKHHz8Hs67/Cd5887u1FGlcnoPgehGzxoSvF95ZvcAHfh0wB2A2WJrVt3zNok6JzEK3RHk8Dk3zSZB6rcr6i41hx3vLen2NfulJ8b/kGNQ/fl0zRhqqzgJKf9aVgbEv8+wKonU/ayGNl7xOJH8cdH7wCc2+e7PT5zPveu/ZPYee+I1hUYFSAz2273Os1rpq8vjgPfyikpm2VoJNsbYRUMEbiaWDSzdVGHivTOkwex9ybJ70OgEWK8mTTLdW+3FwW1EPM+/iQ6MnpOezaP4XJ6bnTr6UFnSpM/7/rTPJJ2f9NO291ZG2nb3m9KE9GXSmf23a59QAaBeid+47g5nsOGJ8zl9dXUDV83/vXqsqqKZjWdkLu1wfK55O0Wl+Z/7VVw8vbTtnj6Mq1UPYZBE18jz7dXBZUwAeayxQuLp6sjGhasPWtYKyjKJ8UBVvTG+nKKNpO3nF0rdvOZZ7s081lwQX8Jri6ePIyomnB1uUpl02KCvbZ139VO2jbCiAm2zEtdFxUcOJ5ss399+nmMgZ8C1wNBrMm3q54wb5sRLBsdAQLC/nBtm63TxmupuO6bh242H9fKkAM+Ba47JPrS0bsggcPzeDEqUUogIVFxZ9tXoOL3/5rtfrqbZ23aBvRwF+Vm+bKFhbJQsv1bDfX++8yBnwL+lzT7tLAXpMmp+fwvYnnEd21Mjo6ghs2rm6lr77ouOKtjhvH1+D6guOKlCl00got14P6rvdflw/XEgO+JX2sadddt8V1pm7CgaOvYX5xEO4FwJ9uKg6qbQSmeKFyckFx3xPHsefQjLVujrRC65atlzqt4HSxguW6GyzCgB+YJpdR9iVTNyEZvG/YuLrwf9oITNFxRV1NCrutiaxCy3UFx/X+q/KlG4oBv4a8oOlzDdd0GeWiGmrWDSW+pkcVdYN304EpOq49h2bw/cmZwkHkutvvwzlM09b16ks3FAN+RXlB0/carskyymUuiLR19k3WpPEtyJTt83Y1XfGGjasbW/Pel3NgU5vXqy8FJwN+RXlB05dmWxaTZZTLSGbquunhe8GZxfVx9zUwNyUvfzZRcPtwfhjwK8oLmr4027K0UctIZuo66eF7wZmlq8ddhY8tr7qyrteyBXcX08Io4IvInQA+BeCV4Uu3qerelM8dA/AGgAUA81lLd3ZBXtD0pdmWp81aRt30sFVwtn1BNlng+xBcXLdgbImnZVr+LFNwdzUtbNTw71LVr5f43FZVfdXC/pzLC5o+NNt8Uic96hQUyYDo6m7MJgp8m9/FpODoQwsmLS2Ty0aXKbi7mhbs0vGID7W4NpT5nlUKirSL2OVyF7b3Y+u7mBYcpi2YNvN31r7KpGWZgtv37tssNgL+rSLyMQATAD6vqmmLRSuAfSKiAL6pqruzNiYiOwDsAICxsTELh2em6UwafwxdnYeJd00TNe+0i7irF2QaW9/FtOAwacG02eLK21c8LUdHBLOv/wqT03OpQb+o377u0hQuFQZ8EXkMwEUpb90O4BsAvoJBQP8KgJ0APpHy2StVdVZELgDwqIj8TFUfT9vfsDDYDQwecVjqWzSk6Uwa3/6ICBZVvWwi2sywTdS8swLi9RtXQ4Y/fUnLOmx1FdkoOOq2YNpsceXtK3nfwnefLH9ncpnuoDL/4zIvFgZ8Vb2qzIZE5G4AP8jYxuzw58si8hCAzQBSA75Pms6k8e1DFSMjAoF6VSO1nWGbqHknAyKw9Jm015e4K7ZpZZ9Dm/UZG11FLicVtNniKtrXprWDBeDmF6pd23XigW99/aazdN6pqi8O/7wOwOGUz7wFwIiqvjH8fRuAL5vs14YyF2DTmTS5/Ts+eAUOz/4SUnN7JjVxkz7PKtKCjo0WRDwgmjyTtgllHijeVk3Q1aSCNgubpvrg2/qfJpn24X9NRDZg0KVzDMCnAUBEVgG4R1W3A7gQwEMiEu3vPlX9oeF+jZS9uJrOpGk106gfv+oCWCYBo2yfp80aeZOBruiY2+5TXbLA2alF3PHwYSyqLvm+TdYEq37fptKn7SnBQPay0XWu7bb+p0lGAV9VP5rx+iyA7cPfjwJ4r8l+bKtycTWdSW3VTE0CRpk+T9MM21YLouiYXfSpxgsgyRiraaomWPX7NpE+LgYty3yPOtd2U/8Tn7yRbP3ZFOS0TNsXl60MbXJcTf6vaaHXdgsi75hd9KnGC6BoNlba6pNN1ASrft8qny87LuFi0NK3vvM8URpFK56OCBpLqyADvs2Ly2aGNjmuOv9bZ4pZGVWekNR2k9dVn2q8iyGtDz/6jO3vn/Z98wJ12fQpm+9dBV7f+s7zRGkUTUlsMq2CDPiAvYurToY2mY1hayZH3SlmVWp00ROYrlj1tkZbEFWOsakCpmi/RaustjlOlBeoy6ZP2XzvsoD1qe88T5RGJ08tYhGDGn5TadXbgO/DOtdpx9DU4GpVVQuqOjW66AlM556TPjPFtiqD8TaPocx+s9L7vieOpw7i2lR1nKhM+pQN5KaB1+Q6bmr8LX5MgPnzHpJdfuzDr8jkOZ9VZWXorCCQduED5TKNzeZx1ZpX1Rpd8glMc2+eLGxBmCpzjE1UBMrsN6tr5Y6HD59+dOLJFro8bNW40/J9VtrWDby+3bSUPKZlIwKIYH7h7Om2QLWCIJ5GUTpGr9vUy4CfVsu0+ZzPpLQMHT+GE6cW8c8/+W9sWPN2rDhvee2HhNhsHleteVWt0dV5ApNpMC5qbUXHFF2gtvJDmbRJS+9d+6ewsHjmZvIRkca7PPIqKFXTPhmk2lgyw0XAj6fNkmNaGFRpFEun2yYLAlet+DS9DPhZtcw2M8yW9edj2Yjg5MIgQzz67Ev48XMvnVUTqDpF1GbzuErNq8q+o+1WeQKTjYxe1NqK8gNgNz+UTZtkem9Zfz7OPWfQdzsyIvjytb/dSv5MHoeNtLcVnON5tM3+/6xpkWk3zcXX4oEIFhYS021jBUGb06vL6GXAr1rLbOrpNjeOr8F9TxxfMvp+4tQiDs/+En973XtOf7ZKpnbZPM7at42mvK2Mntfais6DwP6gWJ3z4svAoo20txGc0/JoG+mTNy0ymTZzb548axA8Od02XhC0Ob26jF4G/CgA3bBxdWEts8kbTa5Y9bbTNbjF4XsK4PuTM7hhOKbQ1kVvu+Zge5VPWwEjLR3j2x5teEynqiYGkKvmJRtpbyMfp+XRW7Ze2vh5ypsWmZY2yXMW/X75RW81HsxtOh70LuBXnW7Y9I0mUffNU8+/jkeffQkKYGHh7LnoTWdqmzWHJlb5tNFdlVVw+1KTblrdyQq20sc0H7uawpk3LbJOV2b87zqajAe9C/hVa7JN32gSzU6ZnJ7D4z9/xdmNILYu6snpOfzDY//VyCqfJhm96Ly3Uai6ZjJZwYf0KZtH8ypedQef86ZF+pA2tvQu4FetJZTNZKY3mvhQyzTNuFl9nW3MsS/SpTsrm+LDZAVTRXk0reIFLO1Hr9O92Kegnqd3Ab9OYC1zsm3caNL1TBXv6xwBcOWlK/HZq97lxXeyWaD69ISiuLJ3EdeZElt1XzbU2Uey4rXn0AwePDTTyEOE2nraXZv5rHcBH7AXWJN31JV9glLXA3uWZKHXRLB3fWdlXg3SRQFQdXA8SoOyU2Jt3w2et93k+0XPCEiTzIMCFHYv1slTbT7trs2bynoZ8G3Iu6OujSco+VjLtDGw2vaMqaryapBtH5PJ4HiZwq/K3eBVvnPVpSaynhGQ9b2S0yL3HJpZ8hChvHn0Zc9f0/PhXd1UxoCfIeuOujZOTt3aTxvq1qJN1ptJ21ZThWFeDbLt/vAHD82cuVksUXtdcd5y7No/ZZQGWeltOh5S5jzG95H1jIAsyTyYVwmpG1ibHhNyNebEgJ8hOXe77o0UddSt/WTxobVQNQhkpXPT6yQV1SDbujAnp+fwvYnnT88NX7ZsBHf+8aDgb/reB9OWXJnzmJwZk/aMgLLyKiF1A2vVNMi6xvJuSnQxiUNUtfhTjoyPj+vExISz/dteFa/Kfm++5wBOnloEBFAdzLgYFeBz2y4/fV+Bzw+gyDqO6MLLOo6i77Rr/xR27juCaAkaAXDuOc1+LxcFZvx7CoAPv3/s9N3Z8feSeaKqMt1sdb677ccqmpyDNgZf064xV9eeiEyq6njae6zh50jWHNq62DetXYE7PngF7nj4MBYW9fQUyORAlM8PoEgqW6Mp6jJyMfXQxSB8smZ6Q2zcyGZ3QPy7JQOjScCqmmZ5nzcNnE2fv6xrzJdrL44B31Nzb57EomrmFEjT+wJcsHHh2Zx66LO8ArKJ7oC0oOpLwPLlOLJkXWM+XXsRBnxPFU2BtHFfQFdVnXrYVXkFpO1aa1pQ9SVg+XIcWbKuMR+vPfbhe6zJfs06+6P+yhpj8SVP+HIcXZDXh8+ATwD8GdwldxhU+4GDtlTI935Sap6LwWlq14jrAyA/RP2ko2L/4SBE5AfW8AmAnwNMRGQXAz6dxiY9Ub+xS4eIKBAM+ETkpcnpOezaP4XJ6TnXh9Ib7NIhIu9wmnAzWMMnIu+kTRMmcwz4ROQdThNuBrt0iMg7nCbcDAZ8IvISpwnbxy4dIqJAGAd8EflLETkiIs+IyNcyPnPN8DNTIvIl030SEVF1Rl06IrIVwLUAfkdVT4jIBSmfGQWwC8DVAGYAHBSRR1T1WZN9ExFRNaY1/M8A+KqqngAAVX055TObAUyp6lFVPQngfgwKCSIiapFpwH8XgN8TkSdE5Cci8r6Uz1wM4PnY3zPD14iIqEWFXToi8hiAi1Leun34/ysAbAHwPgAPiMh6XfpUFUn538ynrojIDgA7AGBsbKzo8IiIqKTCgK+qV2W9JyKfAfDgMMA/KSKLAFYCeCX2sRkAa2J/rwYwm7O/3QB2D7f/iohMFx1jhpUAXq35vyFhOpXDdCqH6VReU2m1NusN03n4/wrgDwD8u4i8C8BynP0FDgK4TEQuAfACgJsAfKTMxlX1HXUPTEQmsh7zRWcwncphOpXDdCrPRVqZ9uHfC2C9iBzGYDD246qqIrJKRPYCgKrOA7gVwI8APAfgAVV9xnC/RERUkVENfzjr5s9TXp8FsD32914Ae032RUREZvp8p+1u1wfQEUyncphO5TCdyms9rWTphBoiIuqrPtfwiYgohgGfiCgQvQv4XKgtm4gcE5GnReSnIjIxfO03RORREfn58GeQ69GKyL0i8vJwxln0WmbaiMhfD/PYERH5QzdH3b6MdLpTRF4Y5qufisj22HuhptMaEdkvIs8NF5b8q+HrTvNUrwJ+bKG2PwLwbgAfFpF3uz0q72xV1Q2x+b9fAvBjVb0MwI+Hf4foWwCuSbyWmjbDPHUTgCuG//NPw7wXgm/h7HQCgLuG+WrDcFZe6Ok0D+DzqvpbGKxEcMswPZzmqV4FfHChtjquBfDt4e/fBvAnDo/FGVV9HMD/Jl7OSptrAdyvqidU9X8ATGGQ93ovI52yhJxOL6rqoeHvb2BwD9LFcJyn+hbwuVBbPgWwT0Qmh2sWAcCFqvoiMMikAM5a4jpgWWnDfHa2W0XkP4ddPlE3BdMJgIisA/C7AJ6A4zzVt4BfaaG2AF2pqhsx6PK6RUR+3/UBdRTz2VLfAPCbADYAeBHAzuHrwaeTiPw6gD0APquq/5f30ZTXrKdV3wJ+pYXaQjO8Azp6bsFDGDQZXxKRdwLA8GfaMw1ClZU2zGcxqvqSqi6o6iKAu3GmKyLodBKRczAI9t9R1QeHLzvNU30L+KcXahOR5RgMgjzi+Ji8ICJvEZG3Rr8D2AbgMAbp8/Hhxz4O4GE3R+ilrLR5BMBNInLucFHAywA86eD4vBAFsKHrMMhXQMDpJCIC4F8APKeqfx97y2meMl0t0yuqOi8i0UJtowDu5UJtp10I4KFBPsQyAPep6g9F5CAGzzH4CwDHAdzo8BidEZHvAvgAgJUiMgPgbwB8FSlpo6rPiMgDAJ7FYDbGLaq64OTAW5aRTh8QkQ0YdEEcA/BpIOx0AnAlgI8CeFpEfjp87TY4zlNcWoGIKBB969IhIqIMDPhERIFgwCciCgQDPhFRIBjwiYgCwYBPRBQIBnwiokD8PwJKpltD0CWVAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub = pd.read_csv('./submission_cp.csv', header = None)\nfinal_sub.head()","execution_count":123,"outputs":[{"output_type":"execute_result","execution_count":123,"data":{"text/plain":"        0         1         2         3         4         5         6    \\\n0 -5.679960 -5.628830 -5.634180 -4.666551 -4.121871 -5.306041 -5.552208   \n1 -5.727033 -5.621867 -5.590736 -5.025705 -4.662688 -5.222306 -5.558617   \n2 -6.046939 -6.217653 -5.983761 -5.020549 -4.778184 -5.734206 -5.707179   \n3 -5.862432 -5.826532 -5.779856 -4.960424 -4.677885 -5.357684 -5.693943   \n4 -5.980521 -6.269011 -6.043952 -5.057303 -4.746848 -5.932042 -5.787375   \n\n        7         8         9    ...       196       197       198       199  \\\n0 -4.974575 -5.668497 -4.092396  ... -5.765751 -5.528400 -5.410370 -4.321980   \n1 -5.027584 -5.553468 -4.396370  ... -5.573226 -5.398426 -5.494564 -3.813975   \n2 -5.624209 -6.067981 -4.872532  ... -6.055518 -6.119366 -5.866686 -5.235461   \n3 -5.158955 -5.753490 -4.549016  ... -5.826924 -5.693624 -5.607389 -4.077273   \n4 -5.742365 -6.221155 -4.850810  ... -6.212705 -6.233780 -5.904562 -5.864781   \n\n        200       201       202       203       204       205  \n0 -5.317147 -5.721891 -4.458188 -5.583466 -5.584235 -5.631935  \n1 -5.040711 -5.606177 -4.321869 -5.634780 -5.442592 -5.482084  \n2 -5.903933 -6.121309 -5.717096 -5.902942 -5.789285 -5.825858  \n3 -5.302030 -5.828968 -4.485175 -5.755053 -5.591357 -5.612889  \n4 -6.126331 -6.203797 -6.207918 -5.976230 -5.991645 -6.031420  \n\n[5 rows x 206 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>196</th>\n      <th>197</th>\n      <th>198</th>\n      <th>199</th>\n      <th>200</th>\n      <th>201</th>\n      <th>202</th>\n      <th>203</th>\n      <th>204</th>\n      <th>205</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-5.679960</td>\n      <td>-5.628830</td>\n      <td>-5.634180</td>\n      <td>-4.666551</td>\n      <td>-4.121871</td>\n      <td>-5.306041</td>\n      <td>-5.552208</td>\n      <td>-4.974575</td>\n      <td>-5.668497</td>\n      <td>-4.092396</td>\n      <td>...</td>\n      <td>-5.765751</td>\n      <td>-5.528400</td>\n      <td>-5.410370</td>\n      <td>-4.321980</td>\n      <td>-5.317147</td>\n      <td>-5.721891</td>\n      <td>-4.458188</td>\n      <td>-5.583466</td>\n      <td>-5.584235</td>\n      <td>-5.631935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-5.727033</td>\n      <td>-5.621867</td>\n      <td>-5.590736</td>\n      <td>-5.025705</td>\n      <td>-4.662688</td>\n      <td>-5.222306</td>\n      <td>-5.558617</td>\n      <td>-5.027584</td>\n      <td>-5.553468</td>\n      <td>-4.396370</td>\n      <td>...</td>\n      <td>-5.573226</td>\n      <td>-5.398426</td>\n      <td>-5.494564</td>\n      <td>-3.813975</td>\n      <td>-5.040711</td>\n      <td>-5.606177</td>\n      <td>-4.321869</td>\n      <td>-5.634780</td>\n      <td>-5.442592</td>\n      <td>-5.482084</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.046939</td>\n      <td>-6.217653</td>\n      <td>-5.983761</td>\n      <td>-5.020549</td>\n      <td>-4.778184</td>\n      <td>-5.734206</td>\n      <td>-5.707179</td>\n      <td>-5.624209</td>\n      <td>-6.067981</td>\n      <td>-4.872532</td>\n      <td>...</td>\n      <td>-6.055518</td>\n      <td>-6.119366</td>\n      <td>-5.866686</td>\n      <td>-5.235461</td>\n      <td>-5.903933</td>\n      <td>-6.121309</td>\n      <td>-5.717096</td>\n      <td>-5.902942</td>\n      <td>-5.789285</td>\n      <td>-5.825858</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-5.862432</td>\n      <td>-5.826532</td>\n      <td>-5.779856</td>\n      <td>-4.960424</td>\n      <td>-4.677885</td>\n      <td>-5.357684</td>\n      <td>-5.693943</td>\n      <td>-5.158955</td>\n      <td>-5.753490</td>\n      <td>-4.549016</td>\n      <td>...</td>\n      <td>-5.826924</td>\n      <td>-5.693624</td>\n      <td>-5.607389</td>\n      <td>-4.077273</td>\n      <td>-5.302030</td>\n      <td>-5.828968</td>\n      <td>-4.485175</td>\n      <td>-5.755053</td>\n      <td>-5.591357</td>\n      <td>-5.612889</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-5.980521</td>\n      <td>-6.269011</td>\n      <td>-6.043952</td>\n      <td>-5.057303</td>\n      <td>-4.746848</td>\n      <td>-5.932042</td>\n      <td>-5.787375</td>\n      <td>-5.742365</td>\n      <td>-6.221155</td>\n      <td>-4.850810</td>\n      <td>...</td>\n      <td>-6.212705</td>\n      <td>-6.233780</td>\n      <td>-5.904562</td>\n      <td>-5.864781</td>\n      <td>-6.126331</td>\n      <td>-6.203797</td>\n      <td>-6.207918</td>\n      <td>-5.976230</td>\n      <td>-5.991645</td>\n      <td>-6.031420</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 206 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.columns = submission.columns[1:]\nfinal_sub[\"sig_id\"] = submission[\"sig_id\"]\n","execution_count":124,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_cols = np.roll(final_sub.columns.values, 1)\nfinal_sub = final_sub[good_cols]","execution_count":125,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.to_csv(\"./submission.csv\", index=False)\nfinal_sub","execution_count":126,"outputs":[{"output_type":"execute_result","execution_count":126,"data":{"text/plain":"            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0     id_0004d9e33                    -5.679960               -5.628830   \n1     id_001897cda                    -5.727033               -5.621867   \n2     id_002429b5b                    -6.046939               -6.217653   \n3     id_00276f245                    -5.862432               -5.826532   \n4     id_0027f1083                    -5.980521               -6.269011   \n...            ...                          ...                     ...   \n3977  id_ff7004b87                    -6.120609               -6.029621   \n3978  id_ff925dd0d                    -5.804338               -6.027048   \n3979  id_ffb710450                    -5.921171               -6.007148   \n3980  id_ffbb869f2                    -5.901772               -5.928934   \n3981  id_ffd5800b6                    -5.725790               -5.672912   \n\n      acat_inhibitor  acetylcholine_receptor_agonist  \\\n0          -5.634180                       -4.666551   \n1          -5.590736                       -5.025705   \n2          -5.983761                       -5.020549   \n3          -5.779856                       -4.960424   \n4          -6.043952                       -5.057303   \n...              ...                             ...   \n3977       -6.076323                       -5.671113   \n3978       -5.967618                       -4.978369   \n3979       -5.969432                       -5.031055   \n3980       -5.872447                       -5.050678   \n3981       -5.628758                       -4.696895   \n\n      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                             -4.121871                       -5.306041   \n1                             -4.662688                       -5.222306   \n2                             -4.778184                       -5.734206   \n3                             -4.677885                       -5.357684   \n4                             -4.746848                       -5.932042   \n...                                 ...                             ...   \n3977                          -5.435343                       -5.909560   \n3978                          -4.265625                       -5.740501   \n3979                          -4.241337                       -5.350413   \n3980                          -4.389804                       -5.599361   \n3981                          -4.260148                       -5.227330   \n\n      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                      -5.552208                      -4.974575   \n1                      -5.558617                      -5.027584   \n2                      -5.707179                      -5.624209   \n3                      -5.693943                      -5.158955   \n4                      -5.787375                      -5.742365   \n...                          ...                            ...   \n3977                   -6.110951                      -5.806555   \n3978                   -5.606171                      -5.475812   \n3979                   -5.561746                      -5.249198   \n3980                   -5.518376                      -5.399906   \n3981                   -5.428792                      -4.953642   \n\n      adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n0                      -5.668497  ...                              -5.765751   \n1                      -5.553468  ...                              -5.573226   \n2                      -6.067981  ...                              -6.055518   \n3                      -5.753490  ...                              -5.826924   \n4                      -6.221155  ...                              -6.212705   \n...                          ...  ...                                    ...   \n3977                   -5.980929  ...                              -6.079699   \n3978                   -6.117903  ...                              -6.143017   \n3979                   -5.990728  ...                              -5.954805   \n3980                   -5.866856  ...                              -6.018552   \n3981                   -5.672046  ...                              -5.710971   \n\n      trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n0        -5.528400        -5.410370          -4.321980   \n1        -5.398426        -5.494564          -3.813975   \n2        -6.119366        -5.866686          -5.235461   \n3        -5.693624        -5.607389          -4.077273   \n4        -6.233780        -5.904562          -5.864781   \n...            ...              ...                ...   \n3977     -5.712262        -5.953594          -3.601126   \n3978     -6.083321        -5.804624          -6.088103   \n3979     -5.951075        -5.854402          -5.676965   \n3980     -5.951058        -5.599330          -4.957850   \n3981     -5.532167        -5.499734          -4.486096   \n\n      tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n0                     -5.317147                              -5.721891   \n1                     -5.040711                              -5.606177   \n2                     -5.903933                              -6.121309   \n3                     -5.302030                              -5.828968   \n4                     -6.126331                              -6.203797   \n...                         ...                                    ...   \n3977                  -5.541431                              -6.055966   \n3978                  -6.104084                              -6.062685   \n3979                  -5.750247                              -5.997140   \n3980                  -5.508211                              -5.966712   \n3981                  -5.312722                              -5.686786   \n\n      vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n0           -4.458188  -5.583466                   -5.584235      -5.631935  \n1           -4.321869  -5.634780                   -5.442592      -5.482084  \n2           -5.717096  -5.902942                   -5.789285      -5.825858  \n3           -4.485175  -5.755053                   -5.591357      -5.612889  \n4           -6.207918  -5.976230                   -5.991645      -6.031420  \n...               ...        ...                         ...            ...  \n3977        -4.649351  -6.066265                   -6.044125      -5.947447  \n3978        -6.022762  -5.869178                   -5.880138      -5.989139  \n3979        -5.708962  -5.886610                   -5.703576      -5.803680  \n3980        -4.853144  -5.817910                   -5.780413      -5.770779  \n3981        -4.710248  -5.606068                   -5.508401      -5.568897  \n\n[3982 rows x 207 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0004d9e33</td>\n      <td>-5.679960</td>\n      <td>-5.628830</td>\n      <td>-5.634180</td>\n      <td>-4.666551</td>\n      <td>-4.121871</td>\n      <td>-5.306041</td>\n      <td>-5.552208</td>\n      <td>-4.974575</td>\n      <td>-5.668497</td>\n      <td>...</td>\n      <td>-5.765751</td>\n      <td>-5.528400</td>\n      <td>-5.410370</td>\n      <td>-4.321980</td>\n      <td>-5.317147</td>\n      <td>-5.721891</td>\n      <td>-4.458188</td>\n      <td>-5.583466</td>\n      <td>-5.584235</td>\n      <td>-5.631935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_001897cda</td>\n      <td>-5.727033</td>\n      <td>-5.621867</td>\n      <td>-5.590736</td>\n      <td>-5.025705</td>\n      <td>-4.662688</td>\n      <td>-5.222306</td>\n      <td>-5.558617</td>\n      <td>-5.027584</td>\n      <td>-5.553468</td>\n      <td>...</td>\n      <td>-5.573226</td>\n      <td>-5.398426</td>\n      <td>-5.494564</td>\n      <td>-3.813975</td>\n      <td>-5.040711</td>\n      <td>-5.606177</td>\n      <td>-4.321869</td>\n      <td>-5.634780</td>\n      <td>-5.442592</td>\n      <td>-5.482084</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_002429b5b</td>\n      <td>-6.046939</td>\n      <td>-6.217653</td>\n      <td>-5.983761</td>\n      <td>-5.020549</td>\n      <td>-4.778184</td>\n      <td>-5.734206</td>\n      <td>-5.707179</td>\n      <td>-5.624209</td>\n      <td>-6.067981</td>\n      <td>...</td>\n      <td>-6.055518</td>\n      <td>-6.119366</td>\n      <td>-5.866686</td>\n      <td>-5.235461</td>\n      <td>-5.903933</td>\n      <td>-6.121309</td>\n      <td>-5.717096</td>\n      <td>-5.902942</td>\n      <td>-5.789285</td>\n      <td>-5.825858</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00276f245</td>\n      <td>-5.862432</td>\n      <td>-5.826532</td>\n      <td>-5.779856</td>\n      <td>-4.960424</td>\n      <td>-4.677885</td>\n      <td>-5.357684</td>\n      <td>-5.693943</td>\n      <td>-5.158955</td>\n      <td>-5.753490</td>\n      <td>...</td>\n      <td>-5.826924</td>\n      <td>-5.693624</td>\n      <td>-5.607389</td>\n      <td>-4.077273</td>\n      <td>-5.302030</td>\n      <td>-5.828968</td>\n      <td>-4.485175</td>\n      <td>-5.755053</td>\n      <td>-5.591357</td>\n      <td>-5.612889</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0027f1083</td>\n      <td>-5.980521</td>\n      <td>-6.269011</td>\n      <td>-6.043952</td>\n      <td>-5.057303</td>\n      <td>-4.746848</td>\n      <td>-5.932042</td>\n      <td>-5.787375</td>\n      <td>-5.742365</td>\n      <td>-6.221155</td>\n      <td>...</td>\n      <td>-6.212705</td>\n      <td>-6.233780</td>\n      <td>-5.904562</td>\n      <td>-5.864781</td>\n      <td>-6.126331</td>\n      <td>-6.203797</td>\n      <td>-6.207918</td>\n      <td>-5.976230</td>\n      <td>-5.991645</td>\n      <td>-6.031420</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3977</th>\n      <td>id_ff7004b87</td>\n      <td>-6.120609</td>\n      <td>-6.029621</td>\n      <td>-6.076323</td>\n      <td>-5.671113</td>\n      <td>-5.435343</td>\n      <td>-5.909560</td>\n      <td>-6.110951</td>\n      <td>-5.806555</td>\n      <td>-5.980929</td>\n      <td>...</td>\n      <td>-6.079699</td>\n      <td>-5.712262</td>\n      <td>-5.953594</td>\n      <td>-3.601126</td>\n      <td>-5.541431</td>\n      <td>-6.055966</td>\n      <td>-4.649351</td>\n      <td>-6.066265</td>\n      <td>-6.044125</td>\n      <td>-5.947447</td>\n    </tr>\n    <tr>\n      <th>3978</th>\n      <td>id_ff925dd0d</td>\n      <td>-5.804338</td>\n      <td>-6.027048</td>\n      <td>-5.967618</td>\n      <td>-4.978369</td>\n      <td>-4.265625</td>\n      <td>-5.740501</td>\n      <td>-5.606171</td>\n      <td>-5.475812</td>\n      <td>-6.117903</td>\n      <td>...</td>\n      <td>-6.143017</td>\n      <td>-6.083321</td>\n      <td>-5.804624</td>\n      <td>-6.088103</td>\n      <td>-6.104084</td>\n      <td>-6.062685</td>\n      <td>-6.022762</td>\n      <td>-5.869178</td>\n      <td>-5.880138</td>\n      <td>-5.989139</td>\n    </tr>\n    <tr>\n      <th>3979</th>\n      <td>id_ffb710450</td>\n      <td>-5.921171</td>\n      <td>-6.007148</td>\n      <td>-5.969432</td>\n      <td>-5.031055</td>\n      <td>-4.241337</td>\n      <td>-5.350413</td>\n      <td>-5.561746</td>\n      <td>-5.249198</td>\n      <td>-5.990728</td>\n      <td>...</td>\n      <td>-5.954805</td>\n      <td>-5.951075</td>\n      <td>-5.854402</td>\n      <td>-5.676965</td>\n      <td>-5.750247</td>\n      <td>-5.997140</td>\n      <td>-5.708962</td>\n      <td>-5.886610</td>\n      <td>-5.703576</td>\n      <td>-5.803680</td>\n    </tr>\n    <tr>\n      <th>3980</th>\n      <td>id_ffbb869f2</td>\n      <td>-5.901772</td>\n      <td>-5.928934</td>\n      <td>-5.872447</td>\n      <td>-5.050678</td>\n      <td>-4.389804</td>\n      <td>-5.599361</td>\n      <td>-5.518376</td>\n      <td>-5.399906</td>\n      <td>-5.866856</td>\n      <td>...</td>\n      <td>-6.018552</td>\n      <td>-5.951058</td>\n      <td>-5.599330</td>\n      <td>-4.957850</td>\n      <td>-5.508211</td>\n      <td>-5.966712</td>\n      <td>-4.853144</td>\n      <td>-5.817910</td>\n      <td>-5.780413</td>\n      <td>-5.770779</td>\n    </tr>\n    <tr>\n      <th>3981</th>\n      <td>id_ffd5800b6</td>\n      <td>-5.725790</td>\n      <td>-5.672912</td>\n      <td>-5.628758</td>\n      <td>-4.696895</td>\n      <td>-4.260148</td>\n      <td>-5.227330</td>\n      <td>-5.428792</td>\n      <td>-4.953642</td>\n      <td>-5.672046</td>\n      <td>...</td>\n      <td>-5.710971</td>\n      <td>-5.532167</td>\n      <td>-5.499734</td>\n      <td>-4.486096</td>\n      <td>-5.312722</td>\n      <td>-5.686786</td>\n      <td>-4.710248</td>\n      <td>-5.606068</td>\n      <td>-5.508401</td>\n      <td>-5.568897</td>\n    </tr>\n  </tbody>\n</table>\n<p>3982 rows × 207 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}