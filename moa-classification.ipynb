{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d   ## smoother\nfrom tqdm.notebook import tqdm\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.preprocessing import MinMaxScaler\n        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/test_features.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process_data(train_features, train_targets_scored):\n    \n    feature_columns = list(train_features.columns[1:])\n    target_columns = list(train_targets_scored.columns[1:])\n    \n    removal_list = ['cp_type', 'cp_dose']\n    for x in removal_list:\n        feature_columns.remove(x)\n        \n    train_cat = train_features.merge(train_targets_scored, on='sig_id')\n    train_cat = train_cat.select_dtypes(exclude=['object'])\n    \n    dummy_df = train_cat.loc[:, train_cat.columns != 'sig_id']\n    df_float = dummy_df.astype(float)\n#     scaler = MinMaxScaler()\n#     df_float_scaled = pd.DataFrame(scaler.fit_transform(df_float), columns = df_float.columns)\n#     df_float_scaled['sig_id'] = train_features['sig_id']\n    return(df_float)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_float_scaled = pre_process_data(train_features, train_targets_scored)\nfeature_columns = list(train_features.columns[1:])\nremoval_list = ['cp_type', 'cp_dose']\nfor x in removal_list:\n    feature_columns.remove(x)\ntarget_columns = list(train_targets_scored.columns[1:])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df, feature_columns, target_columns):\n        \n        self.features  = df[feature_columns].values\n        self.targets = df[target_columns].values\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float()\n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature,target","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_dataset = TrainDataset(df_float_scaled, feature_columns, target_columns)\n\ntrain_size = int(0.9 * len(full_dataset))  ## 90/10 split\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers = 8)\n\nval_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle = True, num_workers = 8)\n\nprint(len(train_loader), \"batches \")\nprint(len(val_loader), \" batches \")","execution_count":19,"outputs":[{"output_type":"stream","text":"335 batches \n38  batches \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \ndef show_deltas(deltas):\n    deltas = gaussian_filter1d(deltas, sigma=3)\n    plt.plot(deltas, \"r\", label = \"Absolute error from label \")\n    plt.ylabel(\"Error\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \ndef show_losses(loss_list):\n    plt.plot(loss_list, 'r', label = 'Loss')\n    plt.ylabel(\"Loss\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \ndef train_step(x, y, model, optimizer, criterion):\n    optimizer.zero_grad()\n    pred = model(x.to(device))\n    y = y.float()\n    loss = criterion(pred,y.to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('Linear') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        \n        \n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(873)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(873, 2048))\n        \n        self.batch_norm2 = nn.BatchNorm1d(2048)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n        \n        self.batch_norm3 = nn.BatchNorm1d(1048)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = (self.dense3(x))\n        \n        return x\n    \n    \n    \nmodel = Model()\nmodel = model.to(device)\nmodel.apply(weights_init)\nprint(model)\n\n\n","execution_count":35,"outputs":[{"output_type":"stream","text":"Model(\n  (batch_norm1): BatchNorm1d(873, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.2, inplace=False)\n  (dense1): Linear(in_features=873, out_features=2048, bias=True)\n  (batch_norm2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (dense2): Linear(in_features=2048, out_features=1048, bias=True)\n  (batch_norm3): BatchNorm1d(1048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout3): Dropout(p=0.5, inplace=False)\n  (dense3): Linear(in_features=1048, out_features=206, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = []\nval_losses = []\nlearning_rates = []\naverage_deltas = []\nval_corr=[]\n\noptimizer = optim.Adam(model.parameters(), lr=0.8e-3)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                 mode='min', \n                                                 factor=0.8, \n                                                 patience=3, \n                                                 eps=1e-4, \n                                                 verbose=True)\ncriterion = nn.BCEWithLogitsLoss()\n","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs=20\nfor epoch in range(num_epochs):\n    \n    total_loss = 0\n    total_correct = 0\n    total_loss2 = 0\n    total_correct2 = 0\n    \n    print (\"epoch \", epoch+1, \" out of \", num_epochs )\n\n    model.train()\n    for batch in tqdm(train_loader, desc = \" Training batches : \"):\n        (x_batch, y_batch) = batch\n        loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n        losses.append(loss)\n    scheduler.step(1.)   ## lr decay caller \n    learning_rates.append(get_lr(optimizer))\n    \n    \n    with torch.no_grad():\n        model.eval()\n\n        for x_val, y_val in tqdm(val_loader, desc = \"running on test set --\"):\n            yhat =model(x_val.to(device))  # pred \n            val_loss = criterion(yhat.to(device), y_val.to(device))\n            val_losses.append(torch.mean(val_loss.view(-1)).item())  ## metrics \n\n\n\n    clear_output(wait = True)\n\n    show_lr(learning_rates)\n    show_losses(val_losses)\n    show_losses(losses)\n    #show_deltas(average_deltas)","execution_count":null,"outputs":[{"output_type":"stream","text":"epoch  1  out of  50\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description=' Training batches : ', max=335.0, style=ProgressStyle(desâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac2a59ea179411db8a4f975411b1267"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(val_losses)","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"0.0006734605995006859"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(val_losses) \n## old = 0.010247811675071716 \n## with leaky = 0.010694394819438457  \n# with weights init = 0.00914179626852274\n# with better LRscheduler = 0.0083483150228858, 0.007986623793840408\n## all run = 0.002\n## unscaled data = 0.0006","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"0.0006734605995006859"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(val_losses[39:])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pre_process_data(test_features, submission )\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TrainDataset(test_df, feature_columns, target_columns)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_preds=[]\nwith torch.no_grad():\n    model.eval()\n    for x_test, y_test in tqdm(test_loader, desc = \"running on test set --\"):\n        pred =model(x_test.to(device, dtype=torch.float))  # pred \n        pred = pred.cpu()\n        pred = pred.sigmoid()\n        list_of_preds.append(list(pred[0].numpy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub_cp = submission\nsub_cp.to_csv('./submission_cp.csv', index=None, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv \na = list_of_preds  \nwith open('./submission_cp.csv', \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerows(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub = pd.read_csv('./submission_cp.csv', header = None)\nfinal_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.columns = submission.columns[1:]\nfinal_sub[\"sig_id\"] = submission[\"sig_id\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_cols = np.roll(final_sub.columns.values, 1)\nfinal_sub = final_sub[good_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.to_csv(\"./submission.csv\", index=False)\nfinal_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}