{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17p9p0CUOY4YpstELAAIXooVRZpv9YYCy",
      "authorship_tag": "ABX9TyOOtJYr0rrZ1a+E1C5PXtIh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f26f6d43a064ad7b73f7eba40fb931a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_72136e0ad42949e98dd4719e7e9e1cb1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab4513211cde43878bc753b826eb1bed",
              "IPY_MODEL_d4aca827b9ee4de280febdc374548bb4"
            ]
          }
        },
        "72136e0ad42949e98dd4719e7e9e1cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab4513211cde43878bc753b826eb1bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b29cd26fede54a0085ddc70e5d8b020d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3982,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3982,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12012af0b5e9453586ee6a2823118366"
          }
        },
        "d4aca827b9ee4de280febdc374548bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9fa71aeda27b48fc91efc647cebcef02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3982/3982 [04:55&lt;00:00, 13.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3c177d9a68144378dcb6a7b1d3d23db"
          }
        },
        "b29cd26fede54a0085ddc70e5d8b020d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12012af0b5e9453586ee6a2823118366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fa71aeda27b48fc91efc647cebcef02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3c177d9a68144378dcb6a7b1d3d23db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOd8RhyuZzp2"
      },
      "source": [
        "!cp /content/drive/\"My Drive\"/kaggle/moa/lish-moa.zip /content/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSua2cgfd2hn",
        "outputId": "950a78b2-d7dc-43b1-f7f8-2cc0b844c6e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!unzip lish-moa.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  lish-moa.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_features.csv       \n",
            "  inflating: train_features.csv      \n",
            "  inflating: train_targets_nonscored.csv  \n",
            "  inflating: train_targets_scored.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB9siuMCJ5pd",
        "outputId": "b7ebdcac-a771-4663-e7ae-fa155b2b8e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!pip install pip install iterative-stratification"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Collecting install\n",
            "  Downloading https://files.pythonhosted.org/packages/41/cf/e3e6b4d494051c07261cae8c403f0f0d0cedad43d980e5255f2c88fd5edf/install-1.3.3-py3-none-any.whl\n",
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.16.0)\n",
            "Installing collected packages: install, iterative-stratification\n",
            "Successfully installed install-1.3.3 iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUpXeUlWeXpI",
        "outputId": "86661f6f-1a23-44cb-8d36-53adf4d2f2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "        \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter1d   ## smoother\n",
        "from tqdm.notebook import tqdm, tnrange\n",
        "import random\n",
        "import os\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 15, 7\n",
        "\n",
        "CGREEN  = '\\33[32m'\n",
        "CBLUE =  '\\033[34m'\n",
        "CRED = '\\033[1;31m'\n",
        "CEND  = '\\33[0m'\n",
        "\n",
        "def seed_everything(seed=1903):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=42)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0UcuWgBr0Hn",
        "outputId": "3182d65f-fdf8-4e7e-8ca2-9706d99a530e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "else:\n",
        "    device='cpu'\n",
        "    \n",
        "device\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZhAwZ3hr45u"
      },
      "source": [
        "train_features = pd.read_csv('train_features.csv')\n",
        "train_targets = pd.read_csv('train_targets_scored.csv')\n",
        "test_features = pd.read_csv('test_features.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWH98yYwp-w"
      },
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    return df\n",
        "\n",
        "train = preprocess(train_features)\n",
        "test = preprocess(test_features)\n",
        "\n",
        "del train_targets['sig_id']\n",
        "\n",
        "target = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n",
        "train = train.loc[train['cp_type']==0].reset_index(drop=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0AtYxrP0kpp"
      },
      "source": [
        "top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "       870, 871, 872, 873, 874]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn34nXaa0qP1"
      },
      "source": [
        "all_columns = train.columns\n",
        "train=train[all_columns[top_features]]\n",
        "test = test[all_columns[top_features]]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBFXoVhT5X4",
        "outputId": "53da6d46-7f4e-496b-fcb1-1fd3a09b3fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21948, 785), (3982, 785))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O52yoAOM14k4"
      },
      "source": [
        "train = train.values\n",
        "target = target.values\n",
        "test = test.values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N0QYVKDsTm-"
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train,targets, noise ):\n",
        "        \n",
        "        self.features  = train\n",
        "        self.targets = targets\n",
        "        self.noise = noise\n",
        "        \n",
        "    def sizes(self):\n",
        "        print(\"features size = \", self.features.shape[1])\n",
        "        print(\"targets size = \", self.targets.shape[1])\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx]).float()\n",
        "            \n",
        "        target = torch.tensor(self.targets[idx]).float()\n",
        "        \n",
        "        return feature, target\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCn4Ba6EsVyt"
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "        \n",
        "def show_lr(learning_rates):\n",
        "    plt.plot(learning_rates, label = \"learning rate\")\n",
        "    plt.ylabel(\"Learning rate\", fontsize = 15)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_step(x, y, model, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x.to(device))\n",
        "    y = y.float()\n",
        "    loss = criterion(pred,y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuz3mR5akCow",
        "outputId": "eb596969-5e4d-4d1c-99e0-4244ab8a1be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "[I 2020-09-26 23:26:27,234] Trial 77 finished with value: 0.015594600699841976 \n",
        "and parameters: {\n",
        "    'num_layer': 3, \n",
        "    'hidden_size': 2076, \n",
        "    'dropout': 0.5145663015913359, \n",
        "    'learning_rate': 0.0037416442804666648\n",
        "}. t\n",
        "Best is trial 77 with value: 0.015594600699841976.\n",
        "\"\"\"\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n[I 2020-09-26 23:26:27,234] Trial 77 finished with value: 0.015594600699841976 \\nand parameters: {\\n    'num_layer': 3, \\n    'hidden_size': 2076, \\n    'dropout': 0.5145663015913359, \\n    'learning_rate': 0.0037416442804666648\\n}. t\\nBest is trial 77 with value: 0.015594600699841976.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZqPFz-CsX4R"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(785)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(785, 2048))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, 206))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6q-PUbsezZ"
      },
      "source": [
        "\n",
        "def train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n",
        "    \n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    learning_rates = []    \n",
        "    best_loss = 1000000\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "            \n",
        "        if train == True:\n",
        "            model.train()\n",
        "            losses_temp = []\n",
        "            for batch in train_loader:\n",
        "                (x_batch, y_batch) = batch\n",
        "                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n",
        "                losses_temp.append(loss)\n",
        "            losses.append(torch.mean(torch.tensor(losses_temp)))\n",
        "            scheduler.step(1.)   ## lr decay caller \n",
        "            learning_rates.append(get_lr(optimizer))\n",
        "            \n",
        "\n",
        "        if validate == True:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_losses_temp = []\n",
        "                for x_val, y_val in val_loader:\n",
        "                    yhat =model(x_val.to(device))  # pred \n",
        "                    val_loss = criterion(yhat.to(device), y_val.to(device))\n",
        "                    val_losses_temp.append(val_loss.item())  ## metrics \n",
        "                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n",
        "\n",
        "        \n",
        "        if train == True:\n",
        "            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n",
        "\n",
        "            if val_losses[-1] <= best_loss:\n",
        "\n",
        "                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n",
        "                best_loss = val_losses[-1]\n",
        "                name = \"./model_\" + str(fold_number)+\".pth\"\n",
        "                print(\"saving model as: \", name)\n",
        "                torch.save(model.state_dict(), name)\n",
        "\n",
        "            else: \n",
        "                print(\"showing no improvements, best loss yet:\", best_loss)\n",
        "\n",
        "        if show_plots == True:\n",
        "\n",
        "            show_lr(learning_rates)\n",
        "            plt.plot(val_losses, label = \"val\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.plot(losses, label = \"train\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "    if train == True:\n",
        "        return losses, val_losses, name \n",
        "    else:\n",
        "        return losses, val_losses"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdN_UgGAskZK"
      },
      "source": [
        "\n",
        "\n",
        "def train_model(num_folds, num_epochs, batch_size, lr =  0.004299882049752947, save_code = 0, ensemble = False, ensemble_model_paths = [] ):\n",
        "\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
        "\n",
        "    fold_val_losses = list()\n",
        "    filenames = []\n",
        "\n",
        "\n",
        "    for k , (train_idx,valid_idx) in enumerate(mskf.split(train,target)):\n",
        "\n",
        "        x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n",
        "\n",
        "        input_size = x_train.shape[1]\n",
        "        output_size = target.shape[1]\n",
        "        \n",
        "        train_dataset = TrainDataset(x_train, y_train, noise = False)\n",
        "        valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n",
        "        \n",
        "        train_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True)\n",
        "        \n",
        "        if ensemble == False:\n",
        "            model = Model()\n",
        "        else:\n",
        "            model = MyEnsemble(ensemble_model_paths, device)\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr = lr , weight_decay=1e-5)\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                        mode='min', \n",
        "                                                        factor=0.1, ## wooo hoo\n",
        "                                                        patience=7, ## was 3 for 158 \n",
        "                                                        eps=1e-4, \n",
        "                                                        verbose=True)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        if k > 1:\n",
        "            print(CRED ,\"fold \", str(k+1), \"  :: mean loss on all folds: \", np.array([min(l) for l in fold_val_losses]).mean(), CEND)\n",
        "    \n",
        "\n",
        "        losses, val_losses, filename = train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+ save_code+1)\n",
        "\n",
        "        fold_val_losses.append(val_losses)\n",
        "        filenames.append(filename)\n",
        "    print(CBLUE, \"Training complete\", CEND)\n",
        "\n",
        "    return fold_val_losses, filenames"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA6r6ST82xKu",
        "outputId": "5c14913c-4c2c-4c47-b904-add0c2a8dd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "individual_losses , filenames = train_model(\n",
        "                                    num_folds = 7,\n",
        "                                    num_epochs = 45,\n",
        "                                    batch_size = 128,\n",
        "                                    save_code = 0\n",
        "                                )"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020819269120693207 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020819269120693207  to  0.019304858520627022 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019304858520627022  to  0.01851486973464489 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01851486973464489  to  0.01765020377933979 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.01765020377933979  to  0.017595935612916946 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017595935612916946  to  0.017429543659090996 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.017429543659090996\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017429543659090996  to  0.016992412507534027 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.016992412507534027\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.016992412507534027  to  0.01651764288544655 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.01651764288544655  to  0.016484787687659264 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016484787687659264  to  0.01623602584004402 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.01623602584004402  to  0.01618320122361183 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.01618320122361183  to  0.016005797311663628 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016005797311663628\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.016005797311663628\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016005797311663628\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016005797311663628\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.016005797311663628  to  0.015812551602721214 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015812551602721214\n",
            "epoch  30  out of  45      >\u001b[32m Val loss decreased from: 0.015812551602721214  to  0.0157700777053833 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.0157700777053833\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020930279046297073 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020930279046297073  to  0.01907561905682087 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.01907561905682087  to  0.018249385058879852 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018249385058879852  to  0.01797197014093399 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.01797197014093399  to  0.017327262088656425 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017327262088656425  to  0.017252117395401 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.017252117395401\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017252117395401  to  0.017149265855550766 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017149265855550766  to  0.016789792105555534 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.016789792105555534  to  0.016355257481336594 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016355257481336594  to  0.016134347766637802 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  12  out of  45      >showing no improvements, best loss yet: 0.016134347766637802\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.016134347766637802\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016134347766637802  to  0.016065634787082672 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016065634787082672  to  0.01588675007224083 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01588675007224083\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01588675007224083  to  0.015827490016818047 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.015827490016818047\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015827490016818047\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015827490016818047\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015827490016818047\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.015827490016818047  to  0.015673013404011726 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015673013404011726\n",
            "epoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.015673013404011726  to  0.015641115605831146 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015641115605831146\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015641115605831146  to  0.015596238896250725 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015596238896250725\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015596238896250725\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015596238896250725\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015596238896250725  to  0.01550215668976307 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01550215668976307\n",
            "\u001b[1;31m fold  3   :: mean loss on all folds:  0.015636117197573185 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.021107934415340424 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.021107934415340424  to  0.019364111125469208 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019364111125469208  to  0.0183845367282629 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.0183845367282629  to  0.018299976363778114 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.018299976363778114  to  0.017432887107133865 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017432887107133865  to  0.017417650669813156 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  7  out of  45      >showing no improvements, best loss yet: 0.017417650669813156\n",
            "epoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017417650669813156  to  0.017387446016073227 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017387446016073227  to  0.01716032437980175 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.01716032437980175  to  0.016771851107478142 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016771851107478142  to  0.016519194468855858 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  12  out of  45      >showing no improvements, best loss yet: 0.016519194468855858\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016519194468855858  to  0.016366878524422646 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016366878524422646  to  0.01617913693189621 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.01617913693189621\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01617913693189621\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01617913693189621\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.01617913693189621  to  0.016152750700712204 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016152750700712204\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016152750700712204  to  0.015939917415380478 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015939917415380478\n",
            "epoch  35  out of  45      >\u001b[32m Val loss decreased from: 0.015939917415380478  to  0.015911219641566277 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015911219641566277\n",
            "epoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.015911219641566277  to  0.015889696776866913 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015889696776866913\n",
            "epoch  45  out of  45      >\u001b[32m Val loss decreased from: 0.015889696776866913  to  0.015857834368944168 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "\u001b[1;31m fold  4   :: mean loss on all folds:  0.015710022921363514 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020886054262518883 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020886054262518883  to  0.019197184592485428 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019197184592485428  to  0.018276959657669067 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018276959657669067  to  0.017598900943994522 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  5  out of  45      >showing no improvements, best loss yet: 0.017598900943994522\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017598900943994522  to  0.01729760318994522 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01729760318994522  to  0.01705353707075119 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.01705353707075119\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.01705353707075119\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.01705353707075119  to  0.016622669994831085 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016622669994831085  to  0.016280170530080795 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016280170530080795  to  0.016051072627305984 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016051072627305984\n",
            "epoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.016051072627305984  to  0.015875259414315224 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.015875259414315224  to  0.015819460153579712 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015819460153579712\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015819460153579712\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015819460153579712\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015819460153579712\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.015819460153579712  to  0.01577792689204216 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.01577792689204216\n",
            "epoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.01577792689204216  to  0.015657253563404083 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015657253563404083\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015657253563404083\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015657253563404083\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015657253563404083\n",
            "epoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.015657253563404083  to  0.01565001718699932 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.01565001718699932\n",
            "\u001b[1;31m fold  5   :: mean loss on all folds:  0.015695021487772465 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.021159566938877106 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.021159566938877106  to  0.01967841573059559 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.01967841573059559  to  0.018490701913833618 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018490701913833618  to  0.017924750223755836 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017924750223755836  to  0.01746596023440361 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  6  out of  45      >showing no improvements, best loss yet: 0.01746596023440361\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01746596023440361  to  0.017430774867534637 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017430774867534637\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.017430774867534637\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017430774867534637  to  0.016797853633761406 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016797853633761406  to  0.016385285183787346 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016385285183787346  to  0.016381440684199333 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016381440684199333  to  0.01627320423722267 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.01627320423722267\n",
            "epoch  15  out of  45      >showing no improvements, best loss yet: 0.01627320423722267\n",
            "epoch  16  out of  45      >showing no improvements, best loss yet: 0.01627320423722267\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01627320423722267\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.01627320423722267  to  0.01627310924232006 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.01627310924232006\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.01627310924232006\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.01627310924232006  to  0.01624881662428379 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.01624881662428379\n",
            "epoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.01624881662428379  to  0.016168130561709404 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.016168130561709404\n",
            "epoch  25  out of  45      >\u001b[32m Val loss decreased from: 0.016168130561709404  to  0.01615188457071781 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.01615188457071781  to  0.016051797196269035 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.016051797196269035\n",
            "epoch  40  out of  45      >\u001b[32m Val loss decreased from: 0.016051797196269035  to  0.015975646674633026 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015975646674633026\n",
            "epoch  42  out of  45      >\u001b[32m Val loss decreased from: 0.015975646674633026  to  0.015840450301766396 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015840450301766396\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015840450301766396\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015840450301766396\n",
            "\u001b[1;31m fold  6   :: mean loss on all folds:  0.015724107250571252 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02113175578415394 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02113175578415394  to  0.019369682297110558 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019369682297110558  to  0.018261924386024475 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.018261924386024475  to  0.018064189702272415 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.018064189702272415  to  0.017414860427379608 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  6  out of  45      >showing no improvements, best loss yet: 0.017414860427379608\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017414860427379608  to  0.017187349498271942 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017187349498271942\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.017187349498271942  to  0.017028743401169777 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017028743401169777  to  0.016483260318636894 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016483260318636894  to  0.016476992517709732 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016476992517709732  to  0.0162747073918581 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.0162747073918581\n",
            "epoch  14  out of  45      >showing no improvements, best loss yet: 0.0162747073918581\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.0162747073918581  to  0.016169868409633636 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.016169868409633636  to  0.01595752313733101 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >showing no improvements, best loss yet: 0.01595752313733101\n",
            "epoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.01595752313733101  to  0.015859732404351234 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.015859732404351234\n",
            "epoch  20  out of  45      >showing no improvements, best loss yet: 0.015859732404351234\n",
            "epoch  21  out of  45      >showing no improvements, best loss yet: 0.015859732404351234\n",
            "epoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.015859732404351234  to  0.01567227579653263 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  26  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  27  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  29  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.01567227579653263\n",
            "epoch  40  out of  45      >\u001b[32m Val loss decreased from: 0.01567227579653263  to  0.015571312978863716 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015571312978863716\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015571312978863716\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015571312978863716\n",
            "epoch  44  out of  45      >showing no improvements, best loss yet: 0.015571312978863716\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015571312978863716\n",
            "\u001b[1;31m fold  7   :: mean loss on all folds:  0.015698641538619995 \u001b[0m\n",
            "epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.02077563665807247 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.02077563665807247  to  0.018872616812586784 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.018872616812586784  to  0.01820017769932747 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01820017769932747  to  0.017763597890734673 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017763597890734673  to  0.0174797885119915 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.0174797885119915  to  0.01712612248957157 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01712612248957157  to  0.017030389979481697 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  8  out of  45      >showing no improvements, best loss yet: 0.017030389979481697\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  45      >showing no improvements, best loss yet: 0.017030389979481697\n",
            "epoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017030389979481697  to  0.016436085104942322 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  11  out of  45      >showing no improvements, best loss yet: 0.016436085104942322\n",
            "epoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016436085104942322  to  0.016283277422189713 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  13  out of  45      >showing no improvements, best loss yet: 0.016283277422189713\n",
            "epoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016283277422189713  to  0.016139967367053032 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016139967367053032  to  0.01611439324915409 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01611439324915409  to  0.016036661341786385 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "Epoch    17: reducing learning rate of group 0 to 4.2999e-05.\n",
            "epoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.016036661341786385  to  0.016021935269236565 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  18  out of  45      >showing no improvements, best loss yet: 0.016021935269236565\n",
            "epoch  19  out of  45      >showing no improvements, best loss yet: 0.016021935269236565\n",
            "epoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016021935269236565  to  0.015980439260601997 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  21  out of  45      >\u001b[32m Val loss decreased from: 0.015980439260601997  to  0.015917878597974777 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  22  out of  45      >showing no improvements, best loss yet: 0.015917878597974777\n",
            "epoch  23  out of  45      >showing no improvements, best loss yet: 0.015917878597974777\n",
            "epoch  24  out of  45      >showing no improvements, best loss yet: 0.015917878597974777\n",
            "epoch  25  out of  45      >showing no improvements, best loss yet: 0.015917878597974777\n",
            "epoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.015917878597974777  to  0.015901165083050728 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.015901165083050728  to  0.015863554552197456 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  28  out of  45      >showing no improvements, best loss yet: 0.015863554552197456\n",
            "epoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.015863554552197456  to  0.015733646228909492 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  30  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  31  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  32  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  33  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  34  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  35  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  36  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  37  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  38  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  39  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  40  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  41  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  42  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  43  out of  45      >showing no improvements, best loss yet: 0.015733646228909492\n",
            "epoch  44  out of  45      >\u001b[32m Val loss decreased from: 0.015733646228909492  to  0.015632253140211105 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  45  out of  45      >showing no improvements, best loss yet: 0.015632253140211105\n",
            "\u001b[34m Training complete \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNT4tBQ_tPBA"
      },
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self, model_list, device):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        \n",
        "        print(\"loading models...\")\n",
        "        self.model_filenames = model_list\n",
        "        self.model_list = [Model() for m in range(len(self.model_filenames))]\n",
        "\n",
        "        for i in range(len(self.model_filenames)):\n",
        "            self.model_list[i].load_state_dict(torch.load(self.model_filenames[i]))\n",
        "            self.model_list[i].to(device)\n",
        "            self.model_list[i].eval()\n",
        "\n",
        "        self.fc1 = nn.Linear(206*len(self.model_list), 2024)\n",
        "        self.fc2 = nn.Linear(2024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 206)\n",
        "        print(\"ensemble initialised with \" , len(self.model_list), \" models\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x_list = [self.model_list[i](x) for i in range(len(self.model_list))]\n",
        "        joined =  torch.cat(tuple(x_list), dim=1)\n",
        "        x = F.relu(self.fc1(joined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlM-Rj1G4uCr",
        "outputId": "9c926e77-9e89-4c84-824d-779c9b06dce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "filenames"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./model_1.pth',\n",
              " './model_2.pth',\n",
              " './model_3.pth',\n",
              " './model_4.pth',\n",
              " './model_5.pth',\n",
              " './model_6.pth',\n",
              " './model_7.pth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ORdaIAbp3f6",
        "outputId": "d4c62cf1-050c-4223-a384-3c160a34c8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "source": [
        "en_individual_losses , en_filenames = train_model(\n",
        "                                    num_folds = 7,\n",
        "                                    num_epochs = 75,\n",
        "                                    batch_size = 128,\n",
        "                                    save_code = 100,\n",
        "                                    ensemble = True,\n",
        "                                    ensemble_model_paths = filenames,\n",
        "                                    lr = 1e-3\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.0197810847312212 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.0197810847312212  to  0.017576148733496666 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.017576148733496666  to  0.016240771859884262 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.016240771859884262  to  0.015310490503907204 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.015310490503907204  to  0.014800924807786942 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.014800924807786942  to  0.014158079400658607 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  7  out of  75      >showing no improvements, best loss yet: 0.014158079400658607\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.014158079400658607  to  0.013542275875806808 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  9  out of  75      >showing no improvements, best loss yet: 0.013542275875806808\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.013542275875806808  to  0.01304987445473671 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.01304987445473671  to  0.012984169647097588 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.012984169647097588\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.012984169647097588  to  0.012887989170849323 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  14  out of  75      >\u001b[32m Val loss decreased from: 0.012887989170849323  to  0.012755410745739937 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.012755410745739937\n",
            "epoch  16  out of  75      >showing no improvements, best loss yet: 0.012755410745739937\n",
            "epoch  17  out of  75      >showing no improvements, best loss yet: 0.012755410745739937\n",
            "epoch  18  out of  75      >\u001b[32m Val loss decreased from: 0.012755410745739937  to  0.012688398361206055 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  19  out of  75      >\u001b[32m Val loss decreased from: 0.012688398361206055  to  0.012629909440875053 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.012629909440875053\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.012629909440875053\n",
            "epoch  22  out of  75      >\u001b[32m Val loss decreased from: 0.012629909440875053  to  0.012551912106573582 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.012551912106573582\n",
            "epoch  24  out of  75      >\u001b[32m Val loss decreased from: 0.012551912106573582  to  0.01248580776154995 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  25  out of  75      >showing no improvements, best loss yet: 0.01248580776154995\n",
            "epoch  26  out of  75      >\u001b[32m Val loss decreased from: 0.01248580776154995  to  0.012454161420464516 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  27  out of  75      >\u001b[32m Val loss decreased from: 0.012454161420464516  to  0.012240475043654442 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.012240475043654442\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.012240475043654442\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.012240475043654442\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.012240475043654442\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.012240475043654442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoiFc0ol31-Y"
      },
      "source": [
        "!cp model_100.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_101.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_102.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_103.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_104.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_105.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_106.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_107.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_108.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_109.pth /content/drive/\"My Drive\"/kaggle/moa\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFdFCHQ50jPU"
      },
      "source": [
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_100.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_101.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_102.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_103.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_104.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_105.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_106.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_107.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_108.pth /content/\n",
        "# !cp /content/drive/\"My Drive\"/kaggle/moa/model_109.pth /content/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6HNY_z-EyxZ",
        "outputId": "1e56642f-7c58-45db-ee36-340d94e78584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "filenames"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./model_1.pth',\n",
              " './model_2.pth',\n",
              " './model_3.pth',\n",
              " './model_4.pth',\n",
              " './model_5.pth',\n",
              " './model_6.pth',\n",
              " './model_7.pth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzyTvqy27niM",
        "outputId": "72472bed-4aa8-4af9-c02a-2c7f5d3ad7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "all_models = [MyEnsemble(filenames, device) for i in range (5)]\n",
        "\n",
        "for i in range (len(all_models)):\n",
        "    \n",
        "    name = \"./model_\" + str(100 + i + 1) + \".pth\"\n",
        "    all_models[i].load_state_dict(torch.load(name))\n",
        "    all_models[i].to(device)\n",
        "    all_models[i].eval()\n",
        "    print(\"Loaded: \", name)\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "Loaded:  ./model_101.pth\n",
            "Loaded:  ./model_102.pth\n",
            "Loaded:  ./model_103.pth\n",
            "Loaded:  ./model_104.pth\n",
            "Loaded:  ./model_105.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXObvCTzaZru",
        "outputId": "431a8de2-f266-4096-a0d0-0b9e275cc8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "all_val_losses = []\n",
        "for i in range(len(all_models)):\n",
        "    print(i)\n",
        "    losses, val_losses = train_one_fold(all_models[i],1 , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 0, train = False, validate = True)\n",
        "    all_val_losses.append(np.mean(np.array(val_losses)))\n",
        "all_val_losses = np.array(all_val_losses)\n",
        "print(\"done validating\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "done validating\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGOb-mslGKuv",
        "outputId": "c7eb40ea-63cc-45de-e538-5d0f79c47279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "all_val_losses"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01221509, 0.01267824, 0.01224699, 0.01227677, 0.01283966])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsKaVNi4bXma"
      },
      "source": [
        "class model_jury(object):   ## only works for dataloaders for batch size 1 \n",
        "    def __init__(self, all_models):\n",
        "        self.all_models = all_models\n",
        "             \n",
        "    def predict(self, x, plot = False, sigmoid = False):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            if sigmoid == False:\n",
        "                preds = [self.all_models[i](x.to(device)).view(-1).cpu().tolist() for i in range(len(self.all_models))]\n",
        "            else:\n",
        "                preds = [self.all_models[i](x.to(device)).view(-1).cpu().sigmoid().tolist() for i in range(len(self.all_models))]\n",
        "\n",
        "        if plot == True:\n",
        "            for pred in preds:\n",
        "                plt.plot(pred)\n",
        "            plt.show()\n",
        "            \n",
        "        preds = np.array(preds)\n",
        "        mean = np.mean(preds, axis = 0)\n",
        "        return mean.flatten()\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atizQ3kF0hLc",
        "outputId": "df910744-b0d5-4989-82c3-0997f05cc867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "jury = model_jury(all_models)\n",
        "\"Using \" + str(len(jury.all_models)) + \"  models\""
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Using 5  models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E65h1GQTbZWe"
      },
      "source": [
        "test_dataset = TrainDataset(test, target, noise = False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
        "val_loader_test_jury = DataLoader(dataset= valid_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsNmfHMQOe5-",
        "outputId": "5e84d939-a7a3-499a-f7de-ce78ebb3055a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9f26f6d43a064ad7b73f7eba40fb931a",
            "72136e0ad42949e98dd4719e7e9e1cb1",
            "ab4513211cde43878bc753b826eb1bed",
            "d4aca827b9ee4de280febdc374548bb4",
            "b29cd26fede54a0085ddc70e5d8b020d",
            "12012af0b5e9453586ee6a2823118366",
            "9fa71aeda27b48fc91efc647cebcef02",
            "a3c177d9a68144378dcb6a7b1d3d23db"
          ]
        }
      },
      "source": [
        "list_of_preds = []\n",
        "for batch in tqdm(test_loader):\n",
        "    x, y = batch\n",
        "    foo = jury.predict(x, plot = False, sigmoid = True)\n",
        "    list_of_preds.append(foo)\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f26f6d43a064ad7b73f7eba40fb931a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3982.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNHdq359gObR"
      },
      "source": [
        "submission = pd.read_csv('sample_submission.csv')\n",
        "sub_cp = submission\n",
        "sub_cp.to_csv('./submission_cp.csv', index=None, header=True)\n",
        "\n",
        "import csv \n",
        "a = list_of_preds  \n",
        "with open('./submission_cp.csv', \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(a)\n",
        "\n",
        "final_sub = pd.read_csv('./submission_cp.csv', header = None)\n",
        "\n",
        "final_sub.columns = submission.columns[1:]\n",
        "final_sub[\"sig_id\"] = submission[\"sig_id\"]\n",
        "\n",
        "good_cols = np.roll(final_sub.columns.values, 1)\n",
        "final_sub = final_sub[good_cols]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtN2F5ib61s"
      },
      "source": [
        "targets = [col for col in final_sub.columns]\n",
        "final_sub.loc[test_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\n",
        "final_sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLsVOaUAG3gf",
        "outputId": "65626f53-1f01-4001-faf3-12c903643b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "final_sub"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>1.062902e-05</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.018909</td>\n",
              "      <td>0.030439</td>\n",
              "      <td>0.005953</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>1.160462e-08</td>\n",
              "      <td>0.006177</td>\n",
              "      <td>0.008534</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>2.885769e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.001898</td>\n",
              "      <td>5.044409e-04</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.005188</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.003545</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.003815</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.004068</td>\n",
              "      <td>0.004161</td>\n",
              "      <td>1.112736e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.415943e-07</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>1.109351e-07</td>\n",
              "      <td>1.197417e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.004941</td>\n",
              "      <td>1.039965e-06</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>3.115015e-05</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>0.006124</td>\n",
              "      <td>0.003520</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.054740</td>\n",
              "      <td>0.004302</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>2.165041e-05</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>1.240706e-08</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>7.635799e-05</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.001906</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000957</td>\n",
              "      <td>2.396159e-04</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>7.569705e-04</td>\n",
              "      <td>0.000079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>2.260456e-07</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.008590</td>\n",
              "      <td>1.545862e-02</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>7.033517e-08</td>\n",
              "      <td>0.021664</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>2.905526e-07</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.003985</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>1.451287e-04</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>4.654794e-08</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>8.279389e-03</td>\n",
              "      <td>2.352935e-04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>7.278010e-07</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>9.111682e-06</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.050590</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.006411</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.003737</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.028157</td>\n",
              "      <td>8.230975e-07</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>6.307424e-05</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>5.678565e-08</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>7.315809e-07</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>7.318613e-06</td>\n",
              "      <td>0.002245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>5.039742e-06</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.013312</td>\n",
              "      <td>0.019374</td>\n",
              "      <td>0.001753</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.006905</td>\n",
              "      <td>5.773765e-08</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>0.019274</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>1.405586e-06</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>2.383033e-03</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.003092</td>\n",
              "      <td>0.001560</td>\n",
              "      <td>0.001367</td>\n",
              "      <td>0.002063</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>0.006056</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.001282</td>\n",
              "      <td>1.839997e-04</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>2.251017e-07</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>9.487835e-07</td>\n",
              "      <td>4.553504e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>9.466257e-06</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>3.879312e-07</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.019126</td>\n",
              "      <td>0.052328</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.005107</td>\n",
              "      <td>0.000780</td>\n",
              "      <td>0.005728</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>3.804671e-04</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.007915</td>\n",
              "      <td>8.596392e-08</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>2.846896e-04</td>\n",
              "      <td>0.002335</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>8.748351e-04</td>\n",
              "      <td>0.006550</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.002722</td>\n",
              "      <td>2.169942e-05</td>\n",
              "      <td>0.001038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>1.390811e-04</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.005404</td>\n",
              "      <td>0.032306</td>\n",
              "      <td>0.033079</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>0.003484</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>2.356906e-07</td>\n",
              "      <td>0.006107</td>\n",
              "      <td>0.012963</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>3.359609e-08</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.003583</td>\n",
              "      <td>1.936807e-03</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.003428</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.006739</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.004921</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.009773</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>1.457312e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>2.512226e-07</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>1.813239e-06</td>\n",
              "      <td>5.262109e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006769</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.007224</td>\n",
              "      <td>1.768752e-03</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>4.041204e-06</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.009353</td>\n",
              "      <td>0.003890</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.024483</td>\n",
              "      <td>0.002872</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>5.411162e-03</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>8.198553e-07</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>4.900316e-04</td>\n",
              "      <td>0.001883</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.008252</td>\n",
              "      <td>7.665242e-04</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.003331</td>\n",
              "      <td>1.829346e-06</td>\n",
              "      <td>0.000269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>4.496332e-08</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.001956</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.004427</td>\n",
              "      <td>4.630411e-07</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>6.856277e-04</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>2.452804e-05</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.000975</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000975</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>3.303657e-04</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>3.039603e-06</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>2.765309e-03</td>\n",
              "      <td>5.504805e-04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004926</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.996179e-07</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>1.649611e-05</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.007571</td>\n",
              "      <td>0.002244</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.006371</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.005658</td>\n",
              "      <td>3.844468e-06</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>1.170486e-06</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>8.700783e-06</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.008044</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>1.696990e-02</td>\n",
              "      <td>0.003613</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>3.231545e-05</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>1.369874e-02</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.006304</td>\n",
              "      <td>0.037603</td>\n",
              "      <td>0.019022</td>\n",
              "      <td>0.007613</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>5.002112e-06</td>\n",
              "      <td>0.074441</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>1.575477e-08</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>6.535483e-05</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.006271</td>\n",
              "      <td>0.004027</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.001442</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.001896</td>\n",
              "      <td>0.003481</td>\n",
              "      <td>0.001822</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.008843</td>\n",
              "      <td>4.087827e-07</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>7.275176e-08</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>2.104502e-07</td>\n",
              "      <td>5.226795e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.009043</td>\n",
              "      <td>1.589798e-06</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>2.437357e-06</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.011488</td>\n",
              "      <td>0.032768</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.035377</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>1.491121e-05</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>6.385940e-07</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>1.852057e-04</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.001341</td>\n",
              "      <td>2.382031e-05</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>3.369216e-07</td>\n",
              "      <td>0.001181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>6.301934e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.025946</td>\n",
              "      <td>0.037948</td>\n",
              "      <td>0.006070</td>\n",
              "      <td>0.002633</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>2.572523e-08</td>\n",
              "      <td>0.012696</td>\n",
              "      <td>0.043949</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>2.896414e-08</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>7.937054e-04</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.003788</td>\n",
              "      <td>0.003312</td>\n",
              "      <td>0.001811</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.003632</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.012567</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>2.850352e-06</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>6.343789e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>3.855445e-08</td>\n",
              "      <td>9.024463e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.007900</td>\n",
              "      <td>3.800623e-07</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>6.437999e-07</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.015059</td>\n",
              "      <td>0.012817</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.021389</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>1.022252e-04</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.004285</td>\n",
              "      <td>1.485096e-08</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>1.326317e-04</td>\n",
              "      <td>0.003754</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>2.673601e-04</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>2.085500e-05</td>\n",
              "      <td>0.000748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>1.884813e-03</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.025084</td>\n",
              "      <td>0.024609</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.004524</td>\n",
              "      <td>3.676797e-07</td>\n",
              "      <td>0.033525</td>\n",
              "      <td>0.019832</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>4.353855e-09</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>5.424684e-04</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.007971</td>\n",
              "      <td>0.002782</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.006992</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>2.798842e-06</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>9.780451e-08</td>\n",
              "      <td>0.001816</td>\n",
              "      <td>8.285455e-08</td>\n",
              "      <td>1.011582e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.001386</td>\n",
              "      <td>0.023272</td>\n",
              "      <td>2.648556e-06</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>2.571455e-06</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.005656</td>\n",
              "      <td>0.004529</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.024251</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>1.987313e-04</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.003675</td>\n",
              "      <td>6.409944e-08</td>\n",
              "      <td>0.000861</td>\n",
              "      <td>1.496265e-04</td>\n",
              "      <td>0.003109</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.001484</td>\n",
              "      <td>1.890469e-05</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>4.711344e-06</td>\n",
              "      <td>0.002471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>5.867521e-06</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.014848</td>\n",
              "      <td>0.015108</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.001977</td>\n",
              "      <td>2.225449e-08</td>\n",
              "      <td>0.003150</td>\n",
              "      <td>0.012267</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>2.230024e-06</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>1.036957e-03</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.003734</td>\n",
              "      <td>0.001585</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.025041</td>\n",
              "      <td>0.001948</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>5.506604e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>2.786502e-08</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>2.878008e-06</td>\n",
              "      <td>2.705486e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>3.177009e-06</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>2.855100e-06</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.017689</td>\n",
              "      <td>0.004848</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.018263</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>8.697155e-05</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>1.879097e-07</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>2.242880e-04</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.002774</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>2.751141e-03</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>6.501822e-06</td>\n",
              "      <td>0.000247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows Ã— 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.000079\n",
              "1     id_001897cda  ...       0.002245\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.001038\n",
              "4     id_0027f1083  ...       0.000269\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.000029\n",
              "3978  id_ff925dd0d  ...       0.001181\n",
              "3979  id_ffb710450  ...       0.000748\n",
              "3980  id_ffbb869f2  ...       0.002471\n",
              "3981  id_ffd5800b6  ...       0.000247\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG22QpImgnux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}