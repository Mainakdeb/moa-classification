{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0219077c4c9d4e879dd19f80654cfc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_72522f9da8d846b5a505e45f4c80b444",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4caa6cbf10504d45848775ea4e43f782",
              "IPY_MODEL_53e7482bfe724664a27bfbb953cbed48"
            ]
          }
        },
        "72522f9da8d846b5a505e45f4c80b444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4caa6cbf10504d45848775ea4e43f782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eae95991b9aa4283a296393e67fb37dc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1688,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1688,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_887b9d54f49a4a6c8586c0679dc86c9f"
          }
        },
        "53e7482bfe724664a27bfbb953cbed48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_20d24fb9513b48f0b5c88e5945b72230",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1688/1688 [02:32&lt;00:00, 11.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56f7794993564b93b913eadc3bbea8e2"
          }
        },
        "eae95991b9aa4283a296393e67fb37dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "887b9d54f49a4a6c8586c0679dc86c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20d24fb9513b48f0b5c88e5945b72230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56f7794993564b93b913eadc3bbea8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "043dd47a12514203ac7cb6ba2b730dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae227a8e7cc1402894dc3e6bef5cf3da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_111d6e46c9f84868bbd6b9a23a4a5152",
              "IPY_MODEL_663f229ec44c4f5ebf07e15481586ec8"
            ]
          }
        },
        "ae227a8e7cc1402894dc3e6bef5cf3da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "111d6e46c9f84868bbd6b9a23a4a5152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22a6cc0638bc4345bc345264b9dac38c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3982,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3982,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f38b7945aeef42d990317a03c5a9e481"
          }
        },
        "663f229ec44c4f5ebf07e15481586ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4799ddbbf32846d5a3cf7f7de745d4eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3982/3982 [01:01&lt;00:00, 64.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e37e8aeaf1814adb808c794fab6e6a5a"
          }
        },
        "22a6cc0638bc4345bc345264b9dac38c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f38b7945aeef42d990317a03c5a9e481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4799ddbbf32846d5a3cf7f7de745d4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e37e8aeaf1814adb808c794fab6e6a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOd8RhyuZzp2"
      },
      "source": [
        "!cp /content/drive/\"My Drive\"/kaggle/moa/lish-moa.zip /content/"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSua2cgfd2hn"
      },
      "source": [
        "!unzip lish-moa.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB9siuMCJ5pd"
      },
      "source": [
        "!pip install pip install iterative-stratification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUpXeUlWeXpI",
        "outputId": "cdc5a417-0183-4c03-ebf9-c1179aa8f0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "        \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter1d   ## smoother\n",
        "from tqdm.notebook import tqdm, tnrange\n",
        "import random\n",
        "import os\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 15, 7\n",
        "\n",
        "CGREEN  = '\\33[32m'\n",
        "CBLUE =  '\\033[34m'\n",
        "CRED = '\\033[1;31m'\n",
        "CEND  = '\\33[0m'\n",
        "\n",
        "def seed_everything(seed=1903):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0UcuWgBr0Hn",
        "outputId": "464cf595-a832-4854-ee24-63d172cdeaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "else:\n",
        "    device='cpu'\n",
        "    \n",
        "device\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZhAwZ3hr45u"
      },
      "source": [
        "train_features = pd.read_csv('train_features.csv')\n",
        "train_targets = pd.read_csv('train_targets_scored.csv')\n",
        "test_features = pd.read_csv('test_features.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWH98yYwp-w"
      },
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    return df\n",
        "\n",
        "train = preprocess(train_features)\n",
        "test = preprocess(test_features)\n",
        "\n",
        "del train_targets['sig_id']\n",
        "\n",
        "target = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n",
        "train = train.loc[train['cp_type']==0].reset_index(drop=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0AtYxrP0kpp"
      },
      "source": [
        "top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "       870, 871, 872, 873, 874]\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn34nXaa0qP1"
      },
      "source": [
        "all_columns = train.columns\n",
        "train=train[all_columns[top_features]]\n",
        "test = test[all_columns[top_features]]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBFXoVhT5X4",
        "outputId": "44678390-0698-4921-da62-0ca0e1691229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21948, 785), (3982, 785))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O52yoAOM14k4"
      },
      "source": [
        "train = train.values\n",
        "target = target.values\n",
        "test = test.values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N0QYVKDsTm-"
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train,targets, noise ):\n",
        "        \n",
        "        self.features  = train\n",
        "        self.targets = targets\n",
        "        self.noise = noise\n",
        "        \n",
        "    def sizes(self):\n",
        "        print(\"features size = \", self.features.shape[1])\n",
        "        print(\"targets size = \", self.targets.shape[1])\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx]).float()\n",
        "            \n",
        "        target = torch.tensor(self.targets[idx]).float()\n",
        "        \n",
        "        return feature, target\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCn4Ba6EsVyt"
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "        \n",
        "def show_lr(learning_rates):\n",
        "    plt.plot(learning_rates, label = \"learning rate\")\n",
        "    plt.ylabel(\"Learning rate\", fontsize = 15)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_step(x, y, model, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x.to(device))\n",
        "    y = y.float()\n",
        "    loss = criterion(pred,y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuz3mR5akCow",
        "outputId": "53fde747-e4f2-4f71-ffe3-f950264f908b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "[I 2020-09-26 23:26:27,234] Trial 77 finished with value: 0.015594600699841976 \n",
        "and parameters: {\n",
        "    'num_layer': 3, \n",
        "    'hidden_size': 2076, \n",
        "    'dropout': 0.5145663015913359, \n",
        "    'learning_rate': 0.0037416442804666648\n",
        "}. t\n",
        "Best is trial 77 with value: 0.015594600699841976.\n",
        "\"\"\"\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n[I 2020-09-26 23:26:27,234] Trial 77 finished with value: 0.015594600699841976 \\nand parameters: {\\n    'num_layer': 3, \\n    'hidden_size': 2076, \\n    'dropout': 0.5145663015913359, \\n    'learning_rate': 0.0037416442804666648\\n}. t\\nBest is trial 77 with value: 0.015594600699841976.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZqPFz-CsX4R"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(785)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(785, 2048))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, 206))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6q-PUbsezZ"
      },
      "source": [
        "\n",
        "def train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n",
        "    \n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    learning_rates = []    \n",
        "    best_loss = 1000000\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "            \n",
        "        if train == True:\n",
        "            model.train()\n",
        "            losses_temp = []\n",
        "            for batch in train_loader:\n",
        "                (x_batch, y_batch) = batch\n",
        "                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n",
        "                losses_temp.append(loss)\n",
        "            losses.append(torch.mean(torch.tensor(losses_temp)))\n",
        "            scheduler.step(1.)   ## lr decay caller \n",
        "            learning_rates.append(get_lr(optimizer))\n",
        "            \n",
        "\n",
        "        if validate == True:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_losses_temp = []\n",
        "                for x_val, y_val in val_loader:\n",
        "                    yhat =model(x_val.to(device))  # pred \n",
        "                    val_loss = criterion(yhat.to(device), y_val.to(device))\n",
        "                    val_losses_temp.append(val_loss.item())  ## metrics \n",
        "                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n",
        "\n",
        "        \n",
        "        if train == True:\n",
        "            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n",
        "\n",
        "            if val_losses[-1] <= best_loss:\n",
        "\n",
        "                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n",
        "                best_loss = val_losses[-1]\n",
        "                name = \"./model_\" + str(fold_number)+\".pth\"\n",
        "                print(\"saving model as: \", name)\n",
        "                torch.save(model.state_dict(), name)\n",
        "\n",
        "            else: \n",
        "                print(\"showing no improvements, best loss yet:\", best_loss)\n",
        "\n",
        "        if show_plots == True:\n",
        "\n",
        "            show_lr(learning_rates)\n",
        "            plt.plot(val_losses, label = \"val\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n",
        "            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.plot(losses, label = \"train\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()\n",
        "        \n",
        "    return losses, val_losses, name "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdN_UgGAskZK",
        "outputId": "b03fce73-a3c2-4d2c-b114-d958f1d0cb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "NFOLDS = 7\n",
        "num_epochs = 15 ## changes here \n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
        "\n",
        "fold_val_losses = list()\n",
        "filenames = []\n",
        "\n",
        "\n",
        "for k , (train_idx,valid_idx) in enumerate(mskf.split(train,target)):\n",
        "\n",
        "    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n",
        "\n",
        "    input_size = x_train.shape[1]\n",
        "    output_size = target.shape[1]\n",
        "    \n",
        "    train_dataset = TrainDataset(x_train, y_train, noise = False)\n",
        "    valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n",
        "    \n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size= 128, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True)\n",
        "    \n",
        "    model = Model()\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 0.004299882049752947, weight_decay=1e-5)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                     mode='min', \n",
        "                                                     factor=0.1, ## wooo hoo\n",
        "                                                     patience=7, ## was 3 for 158 \n",
        "                                                     eps=1e-4, \n",
        "                                                     verbose=True)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if k > 1:\n",
        "        print(CRED ,\"fold \", str(k+1), \"  :: mean loss on all folds: \", np.array([min(l) for l in fold_val_losses]).mean(), CEND)\n",
        "   \n",
        "\n",
        "    losses, val_losses, filename = train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+1)\n",
        "\n",
        "    fold_val_losses.append(val_losses)\n",
        "    filenames.append(filename)\n",
        "print(CBLUE, \"Training complete\", CEND)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.0208125077188015 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.0208125077188015  to  0.0194991622120142 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.0194991622120142  to  0.018166886642575264 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.018166886642575264  to  0.017644604668021202 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.017644604668021202  to  0.017548158764839172 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.017548158764839172  to  0.017323648557066917 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  7  out of  15      >\u001b[32m Val loss decreased from: 0.017323648557066917  to  0.017307434231042862 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017307434231042862  to  0.01722695678472519 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >\u001b[32m Val loss decreased from: 0.01722695678472519  to  0.017195764929056168 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.017195764929056168  to  0.016363898292183876 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.016363898292183876  to  0.016163330525159836 \u001b[0m   >saving model as:  ./model_1.pth\n",
            "epoch  12  out of  15      >showing no improvements, best loss yet: 0.016163330525159836\n",
            "epoch  13  out of  15      >showing no improvements, best loss yet: 0.016163330525159836\n",
            "epoch  14  out of  15      >showing no improvements, best loss yet: 0.016163330525159836\n",
            "epoch  15  out of  15      >showing no improvements, best loss yet: 0.016163330525159836\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.021170251071453094 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.021170251071453094  to  0.020375486463308334 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.020375486463308334  to  0.01818229630589485 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.01818229630589485  to  0.017831968143582344 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.017831968143582344  to  0.017515219748020172 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.017515219748020172  to  0.01721097156405449 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  7  out of  15      >showing no improvements, best loss yet: 0.01721097156405449\n",
            "epoch  8  out of  15      >showing no improvements, best loss yet: 0.01721097156405449\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >\u001b[32m Val loss decreased from: 0.01721097156405449  to  0.01692477986216545 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.01692477986216545  to  0.016223955899477005 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.016223955899477005  to  0.01615927554666996 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  12  out of  15      >showing no improvements, best loss yet: 0.01615927554666996\n",
            "epoch  13  out of  15      >\u001b[32m Val loss decreased from: 0.01615927554666996  to  0.016133420169353485 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  14  out of  15      >\u001b[32m Val loss decreased from: 0.016133420169353485  to  0.016028152778744698 \u001b[0m   >saving model as:  ./model_2.pth\n",
            "epoch  15  out of  15      >showing no improvements, best loss yet: 0.016028152778744698\n",
            "\u001b[1;31m fold  3   :: mean loss on all folds:  0.016095741651952267 \u001b[0m\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.021572986617684364 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.021572986617684364  to  0.019067084416747093 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.019067084416747093  to  0.01858958974480629 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.01858958974480629  to  0.0177513025701046 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.0177513025701046  to  0.01766568422317505 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.01766568422317505  to  0.017468873411417007 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  7  out of  15      >\u001b[32m Val loss decreased from: 0.017468873411417007  to  0.017214568331837654 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017214568331837654  to  0.01718212477862835 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >showing no improvements, best loss yet: 0.01718212477862835\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.01718212477862835  to  0.01655101403594017 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.01655101403594017  to  0.016492800787091255 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  12  out of  15      >\u001b[32m Val loss decreased from: 0.016492800787091255  to  0.016300080344080925 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  13  out of  15      >\u001b[32m Val loss decreased from: 0.016300080344080925  to  0.016187703236937523 \u001b[0m   >saving model as:  ./model_3.pth\n",
            "epoch  14  out of  15      >showing no improvements, best loss yet: 0.016187703236937523\n",
            "epoch  15  out of  15      >showing no improvements, best loss yet: 0.016187703236937523\n",
            "\u001b[1;31m fold  4   :: mean loss on all folds:  0.016126395513614018 \u001b[0m\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.021508486941456795 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.021508486941456795  to  0.019194455817341805 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.019194455817341805  to  0.018434252589941025 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.018434252589941025  to  0.017689986154437065 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.017689986154437065  to  0.017461471259593964 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.017461471259593964  to  0.01721249893307686 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  7  out of  15      >\u001b[32m Val loss decreased from: 0.01721249893307686  to  0.017203768715262413 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017203768715262413  to  0.017160724848508835 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >showing no improvements, best loss yet: 0.017160724848508835\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.017160724848508835  to  0.016499362885951996 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.016499362885951996  to  0.0162068959325552 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  12  out of  15      >\u001b[32m Val loss decreased from: 0.0162068959325552  to  0.016187269240617752 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  13  out of  15      >\u001b[32m Val loss decreased from: 0.016187269240617752  to  0.01615280844271183 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "epoch  14  out of  15      >showing no improvements, best loss yet: 0.01615280844271183\n",
            "epoch  15  out of  15      >\u001b[32m Val loss decreased from: 0.01615280844271183  to  0.016125019639730453 \u001b[0m   >saving model as:  ./model_4.pth\n",
            "\u001b[1;31m fold  5   :: mean loss on all folds:  0.016126051545143127 \u001b[0m\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.02116623893380165 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.02116623893380165  to  0.019298426806926727 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.019298426806926727  to  0.01888095960021019 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.01888095960021019  to  0.0180644653737545 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  5  out of  15      >showing no improvements, best loss yet: 0.0180644653737545\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.0180644653737545  to  0.01776917651295662 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  7  out of  15      >\u001b[32m Val loss decreased from: 0.01776917651295662  to  0.017488595098257065 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017488595098257065  to  0.017381679266691208 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >\u001b[32m Val loss decreased from: 0.017381679266691208  to  0.017335617914795876 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.017335617914795876  to  0.016845639795064926 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.016845639795064926  to  0.016766663640737534 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  12  out of  15      >\u001b[32m Val loss decreased from: 0.016766663640737534  to  0.016414325684309006 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "epoch  13  out of  15      >showing no improvements, best loss yet: 0.016414325684309006\n",
            "epoch  14  out of  15      >showing no improvements, best loss yet: 0.016414325684309006\n",
            "epoch  15  out of  15      >\u001b[32m Val loss decreased from: 0.016414325684309006  to  0.01630650833249092 \u001b[0m   >saving model as:  ./model_5.pth\n",
            "\u001b[1;31m fold  6   :: mean loss on all folds:  0.016162142902612687 \u001b[0m\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.020773500204086304 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.020773500204086304  to  0.019192315638065338 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.019192315638065338  to  0.01793276146054268 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.01793276146054268  to  0.01767568662762642 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.01767568662762642  to  0.017270073294639587 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  6  out of  15      >\u001b[32m Val loss decreased from: 0.017270073294639587  to  0.017160536721348763 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  7  out of  15      >showing no improvements, best loss yet: 0.017160536721348763\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017160536721348763  to  0.017051009461283684 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >showing no improvements, best loss yet: 0.017051009461283684\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.017051009461283684  to  0.016380613669753075 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  11  out of  15      >showing no improvements, best loss yet: 0.016380613669753075\n",
            "epoch  12  out of  15      >\u001b[32m Val loss decreased from: 0.016380613669753075  to  0.01621565781533718 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  13  out of  15      >\u001b[32m Val loss decreased from: 0.01621565781533718  to  0.016215341165661812 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  14  out of  15      >\u001b[32m Val loss decreased from: 0.016215341165661812  to  0.016101304441690445 \u001b[0m   >saving model as:  ./model_6.pth\n",
            "epoch  15  out of  15      >showing no improvements, best loss yet: 0.016101304441690445\n",
            "\u001b[1;31m fold  7   :: mean loss on all folds:  0.016152003159125645 \u001b[0m\n",
            "epoch  1  out of  15      >\u001b[32m Val loss decreased from: 1000000  to  0.020924866199493408 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  2  out of  15      >\u001b[32m Val loss decreased from: 0.020924866199493408  to  0.019235262647271156 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  3  out of  15      >\u001b[32m Val loss decreased from: 0.019235262647271156  to  0.018207930028438568 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  4  out of  15      >\u001b[32m Val loss decreased from: 0.018207930028438568  to  0.017948873341083527 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  5  out of  15      >\u001b[32m Val loss decreased from: 0.017948873341083527  to  0.017341364175081253 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  6  out of  15      >showing no improvements, best loss yet: 0.017341364175081253\n",
            "epoch  7  out of  15      >\u001b[32m Val loss decreased from: 0.017341364175081253  to  0.017288681119680405 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  8  out of  15      >\u001b[32m Val loss decreased from: 0.017288681119680405  to  0.01717165857553482 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "Epoch     9: reducing learning rate of group 0 to 4.2999e-04.\n",
            "epoch  9  out of  15      >\u001b[32m Val loss decreased from: 0.01717165857553482  to  0.01712152548134327 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  10  out of  15      >\u001b[32m Val loss decreased from: 0.01712152548134327  to  0.01651715114712715 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  11  out of  15      >\u001b[32m Val loss decreased from: 0.01651715114712715  to  0.016376258805394173 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  12  out of  15      >\u001b[32m Val loss decreased from: 0.016376258805394173  to  0.016265306621789932 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "epoch  13  out of  15      >showing no improvements, best loss yet: 0.016265306621789932\n",
            "epoch  14  out of  15      >showing no improvements, best loss yet: 0.016265306621789932\n",
            "epoch  15  out of  15      >\u001b[32m Val loss decreased from: 0.016265306621789932  to  0.01603636145591736 \u001b[0m   >saving model as:  ./model_7.pth\n",
            "\u001b[34m Training complete \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNT4tBQ_tPBA"
      },
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self, model_list, device):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        \n",
        "        print(\"loading models...\")\n",
        "        self.model_filenames = model_list\n",
        "        self.model_list = [Model() for m in range(len(self.model_filenames))]\n",
        "\n",
        "        for i in range(len(self.model_filenames)):\n",
        "            self.model_list[i].load_state_dict(torch.load(self.model_filenames[i]))\n",
        "            self.model_list[i].to(device)\n",
        "\n",
        "        self.fc1 = nn.Linear(206*len(self.model_list), 2024)\n",
        "        self.fc2 = nn.Linear(2024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 206)\n",
        "        print(\"ensemble initialised with \" , len(self.model_list), \" models\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x_list = [self.model_list[i](x) for i in range(len(self.model_list))]\n",
        "\n",
        "        joined =  torch.cat(tuple(x_list), dim=1)\n",
        "\n",
        "        x = F.relu(self.fc1(joined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ORdaIAbp3f6",
        "outputId": "43a50676-bf27-4c0c-cb81-a9ca1e39f4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "NFOLDS = 10\n",
        "num_epochs = 75 ## changes here \n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
        "\n",
        "ensemble_fold_val_losses = list()\n",
        "\n",
        "\n",
        "for k , (train_idx,valid_idx) in enumerate(mskf.split(train,target)):\n",
        "\n",
        "    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n",
        "\n",
        "    input_size = x_train.shape[1]\n",
        "    output_size = target.shape[1]\n",
        "    train_dataset = TrainDataset(x_train, y_train, noise = False)\n",
        "    valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size= 128, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True)\n",
        "    \n",
        "    ensemble = MyEnsemble(filenames, device)\n",
        "    ensemble.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(ensemble.parameters(), lr = 1e-3, weight_decay=1e-5)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                     mode='min', \n",
        "                                                     factor=0.1, ## wooo hoo\n",
        "                                                     patience=5, ## was 3 for 158 \n",
        "                                                     eps=1e-4, \n",
        "                                                     verbose=True)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if k > 1:\n",
        "        print(CRED ,\"fold \", str(k+1), \"  :: mean loss on all folds: \", np.array([min(l) for l in ensemble_fold_val_losses]).mean(), CEND)\n",
        "   \n",
        "\n",
        "    ensemble_losses, ensemble_val_losses, name = train_one_fold(ensemble, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+100)\n",
        "    ensemble_fold_val_losses.append(ensemble_val_losses)\n",
        "print(CBLUE, \"Training complete\", CEND)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.020658131688833237 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.020658131688833237  to  0.019085971638560295 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.019085971638560295  to  0.01811828464269638 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.01811828464269638  to  0.017448388040065765 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.017448388040065765  to  0.017057614400982857 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017057614400982857  to  0.016650453209877014 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.016650453209877014  to  0.0165085569024086 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.0165085569024086  to  0.016102198511362076 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.016102198511362076  to  0.016015401110053062 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.016015401110053062  to  0.01590374857187271 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.01590374857187271  to  0.015800589695572853 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.015800589695572853\n",
            "epoch  13  out of  75      >showing no improvements, best loss yet: 0.015800589695572853\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.015800589695572853\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015800589695572853\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.015800589695572853  to  0.015784021466970444 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.015784021466970444  to  0.01576809398829937 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.01576809398829937\n",
            "epoch  19  out of  75      >\u001b[32m Val loss decreased from: 0.01576809398829937  to  0.015712445601820946 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  20  out of  75      >\u001b[32m Val loss decreased from: 0.015712445601820946  to  0.01563824526965618 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.01563824526965618\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.01563824526965618\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.01563824526965618\n",
            "epoch  24  out of  75      >\u001b[32m Val loss decreased from: 0.01563824526965618  to  0.015581618063151836 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015581618063151836  to  0.015521521680057049 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.015521521680057049\n",
            "epoch  32  out of  75      >\u001b[32m Val loss decreased from: 0.015521521680057049  to  0.01548551395535469 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  35  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.01548551395535469\n",
            "epoch  39  out of  75      >\u001b[32m Val loss decreased from: 0.01548551395535469  to  0.015371069312095642 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015371069312095642\n",
            "epoch  41  out of  75      >\u001b[32m Val loss decreased from: 0.015371069312095642  to  0.01534361019730568 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  42  out of  75      >\u001b[32m Val loss decreased from: 0.01534361019730568  to  0.015284448862075806 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  43  out of  75      >showing no improvements, best loss yet: 0.015284448862075806\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015284448862075806\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015284448862075806\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015284448862075806\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015284448862075806\n",
            "epoch  48  out of  75      >\u001b[32m Val loss decreased from: 0.015284448862075806  to  0.015238072723150253 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015238072723150253\n",
            "epoch  50  out of  75      >\u001b[32m Val loss decreased from: 0.015238072723150253  to  0.01517533790320158 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.01517533790320158\n",
            "epoch  57  out of  75      >\u001b[32m Val loss decreased from: 0.01517533790320158  to  0.015076786279678345 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015076786279678345\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015076786279678345\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015076786279678345\n",
            "epoch  61  out of  75      >showing no improvements, best loss yet: 0.015076786279678345\n",
            "epoch  62  out of  75      >\u001b[32m Val loss decreased from: 0.015076786279678345  to  0.015000002458691597 \u001b[0m   >saving model as:  ./model_100.pth\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.015000002458691597\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.01990228146314621 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.01990228146314621  to  0.01878999173641205 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.01878999173641205  to  0.017870496958494186 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.017870496958494186  to  0.017173266038298607 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  5  out of  75      >showing no improvements, best loss yet: 0.017173266038298607\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017173266038298607  to  0.01696273684501648 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.01696273684501648  to  0.016429314389824867 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.016429314389824867  to  0.01592840813100338 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  9  out of  75      >showing no improvements, best loss yet: 0.01592840813100338\n",
            "epoch  10  out of  75      >showing no improvements, best loss yet: 0.01592840813100338\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.01592840813100338  to  0.01588692143559456 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  12  out of  75      >\u001b[32m Val loss decreased from: 0.01588692143559456  to  0.015884755179286003 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.015884755179286003  to  0.015804298222064972 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.015804298222064972\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015804298222064972\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.015804298222064972  to  0.015801841393113136 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  17  out of  75      >showing no improvements, best loss yet: 0.015801841393113136\n",
            "epoch  18  out of  75      >\u001b[32m Val loss decreased from: 0.015801841393113136  to  0.015619440004229546 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015619440004229546\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015619440004229546  to  0.015587682835757732 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015587682835757732\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015587682835757732\n",
            "epoch  28  out of  75      >\u001b[32m Val loss decreased from: 0.015587682835757732  to  0.015562840737402439 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015562840737402439\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015562840737402439\n",
            "epoch  31  out of  75      >\u001b[32m Val loss decreased from: 0.015562840737402439  to  0.015532187186181545 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  32  out of  75      >\u001b[32m Val loss decreased from: 0.015532187186181545  to  0.015516380779445171 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015516380779445171\n",
            "epoch  34  out of  75      >\u001b[32m Val loss decreased from: 0.015516380779445171  to  0.015410453081130981 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  35  out of  75      >showing no improvements, best loss yet: 0.015410453081130981\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015410453081130981\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.015410453081130981\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.015410453081130981\n",
            "epoch  39  out of  75      >\u001b[32m Val loss decreased from: 0.015410453081130981  to  0.015409741550683975 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015409741550683975\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015409741550683975\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015409741550683975\n",
            "epoch  43  out of  75      >\u001b[32m Val loss decreased from: 0.015409741550683975  to  0.015349414199590683 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  44  out of  75      >\u001b[32m Val loss decreased from: 0.015349414199590683  to  0.01521181408315897 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.01521181408315897\n",
            "epoch  46  out of  75      >\u001b[32m Val loss decreased from: 0.01521181408315897  to  0.015200267545878887 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015200267545878887\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015200267545878887\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015200267545878887\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015200267545878887\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.015200267545878887\n",
            "epoch  52  out of  75      >\u001b[32m Val loss decreased from: 0.015200267545878887  to  0.015184732154011726 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.015184732154011726\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.015184732154011726\n",
            "epoch  55  out of  75      >\u001b[32m Val loss decreased from: 0.015184732154011726  to  0.015098605304956436 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.015098605304956436\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015098605304956436\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015098605304956436\n",
            "epoch  59  out of  75      >\u001b[32m Val loss decreased from: 0.015098605304956436  to  0.015072326175868511 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015072326175868511\n",
            "epoch  61  out of  75      >\u001b[32m Val loss decreased from: 0.015072326175868511  to  0.01503784116357565 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.01503784116357565\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.01503784116357565\n",
            "epoch  64  out of  75      >\u001b[32m Val loss decreased from: 0.01503784116357565  to  0.0150319654494524 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.0150319654494524\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.0150319654494524\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.0150319654494524\n",
            "epoch  68  out of  75      >\u001b[32m Val loss decreased from: 0.0150319654494524  to  0.01493642758578062 \u001b[0m   >saving model as:  ./model_101.pth\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.01493642758578062\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  3   :: mean loss on all folds:  0.014968215022236109 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.020299848169088364 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.020299848169088364  to  0.018900824710726738 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.018900824710726738  to  0.018615564331412315 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.018615564331412315  to  0.01778102107346058 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.01778102107346058  to  0.017711235210299492 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017711235210299492  to  0.01690422184765339 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.01690422184765339  to  0.016653360798954964 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.016653360798954964  to  0.016182608902454376 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.016182608902454376  to  0.016062559559941292 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  10  out of  75      >showing no improvements, best loss yet: 0.016062559559941292\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.016062559559941292  to  0.01600254327058792 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  12  out of  75      >\u001b[32m Val loss decreased from: 0.01600254327058792  to  0.015841856598854065 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  13  out of  75      >showing no improvements, best loss yet: 0.015841856598854065\n",
            "epoch  14  out of  75      >\u001b[32m Val loss decreased from: 0.015841856598854065  to  0.015829484909772873 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015829484909772873\n",
            "epoch  16  out of  75      >showing no improvements, best loss yet: 0.015829484909772873\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.015829484909772873  to  0.015827380120754242 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  18  out of  75      >\u001b[32m Val loss decreased from: 0.015827380120754242  to  0.015800468623638153 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.015800468623638153\n",
            "epoch  20  out of  75      >\u001b[32m Val loss decreased from: 0.015800468623638153  to  0.015592826530337334 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.015592826530337334\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.015592826530337334\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.015592826530337334\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015592826530337334\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015592826530337334  to  0.015507032163441181 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015507032163441181\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015507032163441181\n",
            "epoch  28  out of  75      >\u001b[32m Val loss decreased from: 0.015507032163441181  to  0.015491641126573086 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.015491641126573086\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.015491641126573086  to  0.015438202768564224 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015438202768564224\n",
            "epoch  37  out of  75      >\u001b[32m Val loss decreased from: 0.015438202768564224  to  0.01543502975255251 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.01543502975255251\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.01543502975255251\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.01543502975255251\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.01543502975255251\n",
            "epoch  42  out of  75      >\u001b[32m Val loss decreased from: 0.01543502975255251  to  0.015312448143959045 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  43  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.015312448143959045\n",
            "epoch  53  out of  75      >\u001b[32m Val loss decreased from: 0.015312448143959045  to  0.015283267013728619 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015283267013728619\n",
            "epoch  60  out of  75      >\u001b[32m Val loss decreased from: 0.015283267013728619  to  0.01523679681122303 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  61  out of  75      >\u001b[32m Val loss decreased from: 0.01523679681122303  to  0.01519257202744484 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.01519257202744484\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.01519257202744484\n",
            "epoch  64  out of  75      >\u001b[32m Val loss decreased from: 0.01519257202744484  to  0.015060145407915115 \u001b[0m   >saving model as:  ./model_102.pth\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.015060145407915115\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  4   :: mean loss on all folds:  0.01499885848412911 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.019944259896874428 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.019944259896874428  to  0.01898021250963211 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.01898021250963211  to  0.018786847591400146 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.018786847591400146  to  0.0175387654453516 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.0175387654453516  to  0.01735007017850876 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.01735007017850876  to  0.016868295148015022 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.016868295148015022  to  0.01673208177089691 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.01673208177089691  to  0.01596745103597641 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  9  out of  75      >showing no improvements, best loss yet: 0.01596745103597641\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.01596745103597641  to  0.01592095196247101 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  11  out of  75      >showing no improvements, best loss yet: 0.01592095196247101\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.01592095196247101\n",
            "epoch  13  out of  75      >showing no improvements, best loss yet: 0.01592095196247101\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.01592095196247101\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.01592095196247101\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.01592095196247101  to  0.015853580087423325 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.015853580087423325  to  0.015841759741306305 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.015841759741306305\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.015841759741306305\n",
            "epoch  20  out of  75      >\u001b[32m Val loss decreased from: 0.015841759741306305  to  0.01577755995094776 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  21  out of  75      >\u001b[32m Val loss decreased from: 0.01577755995094776  to  0.015732862055301666 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.015732862055301666\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.015732862055301666\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015732862055301666\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015732862055301666  to  0.01571355015039444 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  26  out of  75      >\u001b[32m Val loss decreased from: 0.01571355015039444  to  0.015642262995243073 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015642262995243073\n",
            "epoch  28  out of  75      >\u001b[32m Val loss decreased from: 0.015642262995243073  to  0.015595925971865654 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.015595925971865654\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.015595925971865654  to  0.015534160658717155 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  36  out of  75      >\u001b[32m Val loss decreased from: 0.015534160658717155  to  0.015408775769174099 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.015408775769174099\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.015408775769174099\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.015408775769174099\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015408775769174099\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015408775769174099\n",
            "epoch  42  out of  75      >\u001b[32m Val loss decreased from: 0.015408775769174099  to  0.015381667762994766 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  43  out of  75      >showing no improvements, best loss yet: 0.015381667762994766\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015381667762994766\n",
            "epoch  45  out of  75      >\u001b[32m Val loss decreased from: 0.015381667762994766  to  0.015358079224824905 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  46  out of  75      >\u001b[32m Val loss decreased from: 0.015358079224824905  to  0.01534070074558258 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.01534070074558258\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.01534070074558258\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.01534070074558258\n",
            "epoch  50  out of  75      >\u001b[32m Val loss decreased from: 0.01534070074558258  to  0.015294207260012627 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  51  out of  75      >\u001b[32m Val loss decreased from: 0.015294207260012627  to  0.015238652937114239 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.015238652937114239\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.015238652937114239\n",
            "epoch  54  out of  75      >\u001b[32m Val loss decreased from: 0.015238652937114239  to  0.015132574364542961 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015132574364542961\n",
            "epoch  61  out of  75      >\u001b[32m Val loss decreased from: 0.015132574364542961  to  0.015039963647723198 \u001b[0m   >saving model as:  ./model_103.pth\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.015039963647723198\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  5   :: mean loss on all folds:  0.015009134775027633 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.019873838871717453 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.019873838871717453  to  0.01904674433171749 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.01904674433171749  to  0.01781764067709446 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.01781764067709446  to  0.017519431188702583 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.017519431188702583  to  0.01718323305249214 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.01718323305249214  to  0.01709468849003315 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >showing no improvements, best loss yet: 0.01709468849003315\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.01709468849003315  to  0.016095571219921112 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  9  out of  75      >showing no improvements, best loss yet: 0.016095571219921112\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.016095571219921112  to  0.015938717871904373 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  11  out of  75      >showing no improvements, best loss yet: 0.015938717871904373\n",
            "epoch  12  out of  75      >\u001b[32m Val loss decreased from: 0.015938717871904373  to  0.015913350507616997 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.015913350507616997  to  0.015830151736736298 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.015830151736736298\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015830151736736298\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.015830151736736298  to  0.01576639711856842 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.01576639711856842  to  0.015740977600216866 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015740977600216866\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015740977600216866  to  0.015627412125468254 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015627412125468254\n",
            "epoch  27  out of  75      >\u001b[32m Val loss decreased from: 0.015627412125468254  to  0.01559166144579649 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.01559166144579649\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.01559166144579649\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.01559166144579649\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.01559166144579649\n",
            "epoch  32  out of  75      >\u001b[32m Val loss decreased from: 0.01559166144579649  to  0.015589836984872818 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015589836984872818\n",
            "epoch  34  out of  75      >\u001b[32m Val loss decreased from: 0.015589836984872818  to  0.015550119802355766 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  35  out of  75      >showing no improvements, best loss yet: 0.015550119802355766\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015550119802355766\n",
            "epoch  37  out of  75      >\u001b[32m Val loss decreased from: 0.015550119802355766  to  0.015464887022972107 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.015464887022972107\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.015464887022972107\n",
            "epoch  40  out of  75      >\u001b[32m Val loss decreased from: 0.015464887022972107  to  0.015431510284543037 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  41  out of  75      >\u001b[32m Val loss decreased from: 0.015431510284543037  to  0.015397931449115276 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015397931449115276\n",
            "epoch  43  out of  75      >showing no improvements, best loss yet: 0.015397931449115276\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015397931449115276\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015397931449115276\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015397931449115276\n",
            "epoch  47  out of  75      >\u001b[32m Val loss decreased from: 0.015397931449115276  to  0.015368751250207424 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015368751250207424\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015368751250207424\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015368751250207424\n",
            "epoch  51  out of  75      >\u001b[32m Val loss decreased from: 0.015368751250207424  to  0.015325559303164482 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  52  out of  75      >\u001b[32m Val loss decreased from: 0.015325559303164482  to  0.015290873125195503 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.015290873125195503\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.015290873125195503\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015290873125195503\n",
            "epoch  56  out of  75      >\u001b[32m Val loss decreased from: 0.015290873125195503  to  0.015166365541517735 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  61  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015166365541517735\n",
            "epoch  67  out of  75      >\u001b[32m Val loss decreased from: 0.015166365541517735  to  0.015108429826796055 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015108429826796055\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.015108429826796055\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015108429826796055\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.015108429826796055\n",
            "epoch  72  out of  75      >\u001b[32m Val loss decreased from: 0.015108429826796055  to  0.015078834258019924 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.015078834258019924\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015078834258019924\n",
            "epoch  75  out of  75      >\u001b[32m Val loss decreased from: 0.015078834258019924  to  0.015040725469589233 \u001b[0m   >saving model as:  ./model_104.pth\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  6   :: mean loss on all folds:  0.015015452913939954 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.02030809409916401 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.02030809409916401  to  0.019032206386327744 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.019032206386327744  to  0.01794721931219101 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.01794721931219101  to  0.017187457531690598 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  5  out of  75      >showing no improvements, best loss yet: 0.017187457531690598\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017187457531690598  to  0.016746019944548607 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >showing no improvements, best loss yet: 0.016746019944548607\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.016746019944548607  to  0.015894856303930283 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.015894856303930283  to  0.01585186831653118 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  10  out of  75      >showing no improvements, best loss yet: 0.01585186831653118\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.01585186831653118  to  0.015760447829961777 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.015760447829961777\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.015760447829961777  to  0.01575019769370556 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.01575019769370556\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.01575019769370556\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.01575019769370556  to  0.015644453465938568 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  17  out of  75      >showing no improvements, best loss yet: 0.015644453465938568\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.015644453465938568\n",
            "epoch  19  out of  75      >\u001b[32m Val loss decreased from: 0.015644453465938568  to  0.015631673857569695 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.015631673857569695\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.015631673857569695\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.015631673857569695\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.015631673857569695\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015631673857569695\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015631673857569695  to  0.015614528208971024 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  26  out of  75      >\u001b[32m Val loss decreased from: 0.015614528208971024  to  0.015540730208158493 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  27  out of  75      >\u001b[32m Val loss decreased from: 0.015540730208158493  to  0.015534939244389534 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  28  out of  75      >\u001b[32m Val loss decreased from: 0.015534939244389534  to  0.015409767627716064 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015409767627716064\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015409767627716064\n",
            "epoch  31  out of  75      >\u001b[32m Val loss decreased from: 0.015409767627716064  to  0.01539844460785389 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.01539844460785389\n",
            "epoch  33  out of  75      >\u001b[32m Val loss decreased from: 0.01539844460785389  to  0.0153575474396348 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.0153575474396348\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.0153575474396348  to  0.015312520787119865 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015312520787119865\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.015312520787119865\n",
            "epoch  38  out of  75      >showing no improvements, best loss yet: 0.015312520787119865\n",
            "epoch  39  out of  75      >\u001b[32m Val loss decreased from: 0.015312520787119865  to  0.015120484866201878 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015120484866201878\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015120484866201878\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015120484866201878\n",
            "epoch  43  out of  75      >\u001b[32m Val loss decreased from: 0.015120484866201878  to  0.015095815062522888 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015095815062522888\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015095815062522888\n",
            "epoch  46  out of  75      >\u001b[32m Val loss decreased from: 0.015095815062522888  to  0.015026488341391087 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015026488341391087\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015026488341391087\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015026488341391087\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015026488341391087\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.015026488341391087\n",
            "epoch  52  out of  75      >\u001b[32m Val loss decreased from: 0.015026488341391087  to  0.014905237592756748 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.014905237592756748\n",
            "epoch  61  out of  75      >\u001b[32m Val loss decreased from: 0.014905237592756748  to  0.01490283478051424 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.01490283478051424\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.01490283478051424\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.01490283478051424\n",
            "epoch  65  out of  75      >\u001b[32m Val loss decreased from: 0.01490283478051424  to  0.014896403066813946 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.014896403066813946\n",
            "epoch  73  out of  75      >\u001b[32m Val loss decreased from: 0.014896403066813946  to  0.014888714998960495 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  74  out of  75      >\u001b[32m Val loss decreased from: 0.014888714998960495  to  0.014871963299810886 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "epoch  75  out of  75      >\u001b[32m Val loss decreased from: 0.014871963299810886  to  0.014753947965800762 \u001b[0m   >saving model as:  ./model_105.pth\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  7   :: mean loss on all folds:  0.014971868755916754 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.02022281475365162 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.02022281475365162  to  0.019134564325213432 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.019134564325213432  to  0.018537161871790886 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.018537161871790886  to  0.0178823284804821 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.0178823284804821  to  0.017619796097278595 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017619796097278595  to  0.017062446102499962 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.017062446102499962  to  0.016733022406697273 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.016733022406697273  to  0.015968473628163338 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  9  out of  75      >showing no improvements, best loss yet: 0.015968473628163338\n",
            "epoch  10  out of  75      >showing no improvements, best loss yet: 0.015968473628163338\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.015968473628163338  to  0.01593618467450142 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.01593618467450142\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.01593618467450142  to  0.015871955081820488 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  14  out of  75      >\u001b[32m Val loss decreased from: 0.015871955081820488  to  0.015775633975863457 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015775633975863457\n",
            "epoch  16  out of  75      >showing no improvements, best loss yet: 0.015775633975863457\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.015775633975863457  to  0.015732163563370705 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.015732163563370705\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.015732163563370705\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.015732163563370705\n",
            "epoch  21  out of  75      >\u001b[32m Val loss decreased from: 0.015732163563370705  to  0.015691155567765236 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  22  out of  75      >\u001b[32m Val loss decreased from: 0.015691155567765236  to  0.01561796199530363 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.01561796199530363\n",
            "epoch  24  out of  75      >\u001b[32m Val loss decreased from: 0.01561796199530363  to  0.015609274618327618 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.015609274618327618  to  0.015555709600448608 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015555709600448608\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015555709600448608\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.015555709600448608\n",
            "epoch  29  out of  75      >\u001b[32m Val loss decreased from: 0.015555709600448608  to  0.015469908714294434 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  30  out of  75      >\u001b[32m Val loss decreased from: 0.015469908714294434  to  0.015363047830760479 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  35  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015363047830760479\n",
            "epoch  37  out of  75      >\u001b[32m Val loss decreased from: 0.015363047830760479  to  0.015360615216195583 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  38  out of  75      >\u001b[32m Val loss decreased from: 0.015360615216195583  to  0.015292022377252579 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.015292022377252579\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015292022377252579\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015292022377252579\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015292022377252579\n",
            "epoch  43  out of  75      >\u001b[32m Val loss decreased from: 0.015292022377252579  to  0.015272678807377815 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  44  out of  75      >\u001b[32m Val loss decreased from: 0.015272678807377815  to  0.015119489282369614 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015119489282369614\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015119489282369614\n",
            "epoch  47  out of  75      >\u001b[32m Val loss decreased from: 0.015119489282369614  to  0.015086927451193333 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015086927451193333\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015086927451193333\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015086927451193333\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.015086927451193333\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.015086927451193333\n",
            "epoch  53  out of  75      >\u001b[32m Val loss decreased from: 0.015086927451193333  to  0.015083612874150276 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.015083612874150276\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015083612874150276\n",
            "epoch  56  out of  75      >\u001b[32m Val loss decreased from: 0.015083612874150276  to  0.014977789483964443 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  61  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.014977789483964443\n",
            "epoch  66  out of  75      >\u001b[32m Val loss decreased from: 0.014977789483964443  to  0.014915178529918194 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.014915178529918194\n",
            "epoch  68  out of  75      >\u001b[32m Val loss decreased from: 0.014915178529918194  to  0.01487727090716362 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.01487727090716362\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.01487727090716362\n",
            "epoch  71  out of  75      >\u001b[32m Val loss decreased from: 0.01487727090716362  to  0.014857466332614422 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.014857466332614422\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.014857466332614422\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.014857466332614422\n",
            "epoch  75  out of  75      >\u001b[32m Val loss decreased from: 0.014857466332614422  to  0.014801676385104656 \u001b[0m   >saving model as:  ./model_106.pth\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  8   :: mean loss on all folds:  0.014947555560086454 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.020743265748023987 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.020743265748023987  to  0.019369622692465782 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.019369622692465782  to  0.018466170877218246 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.018466170877218246  to  0.017549345269799232 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.017549345269799232  to  0.017298908904194832 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017298908904194832  to  0.01695861481130123 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >showing no improvements, best loss yet: 0.01695861481130123\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.01695861481130123  to  0.01614220067858696 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.01614220067858696  to  0.016097208485007286 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.016097208485007286  to  0.0160613264888525 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  11  out of  75      >\u001b[32m Val loss decreased from: 0.0160613264888525  to  0.016005802899599075 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.016005802899599075\n",
            "epoch  13  out of  75      >showing no improvements, best loss yet: 0.016005802899599075\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.016005802899599075\n",
            "epoch  15  out of  75      >\u001b[32m Val loss decreased from: 0.016005802899599075  to  0.015909811481833458 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  16  out of  75      >showing no improvements, best loss yet: 0.015909811481833458\n",
            "epoch  17  out of  75      >showing no improvements, best loss yet: 0.015909811481833458\n",
            "epoch  18  out of  75      >\u001b[32m Val loss decreased from: 0.015909811481833458  to  0.015874633565545082 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  19  out of  75      >\u001b[32m Val loss decreased from: 0.015874633565545082  to  0.01578044891357422 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.01578044891357422\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.01578044891357422\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.01578044891357422\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.01578044891357422\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.01578044891357422\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.01578044891357422  to  0.015778427943587303 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  26  out of  75      >\u001b[32m Val loss decreased from: 0.015778427943587303  to  0.01568564958870411 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  27  out of  75      >\u001b[32m Val loss decreased from: 0.01568564958870411  to  0.015569438226521015 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.015569438226521015\n",
            "epoch  29  out of  75      >showing no improvements, best loss yet: 0.015569438226521015\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015569438226521015\n",
            "epoch  31  out of  75      >\u001b[32m Val loss decreased from: 0.015569438226521015  to  0.01551818661391735 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.01551818661391735\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.01551818661391735\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.01551818661391735\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.01551818661391735  to  0.01549084484577179 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.01549084484577179\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.01549084484577179\n",
            "epoch  38  out of  75      >\u001b[32m Val loss decreased from: 0.01549084484577179  to  0.015390904620289803 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.015390904620289803\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015390904620289803\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015390904620289803\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015390904620289803\n",
            "epoch  43  out of  75      >\u001b[32m Val loss decreased from: 0.015390904620289803  to  0.01534570287913084 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.01534570287913084\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.01534570287913084\n",
            "epoch  46  out of  75      >\u001b[32m Val loss decreased from: 0.01534570287913084  to  0.015273087657988071 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015273087657988071\n",
            "epoch  48  out of  75      >\u001b[32m Val loss decreased from: 0.015273087657988071  to  0.01521170511841774 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  51  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.01521170511841774\n",
            "epoch  57  out of  75      >\u001b[32m Val loss decreased from: 0.01521170511841774  to  0.015185939148068428 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  58  out of  75      >\u001b[32m Val loss decreased from: 0.015185939148068428  to  0.015173550695180893 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  59  out of  75      >\u001b[32m Val loss decreased from: 0.015173550695180893  to  0.015145589597523212 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015145589597523212\n",
            "epoch  61  out of  75      >\u001b[32m Val loss decreased from: 0.015145589597523212  to  0.015137208625674248 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.015137208625674248\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015137208625674248\n",
            "epoch  64  out of  75      >\u001b[32m Val loss decreased from: 0.015137208625674248  to  0.015076892450451851 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015076892450451851\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015076892450451851\n",
            "epoch  67  out of  75      >\u001b[32m Val loss decreased from: 0.015076892450451851  to  0.015043940395116806 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015043940395116806\n",
            "epoch  69  out of  75      >showing no improvements, best loss yet: 0.015043940395116806\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015043940395116806\n",
            "epoch  71  out of  75      >\u001b[32m Val loss decreased from: 0.015043940395116806  to  0.014997083693742752 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  72  out of  75      >\u001b[32m Val loss decreased from: 0.014997083693742752  to  0.014886261895298958 \u001b[0m   >saving model as:  ./model_107.pth\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.014886261895298958\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.014886261895298958\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.014886261895298958\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  9   :: mean loss on all folds:  0.014939893851988018 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.02007472701370716 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.02007472701370716  to  0.018703332170844078 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.018703332170844078  to  0.01791263185441494 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.01791263185441494  to  0.017544247210025787 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.017544247210025787  to  0.01719890534877777 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.01719890534877777  to  0.016802264377474785 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >\u001b[32m Val loss decreased from: 0.016802264377474785  to  0.016672303900122643 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.016672303900122643  to  0.016104042530059814 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.016104042530059814  to  0.015981104224920273 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.015981104224920273  to  0.01592182368040085 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  11  out of  75      >showing no improvements, best loss yet: 0.01592182368040085\n",
            "epoch  12  out of  75      >\u001b[32m Val loss decreased from: 0.01592182368040085  to  0.015894895419478416 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  13  out of  75      >showing no improvements, best loss yet: 0.015894895419478416\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.015894895419478416\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015894895419478416\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.015894895419478416  to  0.015854492783546448 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  17  out of  75      >showing no improvements, best loss yet: 0.015854492783546448\n",
            "epoch  18  out of  75      >\u001b[32m Val loss decreased from: 0.015854492783546448  to  0.01580534502863884 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.01580534502863884\n",
            "epoch  20  out of  75      >\u001b[32m Val loss decreased from: 0.01580534502863884  to  0.015795746818184853 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.015795746818184853\n",
            "epoch  22  out of  75      >\u001b[32m Val loss decreased from: 0.015795746818184853  to  0.015778658911585808 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  23  out of  75      >\u001b[32m Val loss decreased from: 0.015778658911585808  to  0.015707995742559433 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.015707995742559433\n",
            "epoch  25  out of  75      >showing no improvements, best loss yet: 0.015707995742559433\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.015707995742559433\n",
            "epoch  27  out of  75      >showing no improvements, best loss yet: 0.015707995742559433\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.015707995742559433\n",
            "epoch  29  out of  75      >\u001b[32m Val loss decreased from: 0.015707995742559433  to  0.01558053120970726 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.01558053120970726\n",
            "epoch  31  out of  75      >\u001b[32m Val loss decreased from: 0.01558053120970726  to  0.015578893944621086 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.015578893944621086\n",
            "epoch  33  out of  75      >showing no improvements, best loss yet: 0.015578893944621086\n",
            "epoch  34  out of  75      >showing no improvements, best loss yet: 0.015578893944621086\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.015578893944621086  to  0.015552150085568428 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.015552150085568428\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.015552150085568428\n",
            "epoch  38  out of  75      >\u001b[32m Val loss decreased from: 0.015552150085568428  to  0.015432070009410381 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  39  out of  75      >showing no improvements, best loss yet: 0.015432070009410381\n",
            "epoch  40  out of  75      >showing no improvements, best loss yet: 0.015432070009410381\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.015432070009410381\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.015432070009410381\n",
            "epoch  43  out of  75      >\u001b[32m Val loss decreased from: 0.015432070009410381  to  0.015357641503214836 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  44  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  48  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015357641503214836\n",
            "epoch  51  out of  75      >\u001b[32m Val loss decreased from: 0.015357641503214836  to  0.015244189649820328 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  54  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  61  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.015244189649820328\n",
            "epoch  65  out of  75      >\u001b[32m Val loss decreased from: 0.015244189649820328  to  0.015160675160586834 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  66  out of  75      >showing no improvements, best loss yet: 0.015160675160586834\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.015160675160586834\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015160675160586834\n",
            "epoch  69  out of  75      >\u001b[32m Val loss decreased from: 0.015160675160586834  to  0.015158806927502155 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015158806927502155\n",
            "epoch  71  out of  75      >\u001b[32m Val loss decreased from: 0.015158806927502155  to  0.015157295390963554 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.015157295390963554\n",
            "epoch  73  out of  75      >\u001b[32m Val loss decreased from: 0.015157295390963554  to  0.015096058137714863 \u001b[0m   >saving model as:  ./model_108.pth\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015096058137714863\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.015096058137714863\n",
            "loading models...\n",
            "ensemble initialised with  7  models\n",
            "\u001b[1;31m fold  10   :: mean loss on all folds:  0.014957245439291 \u001b[0m\n",
            "epoch  1  out of  75      >\u001b[32m Val loss decreased from: 1000000  to  0.020442239940166473 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  2  out of  75      >\u001b[32m Val loss decreased from: 0.020442239940166473  to  0.018883245065808296 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  3  out of  75      >\u001b[32m Val loss decreased from: 0.018883245065808296  to  0.017864547669887543 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  4  out of  75      >\u001b[32m Val loss decreased from: 0.017864547669887543  to  0.017421700060367584 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  5  out of  75      >\u001b[32m Val loss decreased from: 0.017421700060367584  to  0.017262108623981476 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  6  out of  75      >\u001b[32m Val loss decreased from: 0.017262108623981476  to  0.01681046560406685 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch  7  out of  75      >showing no improvements, best loss yet: 0.01681046560406685\n",
            "epoch  8  out of  75      >\u001b[32m Val loss decreased from: 0.01681046560406685  to  0.016145477071404457 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  9  out of  75      >\u001b[32m Val loss decreased from: 0.016145477071404457  to  0.01594105362892151 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  10  out of  75      >\u001b[32m Val loss decreased from: 0.01594105362892151  to  0.015858523547649384 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  11  out of  75      >showing no improvements, best loss yet: 0.015858523547649384\n",
            "epoch  12  out of  75      >showing no improvements, best loss yet: 0.015858523547649384\n",
            "epoch  13  out of  75      >\u001b[32m Val loss decreased from: 0.015858523547649384  to  0.015825416892766953 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  14  out of  75      >showing no improvements, best loss yet: 0.015825416892766953\n",
            "epoch  15  out of  75      >showing no improvements, best loss yet: 0.015825416892766953\n",
            "epoch  16  out of  75      >\u001b[32m Val loss decreased from: 0.015825416892766953  to  0.015809636563062668 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  17  out of  75      >\u001b[32m Val loss decreased from: 0.015809636563062668  to  0.01573673076927662 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  18  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  19  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  20  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  21  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  22  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  23  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  24  out of  75      >showing no improvements, best loss yet: 0.01573673076927662\n",
            "epoch  25  out of  75      >\u001b[32m Val loss decreased from: 0.01573673076927662  to  0.01567014306783676 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  26  out of  75      >showing no improvements, best loss yet: 0.01567014306783676\n",
            "epoch  27  out of  75      >\u001b[32m Val loss decreased from: 0.01567014306783676  to  0.015653707087039948 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  28  out of  75      >showing no improvements, best loss yet: 0.015653707087039948\n",
            "epoch  29  out of  75      >\u001b[32m Val loss decreased from: 0.015653707087039948  to  0.015626106411218643 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  30  out of  75      >showing no improvements, best loss yet: 0.015626106411218643\n",
            "epoch  31  out of  75      >showing no improvements, best loss yet: 0.015626106411218643\n",
            "epoch  32  out of  75      >showing no improvements, best loss yet: 0.015626106411218643\n",
            "epoch  33  out of  75      >\u001b[32m Val loss decreased from: 0.015626106411218643  to  0.015596535056829453 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  34  out of  75      >\u001b[32m Val loss decreased from: 0.015596535056829453  to  0.015521707013249397 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  35  out of  75      >\u001b[32m Val loss decreased from: 0.015521707013249397  to  0.01550483051687479 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  36  out of  75      >showing no improvements, best loss yet: 0.01550483051687479\n",
            "epoch  37  out of  75      >showing no improvements, best loss yet: 0.01550483051687479\n",
            "epoch  38  out of  75      >\u001b[32m Val loss decreased from: 0.01550483051687479  to  0.015491928905248642 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  39  out of  75      >\u001b[32m Val loss decreased from: 0.015491928905248642  to  0.015491877682507038 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  40  out of  75      >\u001b[32m Val loss decreased from: 0.015491877682507038  to  0.01539995800703764 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  41  out of  75      >showing no improvements, best loss yet: 0.01539995800703764\n",
            "epoch  42  out of  75      >showing no improvements, best loss yet: 0.01539995800703764\n",
            "epoch  43  out of  75      >showing no improvements, best loss yet: 0.01539995800703764\n",
            "epoch  44  out of  75      >\u001b[32m Val loss decreased from: 0.01539995800703764  to  0.015332170762121677 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  45  out of  75      >showing no improvements, best loss yet: 0.015332170762121677\n",
            "epoch  46  out of  75      >showing no improvements, best loss yet: 0.015332170762121677\n",
            "epoch  47  out of  75      >showing no improvements, best loss yet: 0.015332170762121677\n",
            "epoch  48  out of  75      >\u001b[32m Val loss decreased from: 0.015332170762121677  to  0.015295066870748997 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  49  out of  75      >showing no improvements, best loss yet: 0.015295066870748997\n",
            "epoch  50  out of  75      >showing no improvements, best loss yet: 0.015295066870748997\n",
            "epoch  51  out of  75      >\u001b[32m Val loss decreased from: 0.015295066870748997  to  0.01518695056438446 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  52  out of  75      >showing no improvements, best loss yet: 0.01518695056438446\n",
            "epoch  53  out of  75      >showing no improvements, best loss yet: 0.01518695056438446\n",
            "epoch  54  out of  75      >\u001b[32m Val loss decreased from: 0.01518695056438446  to  0.015185505151748657 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  55  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  56  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  57  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  58  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  59  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  60  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  61  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  62  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  63  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  64  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  65  out of  75      >showing no improvements, best loss yet: 0.015185505151748657\n",
            "epoch  66  out of  75      >\u001b[32m Val loss decreased from: 0.015185505151748657  to  0.015130619518458843 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  67  out of  75      >showing no improvements, best loss yet: 0.015130619518458843\n",
            "epoch  68  out of  75      >showing no improvements, best loss yet: 0.015130619518458843\n",
            "epoch  69  out of  75      >\u001b[32m Val loss decreased from: 0.015130619518458843  to  0.015056525357067585 \u001b[0m   >saving model as:  ./model_109.pth\n",
            "epoch  70  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "epoch  71  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "epoch  72  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "epoch  73  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "epoch  74  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "epoch  75  out of  75      >showing no improvements, best loss yet: 0.015056525357067585\n",
            "\u001b[34m Training complete \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoiFc0ol31-Y"
      },
      "source": [
        "!cp model_100.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_101.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_102.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_103.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_104.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_105.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_106.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_107.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_108.pth /content/drive/\"My Drive\"/kaggle/moa\n",
        "!cp model_109.pth /content/drive/\"My Drive\"/kaggle/moa\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUB-9XDi16CM",
        "outputId": "ec23cc28-30d9-48eb-b63b-733e7dfce500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "plt.plot([min(x) for x in fold_val_losses], label = \"individual model losses\")\n",
        "# plt.axhline(y = min([min(x) for x in fold_val_losses]), linestyle = \"--\", c = \"r\")\n",
        "plt.plot([min(m) for m in ensemble_fold_val_losses], label = \"ensemble model fold losses\")\n",
        "plt.grid()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ukunapB5Jhf"
      },
      "source": [
        "for l in fold_val_losses:\n",
        "    plt.plot(l)\n",
        "    plt.scatter([l.index(min(l))], [min(l)],s = 95, label = str(min(l)))\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "minimas = []\n",
        "for i in fold_val_losses:\n",
        "    minimas.append(min(i))\n",
        "\n",
        "np.array(minimas).mean()\n",
        "## best yet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzyTvqy27niM"
      },
      "source": [
        "all_models = [Model() for i in range (NFOLDS)]\n",
        "\n",
        "for i in range (len(all_models)):\n",
        "    \n",
        "    name = \"./model_\" + str(i + 1) + \".pth\"\n",
        "    all_models[i].load_state_dict(torch.load(name))\n",
        "    all_models[i].to(device)\n",
        "    print(\"Loaded: \", name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXObvCTzaZru",
        "outputId": "86631283-56f6-48de-8994-ecfd5470c79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "all_val_losses = []\n",
        "for i in range(len(all_models)):\n",
        "    print(i)\n",
        "    losses, val_losses = train_one_fold(all_models[i],1 , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 0, train = False, validate = True)\n",
        "    all_val_losses.append(np.mean(np.array(val_losses)))\n",
        "all_val_losses = np.array(all_val_losses)\n",
        "print(\"done validating\")"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "done validating\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsKaVNi4bXma"
      },
      "source": [
        "class model_jury(object):   ## only works for dataloaders for batch size 1 \n",
        "    def __init__(self, all_models):\n",
        "        self.all_models = all_models\n",
        "             \n",
        "    def predict(self, x, plot = False, sigmoid = False):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            if sigmoid == False:\n",
        "                preds = [self.all_models[i](x.to(device)).view(-1).cpu().tolist() for i in range(len(self.all_models))]\n",
        "            else:\n",
        "                preds = [self.all_models[i](x.to(device)).view(-1).cpu().sigmoid().tolist() for i in range(len(self.all_models))]\n",
        "\n",
        "        if plot == True:\n",
        "            for pred in preds:\n",
        "                plt.plot(pred)\n",
        "            plt.show()\n",
        "            \n",
        "        preds = np.array(preds)\n",
        "        mean = np.mean(preds, axis = 0)\n",
        "        return mean.flatten()\n",
        "jury = model_jury(all_models)"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atizQ3kF0hLc",
        "outputId": "37a49733-21af-43ff-c63e-e5a94f004030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"Using \" + str(len(jury.all_models)) + \"  models\""
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Using 8  models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E65h1GQTbZWe"
      },
      "source": [
        "test_dataset = TrainDataset(test, target, noise = False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
        "val_loader_test_jury = DataLoader(dataset= valid_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkDxhM9MdRb4",
        "outputId": "638498fc-a33b-4299-ec36-09204bb87358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0219077c4c9d4e879dd19f80654cfc36",
            "72522f9da8d846b5a505e45f4c80b444",
            "4caa6cbf10504d45848775ea4e43f782",
            "53e7482bfe724664a27bfbb953cbed48",
            "eae95991b9aa4283a296393e67fb37dc",
            "887b9d54f49a4a6c8586c0679dc86c9f",
            "20d24fb9513b48f0b5c88e5945b72230",
            "56f7794993564b93b913eadc3bbea8e2"
          ]
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    benchmark_losses = []\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    for batch in tqdm(val_loader_test_jury):\n",
        "        x, y = batch\n",
        "        pred = jury.predict(x, plot = False, sigmoid = False)\n",
        "        pred = torch.tensor(pred).view(1,-1)\n",
        "        benchmark_losses.append(criterion(pred, y))"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0219077c4c9d4e879dd19f80654cfc36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1688.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4XJCHxndVS5",
        "outputId": "5db3dfc8-efea-417a-85a7-6ba395a63097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "plt.plot(all_val_losses)\n",
        "plt.axhline(y = all_val_losses.mean(), label = \"loss mean = \" + str(all_val_losses.mean()), c = \"r\", linestyle = \"--\")\n",
        "plt.axhline(y = np.array(benchmark_losses).mean(), label = \"jury loss = \" + str(np.array(benchmark_losses).mean()), c = \"g\", linestyle = \"--\")\n",
        "plt.legend(fontsize = 17)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsNmfHMQOe5-",
        "outputId": "f17011e9-255b-48ee-c918-ab156638ad96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "043dd47a12514203ac7cb6ba2b730dd5",
            "ae227a8e7cc1402894dc3e6bef5cf3da",
            "111d6e46c9f84868bbd6b9a23a4a5152",
            "663f229ec44c4f5ebf07e15481586ec8",
            "22a6cc0638bc4345bc345264b9dac38c",
            "f38b7945aeef42d990317a03c5a9e481",
            "4799ddbbf32846d5a3cf7f7de745d4eb",
            "e37e8aeaf1814adb808c794fab6e6a5a"
          ]
        }
      },
      "source": [
        "list_of_preds = []\n",
        "for batch in tqdm(test_loader):\n",
        "    x, y = batch\n",
        "    foo = jury.predict(x, plot = False, sigmoid = True)\n",
        "    list_of_preds.append(foo)\n"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "043dd47a12514203ac7cb6ba2b730dd5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3982.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNHdq359gObR"
      },
      "source": [
        "submission = pd.read_csv('sample_submission.csv')\n",
        "sub_cp = submission\n",
        "sub_cp.to_csv('./submission_cp.csv', index=None, header=True)\n",
        "\n",
        "import csv \n",
        "a = list_of_preds  \n",
        "with open('./submission_cp.csv', \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(a)\n",
        "\n",
        "final_sub = pd.read_csv('./submission_cp.csv', header = None)\n",
        "\n",
        "final_sub.columns = submission.columns[1:]\n",
        "final_sub[\"sig_id\"] = submission[\"sig_id\"]\n",
        "\n",
        "good_cols = np.roll(final_sub.columns.values, 1)\n",
        "final_sub = final_sub[good_cols]"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtN2F5ib61s"
      },
      "source": [
        "targets = [col for col in final_sub.columns]\n",
        "final_sub.loc[test_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\n",
        "final_sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juRuyTPb1Nk6",
        "outputId": "e0b17137-cef3-474b-f737-4a67aa94e388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "final_sub"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.002042</td>\n",
              "      <td>0.013440</td>\n",
              "      <td>0.019780</td>\n",
              "      <td>0.003887</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.005382</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.010027</td>\n",
              "      <td>0.016236</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.005099</td>\n",
              "      <td>0.008744</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>0.003727</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.004956</td>\n",
              "      <td>0.001420</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>0.001717</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.011354</td>\n",
              "      <td>0.007387</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.003320</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.022105</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.001554</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>0.001467</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.002273</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.001030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.003301</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>0.001579</td>\n",
              "      <td>0.005215</td>\n",
              "      <td>0.006508</td>\n",
              "      <td>0.006297</td>\n",
              "      <td>0.015845</td>\n",
              "      <td>0.012908</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.012651</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.002791</td>\n",
              "      <td>0.003339</td>\n",
              "      <td>0.009280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.001306</td>\n",
              "      <td>0.002223</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.017750</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>0.002487</td>\n",
              "      <td>0.008545</td>\n",
              "      <td>0.003289</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.003330</td>\n",
              "      <td>0.007222</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.021344</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>0.003802</td>\n",
              "      <td>0.001993</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000795</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.002006</td>\n",
              "      <td>0.004418</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>0.005541</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.009803</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.009890</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>0.015279</td>\n",
              "      <td>0.004348</td>\n",
              "      <td>0.002985</td>\n",
              "      <td>0.005112</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.007802</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.003411</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.002112</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>0.005068</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.002463</td>\n",
              "      <td>0.002475</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.002124</td>\n",
              "      <td>0.005519</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001993</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000286</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.002610</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.001146</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.014010</td>\n",
              "      <td>0.040485</td>\n",
              "      <td>0.002321</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>0.004439</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.007908</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.004323</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.000490</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.002759</td>\n",
              "      <td>0.014324</td>\n",
              "      <td>0.003775</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.001169</td>\n",
              "      <td>0.001543</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.002228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>0.015589</td>\n",
              "      <td>0.020677</td>\n",
              "      <td>0.004354</td>\n",
              "      <td>0.005789</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.011944</td>\n",
              "      <td>0.018188</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.002412</td>\n",
              "      <td>0.003918</td>\n",
              "      <td>0.002038</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>0.004297</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.004479</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.005842</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.001785</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003965</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.004920</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.002125</td>\n",
              "      <td>0.002059</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.012977</td>\n",
              "      <td>0.011169</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.019353</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>0.001056</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.002006</td>\n",
              "      <td>0.000359</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.001664</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.001807</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.004669</td>\n",
              "      <td>0.003242</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.001948</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.001588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.006907</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.003349</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.004368</td>\n",
              "      <td>0.007436</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.006350</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.002874</td>\n",
              "      <td>0.004461</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>0.002424</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>0.002707</td>\n",
              "      <td>0.006774</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003220</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>0.003861</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.006941</td>\n",
              "      <td>0.014686</td>\n",
              "      <td>0.002772</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.010036</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.004421</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.060397</td>\n",
              "      <td>0.004748</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.002112</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>0.000929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.003581</td>\n",
              "      <td>0.002149</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.007052</td>\n",
              "      <td>0.027519</td>\n",
              "      <td>0.007171</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>0.000834</td>\n",
              "      <td>0.032871</td>\n",
              "      <td>0.030052</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.002563</td>\n",
              "      <td>0.006103</td>\n",
              "      <td>0.005007</td>\n",
              "      <td>0.002059</td>\n",
              "      <td>0.001485</td>\n",
              "      <td>0.003195</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.003609</td>\n",
              "      <td>0.002161</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.003815</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.002879</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.011391</td>\n",
              "      <td>0.028576</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.001169</td>\n",
              "      <td>0.022096</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.004363</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.001679</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.002843</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.001598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>0.000792</td>\n",
              "      <td>0.014967</td>\n",
              "      <td>0.031528</td>\n",
              "      <td>0.004683</td>\n",
              "      <td>0.002602</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.009972</td>\n",
              "      <td>0.028229</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.001594</td>\n",
              "      <td>0.005372</td>\n",
              "      <td>0.003318</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.004516</td>\n",
              "      <td>0.002185</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.002084</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.003991</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>0.010190</td>\n",
              "      <td>0.023705</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.001723</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.016507</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.003015</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.002249</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.001895</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.001177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.023044</td>\n",
              "      <td>0.025644</td>\n",
              "      <td>0.005227</td>\n",
              "      <td>0.005016</td>\n",
              "      <td>0.003611</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.020926</td>\n",
              "      <td>0.023599</td>\n",
              "      <td>0.000948</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.007827</td>\n",
              "      <td>0.002753</td>\n",
              "      <td>0.001617</td>\n",
              "      <td>0.002285</td>\n",
              "      <td>0.003230</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.003588</td>\n",
              "      <td>0.000906</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.004818</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004176</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.006294</td>\n",
              "      <td>0.001153</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.010735</td>\n",
              "      <td>0.010418</td>\n",
              "      <td>0.001712</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.012596</td>\n",
              "      <td>0.001471</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.006567</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.003237</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.001774</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.002806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>0.011365</td>\n",
              "      <td>0.019145</td>\n",
              "      <td>0.003340</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.003932</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.007213</td>\n",
              "      <td>0.017693</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.001746</td>\n",
              "      <td>0.002827</td>\n",
              "      <td>0.008406</td>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>0.002658</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.005201</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.002290</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.003178</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005375</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.002765</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.011060</td>\n",
              "      <td>0.013547</td>\n",
              "      <td>0.002187</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.016622</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>0.002571</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.001048</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.002392</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.001780</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.001030\n",
              "1     id_001897cda  ...       0.003314\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.002228\n",
              "4     id_0027f1083  ...       0.001588\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.000929\n",
              "3978  id_ff925dd0d  ...       0.001598\n",
              "3979  id_ffb710450  ...       0.001177\n",
              "3980  id_ffbb869f2  ...       0.002806\n",
              "3981  id_ffd5800b6  ...       0.001067\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG22QpImgnux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}