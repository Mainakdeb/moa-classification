{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d   ## smoother\nfrom tqdm.notebook import tqdm, tnrange\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.preprocessing import MinMaxScaler\n        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\n\n\ndef seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":31,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'\n    \ndevice","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"'cuda'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]\n\ndrop_list =[]\nfor i in range(875):\n    if i not in top_features:\n        drop_list.append(i)\n        \nprint(len(drop_list), \"features to be dropped\")","execution_count":34,"outputs":[{"output_type":"stream","text":"90 features to be dropped\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"         sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n0  id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n1  id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n2  id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n3  id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n4  id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n\n      g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n0 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n1  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n2 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n3  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n4  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n\n     c-96    c-97    c-98    c-99  \n0 -0.3981  0.2139  0.3801  0.4176  \n1  0.1522  0.1241  0.6077  0.7371  \n2 -0.6417 -0.2187 -1.4080  0.6931  \n3 -1.6210 -0.8784 -0.3876 -0.8154  \n4  0.1094  0.2885 -0.3786  0.7125  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features=train_features.drop(train_features.columns[drop_list], axis=1)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"  cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3     g-5     g-7  \\\n0  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208 -1.0120 -0.0326   \n1  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604  0.5207  0.3372   \n2  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764  1.2390  0.2155   \n3  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288 -0.8095  0.1792   \n4  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919 -0.8244 -0.1498   \n\n     g-10  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n0  1.1830  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n1 -1.1520  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899  0.1522   \n2 -0.4797  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174 -0.6417   \n3 -0.8269  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880 -1.6210   \n4 -0.2219  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031  0.1094   \n\n     c-97    c-98    c-99  \n0  0.2139  0.3801  0.4176  \n1  0.1241  0.6077  0.7371  \n2 -0.2187 -1.4080  0.6931  \n3 -0.8784 -0.3876 -0.8154  \n4  0.2885 -0.3786  0.7125  \n\n[5 rows x 786 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-5</th>\n      <th>g-7</th>\n      <th>g-10</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-1.0120</td>\n      <td>-0.0326</td>\n      <td>1.1830</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>0.5207</td>\n      <td>0.3372</td>\n      <td>-1.1520</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>1.2390</td>\n      <td>0.2155</td>\n      <td>-0.4797</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>-0.8095</td>\n      <td>0.1792</td>\n      <td>-0.8269</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>-0.8244</td>\n      <td>-0.1498</td>\n      <td>-0.2219</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 786 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore_columns = ['sig_id', \"cp_type\"]\n\ntrain_columns = [x for x in train_features.columns if x not in ignore_columns]\n\ntrain = train_features[train_columns]\ntest = test_features[train_columns]\ntarget = train_targets_scored.iloc[:,1:].values","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = ColumnTransformer([\n                            ('o',OneHotEncoder(),[0,1]),\n                            ('s',Normalizer(),list(range(3,train.shape[1])))  ## remove\n                        ])\n\n\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, target.shape","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"((23814, 787), (3982, 787), (23814, 206))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, train,targets, noise ):\n        \n        self.features  = train\n        self.targets = targets\n        self.noise = noise\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float() \n        \n#         if self.noise == True:\n# #             print(\"noisy boi\")\n#             feature  = feature + torch.randn_like(feature)/150\n            \n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature,target","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef train_step(x, y, model, optimizer, criterion):\n    optimizer.zero_grad()\n    pred = model(x.to(device))\n    y = y.float()\n    loss = criterion(pred,y.to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('Linear') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        \n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(787)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(787, 1048))\n        \n        self.batch_norm2 = nn.BatchNorm1d(1048)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(1048, 2096))\n        \n        self.batch_norm3 = nn.BatchNorm1d(2096)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(2096, 2096))\n        \n        self.batch_norm4 = nn.BatchNorm1d(2096)\n        self.dropout4 = nn.Dropout(0.5)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(2096, 1048))\n        \n        self.batch_norm5 = nn.BatchNorm1d(1048)\n        #self.dropout5 = nn.Dropout(0.5)\n        self.dense5 = nn.utils.weight_norm(nn.Linear(1048, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n        \n        x = self.batch_norm5(x)\n        #x = self.dropout5(x)\n        x = (self.dense5(x))\n        \n        return x\n    \n    \n    \nmodel = Model()","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n    \n    losses = []\n    val_losses = []\n    learning_rates = []    \n    best_loss = 1000000\n\n    for epoch in range(num_epochs):\n\n            \n        if train == True:\n            model.train()\n            losses_temp = []\n            for batch in train_loader:\n                (x_batch, y_batch) = batch\n                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n                losses_temp.append(loss)\n            losses.append(torch.mean(torch.tensor(losses_temp)))\n            scheduler.step(1.)   ## lr decay caller \n            learning_rates.append(get_lr(optimizer))\n            \n\n        if validate == True:\n            with torch.no_grad():\n                model.eval()\n                val_losses_temp = []\n                for x_val, y_val in val_loader:\n                    yhat =model(x_val.to(device))  # pred \n                    val_loss = criterion(yhat.to(device), y_val.to(device))\n                    val_losses_temp.append(val_loss.item())  ## metrics \n                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n\n        \n        if train == True:\n            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n\n            if val_losses[-1] <= best_loss:\n\n                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n                best_loss = val_losses[-1]\n\n                name = \"./model_\" + str(fold_number)+\".pth\"\n\n                print(\"saving model as: \", name)\n\n                torch.save(model.state_dict(), name)\n\n            else: \n                print(\"showing no improvements, best loss yet:\", best_loss)\n\n        if show_plots == True:\n\n            show_lr(learning_rates)\n            plt.plot(val_losses, label = \"val\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(losses, label = \"train\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n        \n    return losses, val_losses","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building folds \n\nCGREEN  = '\\33[32m'\nCBLUE =  '\\033[34m'\nCRED = '\\033[1;31m'\nCEND  = '\\33[0m'\n\nNFOLDS =10\nnum_epochs = 40\n\nkfold = KFold(NFOLDS,shuffle=True,random_state=42)\nfold_train_losses = list()\nfold_valid_losses = list()\n\n\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(train)):\n\n    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n\n    input_size = x_train.shape[1]\n    output_size = target.shape[1]\n    \n    \n    train_dataset = TrainDataset(x_train, y_train, noise = False)\n    valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n    \n    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers = 8)\n\n    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True, num_workers = 8)\n    \n    model = Model()\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.4e-3)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     mode='min', \n                                                     factor=0.5, \n                                                     patience=5, \n                                                     eps=1e-5, \n                                                     verbose=True)\n    criterion = nn.BCEWithLogitsLoss()\n    print(CRED ,\"fold \", str(k+1), CEND)\n\n    train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+1)\n    \nprint(CBLUE, \"Training complete\", CEND)","execution_count":45,"outputs":[{"output_type":"stream","text":"\u001b[1;31m fold  1 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06186182051897049 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06186182051897049  to  0.02875644899904728 \u001b[0m   >saving model as:  ./model_1.pth\n\u001b[1;31m fold  2 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06153801828622818 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06153801828622818  to  0.029762383550405502 \u001b[0m   >saving model as:  ./model_2.pth\n\u001b[1;31m fold  3 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06069359928369522 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06069359928369522  to  0.028773153200745583 \u001b[0m   >saving model as:  ./model_3.pth\n\u001b[1;31m fold  4 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06014334037899971 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06014334037899971  to  0.028532003983855247 \u001b[0m   >saving model as:  ./model_4.pth\n\u001b[1;31m fold  5 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06021584942936897 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06021584942936897  to  0.029019419103860855 \u001b[0m   >saving model as:  ./model_5.pth\n\u001b[1;31m fold  6 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.0601978525519371 \u001b[0m   >saving model as:  ./model_6.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.0601978525519371  to  0.02874979004263878 \u001b[0m   >saving model as:  ./model_6.pth\n\u001b[1;31m fold  7 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06056574732065201 \u001b[0m   >saving model as:  ./model_7.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06056574732065201  to  0.028862398117780685 \u001b[0m   >saving model as:  ./model_7.pth\n\u001b[1;31m fold  8 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.060015566647052765 \u001b[0m   >saving model as:  ./model_8.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.060015566647052765  to  0.028309527784585953 \u001b[0m   >saving model as:  ./model_8.pth\n\u001b[1;31m fold  9 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.060309577733278275 \u001b[0m   >saving model as:  ./model_9.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.060309577733278275  to  0.028312211856245995 \u001b[0m   >saving model as:  ./model_9.pth\n\u001b[1;31m fold  10 \u001b[0m\nepoch  1  out of  2      >\u001b[32m Val loss decreased from: 1000000  to  0.06032248213887215 \u001b[0m   >saving model as:  ./model_10.pth\nepoch  2  out of  2      >\u001b[32m Val loss decreased from: 0.06032248213887215  to  0.02887822687625885 \u001b[0m   >saving model as:  ./model_10.pth\n\u001b[34m Training complete \u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [Model() for i in range (NFOLDS)]\n\nfor i in range (len(all_models)):\n    \n    name = \"./model_\" + str(i + 1) + \".pth\"\n    all_models[i].load_state_dict(torch.load(name))\n    all_models[i].to(device)\n    print(\"Loaded: \", name)","execution_count":46,"outputs":[{"output_type":"stream","text":"Loaded:  ./model_1.pth\nLoaded:  ./model_2.pth\nLoaded:  ./model_3.pth\nLoaded:  ./model_4.pth\nLoaded:  ./model_5.pth\nLoaded:  ./model_6.pth\nLoaded:  ./model_7.pth\nLoaded:  ./model_8.pth\nLoaded:  ./model_9.pth\nLoaded:  ./model_10.pth\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_val_losses = []\nfor i in range(NFOLDS):\n    losses, val_losses = train_one_fold(all_models[i],5 , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 0, train = False, validate = True)\n    all_val_losses.append(np.mean(np.array(val_losses)))\nall_val_losses = np.array(all_val_losses)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class model_jury(object):   ## only works for dataloaders for batch size 1 \n    def __init__(self, all_models):\n        self.all_models = all_models\n        \n        \n        \n    def predict(self, x, plot = False, sigmoid = False):\n        \n        with torch.no_grad():\n            \n            if sigmoid == False:\n                preds = [self.all_models[i](x.to(device)).view(-1).cpu().tolist() for i in range(len(self.all_models))]\n            else:\n                preds = [self.all_models[i](x.to(device)).view(-1).cpu().sigmoid().tolist() for i in range(len(self.all_models))]\n\n        \n        if plot == True:\n            for pred in preds:\n                plt.plot(pred)\n            plt.show()\n            \n        preds = np.array(preds)\n        mean = np.mean(preds, axis = 0)\n        return mean.flatten()\njury = model_jury(all_models)\n","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TrainDataset(test, target, noise = False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers = 8)\nval_loader_test_jury = DataLoader(dataset= valid_dataset, batch_size=1, shuffle=False, num_workers = 8)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    benchmark_losses = []\n    criterion = nn.BCEWithLogitsLoss()\n    for batch in tqdm(val_loader_test_jury):\n        x, y = batch\n        pred = jury.predict(x, plot = False, sigmoid = False)\n        pred = torch.tensor(pred).view(1,-1)\n\n        benchmark_losses.append(criterion(pred, y))","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2381.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7225bbdf7b2244c6943f7abffc26cb89"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_val_losses)\nplt.axhline(y = all_val_losses.mean(), label = \"loss mean = \" + str(all_val_losses.mean()), c = \"r\", linestyle = \"--\")\nplt.axhline(y = np.array(benchmark_losses).mean(), label = \"jury loss = \" + str(np.array(benchmark_losses).mean()), c = \"g\", linestyle = \"--\")\nplt.legend(fontsize = 17)\nplt.grid()\nplt.show()","execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAcEAAAD4CAYAAACUuV05AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3xUVd748c+Z9N4boaRAEkA60iEFRECKBRUbqCAWXNe2a1l/lgUf93FdVlcBRSkr8qiIKIggRRNAegstJBBaSAKkQHqfOb8/MomZ1AlMmCRz3q/XvJg595w733szzHfuueeeK6SUKIqiKIol0pg7AEVRFEUxF5UEFUVRFIulkqCiKIpisVQSVBRFUSyWSoKKoiiKxbI2dwCm4O3tLYOCgq67fWFhIU5OTqYLqA1T+8KQ2h9/UPvCUHvYHwcPHsySUvqYOw5zahdJMCgoiAMHDlx3+7i4OKKiokwXUBum9oUhtT/+oPaFofawP4QQF8wdg7mp7lBFURTFYqkkqCiKolgslQQVRVEUi6WSoKIoimKxVBJUFEVRLJZKgoqiKIrFUklQURRFsVjt4jpBRWkJWxOukF2oM3cYiqK0IHUkqCj1yC0u58mvDvLdqTJzh6IoSgtSSVBR6vH76Sy0OsnxLC3lWnU0qCjtlUqCilKP2KQMAEq0sP/8VTNHoyhKS1FJUFFq0ekkcUmZxET4Yi0gNjHD3CEpitJCVBJUlFoSLuWRVVDKHb0CCPfUEJuUae6QFEVpISoJKkotVUd+keE+9PGxJjmjgItXi8wclaIoLUElQUWpJTYpgz4d3fB2tqO3jxUAv6kuUUVpl1QSVJQarhaWcfhiDlHhvgD4O2kI9naqHiijKEr7opKgotSw43QmUkJ0hG91WXS4L7vPZFNcpjVjZIqitASVBBWlhtjEDLycbOkd6FZdFh3hQ2mFjl1nsswYmaIoLUElQUXR0+ok205lMirMB41GVJcPCvbE0dZKdYkqSjukkqCi6B1JzeFaUTlR4T4G5XbWVozo6k1sYiZSSjNFpyhKS1BJUFH04pIy0QgY1c2nzrLoCF/Scoo5daXADJEpitJSVBJUFL24pAz6dfbAw8m2zrJo/WhR1SWqKO2LSoKKAmTml3I0NZfo8LpHgQD+bvb0CHBV1wsqSjujkqCiANtOVU6NVnV9YH2iI3w4eOEauUXlNyssRVFamEqCikJlN6evix09O7g2WCcmwhetTrIjWc0lqijthUqCisWr0OrYfiqTqHAfhBAN1uvbyQN3RxvVJaoo7YhKgorFO5SSQ35JRaNdoQBWGkFkmA/bkjLR6dSlEorSHqgkqFi82KQMrDSCEd28m6wbE+FLdmEZR9Nyb0JkiqK0NJUEFYsXl5TJwC4euNrbNFl3VDcfNELdVUJR2guVBBWLdjm3hJOX8gwmzG6Mh5Mt/Tp7qLvNW7j4izkczqgwdxiKCagkqFi0OP3F79FNnA+sKSbCl2NpuWTkl7RUWEor9/qaY3xyuJRzWYXmDkW5QSoJKhYtNimDDm72hPk5G92mKmHGJalLJSzR6Sv5JFzKQyvhvQ0nzR2OcoNUElQsVlmFjt9PZxEV4dvopRG1dQ9wwd/VXnWJWqh1R9LRCLitizWbE66oW2y1cSoJKhbrwPmrFJZpm9UVCiCEIDrChx2nsyjX6looOqU1klKyNj6d4V29uTfMlkB3B+atP4lWXTLTZhmVBIUQ44QQSUKIZCHEq/UsF0KI/+iXHxVC9NeXdxJCxAohTgohTggh/lyjTV8hxB4hRLwQ4oAQYpC+3FYIsUwIcUwIcUQIEWWibVUUA7FJGdhaaRgW6tXsttHhvhSUVrD//NUWiExpreIv5pBytYjJfTpgayV4ZXwECZfy+P5gqrlDU65Tk0lQCGEFLADGAz2AB4QQPWpVGw900z9mA4v05RXAS1LK7sAQYE6Ntu8D70gp+wJv6l8DPAEgpewF3Ab8SwihjlgVk4tNymRQsCdOdtbNbju8qze2VhrVJWph1sanY2ut4fZb/AGY1DuA/p3deX9TEgWlarRoW2RMchkEJEspz0opy4BvgCm16kwBvpSV9gDuQogAKeUlKeUhACllPnASCNS3kUDVRI1uQLr+eQ/gV32bDCAHGHhdW6coDbh4tYjkjII6N9A1lpOdNYNDPIlVg2MsRoVWx/qjlxgd4Vt9TakQgv83sQdZBaUsiks2c4TK9TDmJ3AgcLHG61RgsBF1AoFLVQVCiCCgH7BXX/Q8sEkI8QGVyXiYvvwIMEUI8Q3QCRig/3dfzTcUQsym8qgTPz8/4uLijNiU+hUUFNxQ+/bEUvbFbymVd4Jwyj1PXFxKg/Ua2x+drMrZkVHGdxt+w8ex/XdWWMpnoyHHs7RkFZQSan2NuLg4g/0xNMCKz7adIViXjrdD+/8stCfGJMH6hs3VPgvcaB0hhDPwPfC8lDJPX/w08IKU8nshxH3AEmAMsBToDhwALgC7qOxWNVy5lIuBxQADBw6UUVFRRmxK/eLi4riR9u2JpeyLFcv309mzgGl3RDU6MrSx/dElq5D/S4yjyD2EqGFBLRNoK2Ipn42GrP/uCC52l3n2nmjsbawM9kdY32Ji/hXHthwPPh7fz7yBKs1izE+WVCqPxKp05I+uyybrCCFsqEyAK6WUa2rUmQFUvf6Oym5XpJQVUsoXpJR9pZRTAHfgtHGboyhNKynXsvNMFtFN3DWiKcHeTgR7O6m7zVuAknItvxy/zLhb/LG3saqzvIO7A7NHhvDTkXQOXrhmhgiV62VMEtwPdBNCBAshbIFpwLpaddYB0/WjRIcAuVLKS6LyG2YJcFJKOb9Wm3QgUv88Bn2iE0I4CiGc9M9vAyqklAnXs3GKUp+9565SUq4jysip0hoTFe7D7jPZFJdpTRCZ0lrFJmZQUFrBlL6BDdZ5MjIUXxc75q5PUHcZaUOaTIJSygrgWWATlQNbVkkpTwghnhJCPKWvtgE4CyQDnwPP6MuHA48AMfpLIeKFEBP0y56gcuTnEeB/0J/fA3yBQ0KIk8Ar+vaKYjKxiRnYWWsYGtL8SyNqi4nwpbRCpy6YbufWxqfj7WzH0EYup3Gys+Yvt4cTfzGHdUdqd5YprZVRY8OllBuoTHQ1yz6t8VwCc+pp9zv1ny+sWjagnvLzQLgxcbUX57IK+fXkFR4dFoS1lTqp3tLikjIYGupVb7dWcw0K9sTR1orYpAxGd/czQXRKa5NbXM5vSRk8NLgzVprGu8/v6d+R/+4+z//+ksjtPf1xsL3xz5jSstQ3rpkdT8vlnkW7mPfzSX4+dqnpBsoNOZdVyPnsombPEtMQO2srhnf1JjYxk8rfgkp7s+nEZcoqdI12hVbRaARvTuzJpdwSPt9x9iZEp9wolQTNaM/ZbKYt3oODjRVBXo4sijujvkhb2PXcNaIpMRG+pOUUc+pKgcnWqbQe6+LT6eLlSJ+ObkbVHxTsyfhb/FkUd4YreepOI62dSoJmsiXhCtOX7sPfzZ7VTw/lTzHdSLycr0YatrDYpExCfJzo7OVosnVWJVT1t2t/MvJK2HUmiyl9OjRrJPFr47uj1Un+uSmpBaNTTEElQTNYcyiVp746SHd/F1Y9OZQANwcm9+1AoLsDC2PPmDu8dquorII9Z7NNehQI4O9mT/cAV3W3+XZo/dFL6CRM7tuhWe06ezny2PAgvj+UyrHU3BaKTjEFlQRvsmU7z/HiqiMMDvZk5RND8HSyBcDGSsMTI4M5cOGampS5hew+k01Zhc7kSRAgJsKHgxeukVtUbvJ1K+az9kg6PTu40tXXpdlt58R0xdPRlrnrE9RpjlZMJcGbRErJv7ec4p2fEhjbw4+lj96Kc62Jm++/tTNeTrYsjFVzELaE2KQMHG2tuDXYw+TrjonwRauT7EhWc4m2F+ezCjlyMYcpzTwKrOJqb8OLY8PYd/4qvxy/bOLoFFNRSfAm0Okk7/yUwEe/nmbqgI4sfKh/vcPzHWyteGx4ELFJmSSk59WzJuV6SSmJTcxkeFdv7KxNP2y9bycP3B1tVJdoO7LuSDpCwKQ+15cEAe4f2IlwPxfe25hIaYWaUKE1UkmwhZVrdbz03RGW7zrPrBHBvH9P70avBXxkSBBOtlZ8uk2dGzSl5IwC0nKKW6QrFMBKI4gM82FbUqaaLaQdkFLyY3wag4I8CXBzuO71WFtpeGNid1KuFrF853nTBaiYjEqCLaikXMtTKw7yw+E0/nJ7OH+7ozuaJi62dXO04eEhXVh/NJ0L2YU3KdL2r2rk5vXeOskYMRG+ZBeWcTRNDYRo606k53E2s9CoawObMrKbDzERvnzyWzJZBaUmiE4xJZUEW0heSTnTl+7jt6QM5t55C3Oiuxo9xHrmiGCsNRo+264utjWV2MRMwv1c6OB+/b/qmzKqmw8ageoSbQfWHUnHxkowXn/z3Bv1+oTuFJdr+feWUyZZn2I6Kgm2gKyCUh5YvIdDF67x0bR+PDKkS7Pa+7rac8+Ajqw+kEqGutj2huWXlHPgwlWiIlruKBDAw8mWfp091N3m2zidTrIuPp3IMB889KO3b1RXX2ceHtKFr/elkHhZne9vTVQSNLG0nGLu+3Q3ZzIL+HzGQCZf50n1pyJDqNDpWLLznIkjtDw7k7Mp18oWOx9YU0yEL8fScsnIVz9e2qp9569yOa+EySboCq3pz6O74WJvw7s/n1SXTLQiKgmaUHJGAVMX7SKzoJQVMwff0JduFy8n7ujdgZV7UsgtVtee3Yi4pAxc7KwZ0MX0l0bUVnXOMS5JXSrRVq2NT8fR1oox3U37o8nDyZY/j+7GjtNZanahVkQlQRM5mprDfZ/tplwr+Xb2UG4N8rzhdT4VGUJBaQVf7blggggtk5SS2KQMRoZ5Y3MT7tDRI8AVf1d71SXaRpVV6Nhw7BJje/jhaGvUTXaa5ZGhXQjxdmLezycp1+pMvn6l+VQSNIHdZ7J5YPEeHG2tWP3UUHp0cDXJent2cCMq3Ielv59TN229Ticv5XMlr5Som9AVCiCEIDrChx2ns9SXXBu0/VQmucXlJhkVWh8bKw2vT+jO2cxCVqoft62CSoI3aEvCFWYs20cHdwdWPzWMIG8nk67/maiuZBeWserARZOu11JUXxoR1rKDYmqKCveloLRCTX/XBq09ko6How0junm32HuM7u7L8K5efPjraXKKylrsfRTjqCR4A74/qJ8IO8CVVU8Oxd/N3uTvcWuQBwO6eLB4+1l1ZHEd4pIy6NnBFV9X0/9tGjKiqze2VhrVJdrGFJZWsCXhMnf0DmjRrnMhBG/c0YO84nI++vV0i72PYhyVBK/T0t/P8dJ3RxgS4snKWYNNNpS6NiEEz0SFkpZTzE9H0lvkPdqr3KJyDl64dlNGhdbkZGfN4BBPYtXgmDZlS8IVSsqNu3nujeoe4Mr9t3Zixe4LnMlU96E0J5UEm0lKyfzNSfx9fQLjevrXOxG2qcVE+BLu58KiuDNqSq5m2JGciU5CdAtfH1ifqHBfkjMKuHi16Ka/t3J91sanEejuwIDOLT+KGODF28Kxt7HivQ0nb8r7KfVTSbAZdDrJ2+tO8J/fkrlvYEc+ebBfi0zGXJsQgqejQjmdUcCvqovNaLGJmbg72tC30835UqspJqLy6FPNHtM2ZBeUsv10FpP6dGhyakNT8XGxY050V7aezGBnctZNeU+lLpUEjVSu1fHiqnj+u/sCs0eF8L9NTIRtahN7B9DJ04GFccnqQlsj6HSSbacyGNXNB6ub9KVWU7C3E8HeTup6sDZiw/HLaHXyum+bdL0eGx5ERw8H5q5PQKt6ecxCJUEjlJRreXLFQX6MT+cvt4fz2vgIo+cBNRVrKw2zR4VyOCWHPWfVqMOmHE/PJaugzCxdoVWiwn3YfSZbXd7SBqyLTyPMz5kI/+bfPPdG2NtY8dr47iRezlcjwM1EJcEm5JWUM33JPmKTMpjXzImwTe3eAR3xdrZlkbrNUpNiEzMRonJSa3OJifCltELHrjOqq6s1S71WxP7z15jSN9As/7cn9PLn1iAP/rU5ifwSNTvUzaaSYCMy80uZ9tkeDqVc4z/T+vFwMyfCNjV7GyseHxHM9lOZHFe362lUbFIGfTq64+VsZ7YYBgV74mhrpbpEW7mfjlwCuO55fm9U1SUTWQVlLIhVP3BvNpUEG5B6rYj7PtvN2awCvpgx8IbuLm1KDw/pgoudNYvi1H+WhmQXlHIkNadF7x1oDDtrK4Z39SY2MVOdx23F1san0b+zO508Hc0WQ59O7tzdP5Clv59TI4pvMpUE65Gckc/URbvJLijlq5mDb9qUW8Zwtbfh4aFd2HD8EmfV9UX12n46Eym56dcH1icmwpe0nGJOXVF/q9Yo6XI+iZfzb8q1gU356+0RWGkE/9iYaO5QLIpKgrUcTc3h3k93U6GTfPvkUAaaYCJsU3t8eDC2VhoWq5vu1isuKRMvJ1t6BbqZO5TqRKy6RFundUfSsNIIJvQKMHco+LvZ82RkCD8fu6Sm3LuJVBKsYdeZLB5YvAcnO2tWPzWU7gGmmQjb1Hxc7LhvYCe+P5TK5Vx137qatDrJtlOZRIb73LTrvRrj72ZP9wBXdb1gKySlZG18OsO7euPjYr5zxzXNHhWCv6s9c9cnqIkxbhKVBPU2nbjMo8v2E+jRMhNhm9rsUSHoJHyxQx0N1hR/MYecovJW0RVaJSbCh4MXrpFbpEb+tSaHUnJIvVbMlFZyvh/A0daav44L52hqLj8cTjN3OBZBJUFgR2o5T391kB4tOBG2qXXydGRS7wD+b1+Kmom+hrikDDRmvjSituhwX7Q6yY5kNZdoa7IuPg07aw1je/qZOxQDd/YNpHdHN97flEhRWYW5w2n3LD4JLvn9HEuOlzEs1JuVswbj7tgyE2G3hKeiQikq0/LfXeq+ZFVikzIY0MUDN0cbc4dSrV9nD9wdbVSXaCtSodWx/uglxnT3w8W+9XxWADQawf+b2IMreaV8tk319LQ0o5KgEGKcECJJCJEshHi1nuVCCPEf/fKjQoj++vJOQohYIcRJIcQJIcSfa7TpK4TYI4SIF0IcEEIM0pfbCCH+K4Q4pm/3mqk2trYdpzOZuz6BgX5WLHl0IE4tPBG2qUX4uzI6wpflu86pX4xARl4Jx9PyWtVoXgArjSAyzIdtSZnqPE8rsfNMNtmFZUy+ydOkGevWIE/u6B3AZ9vPcCm32NzhtGtNJkEhhBWwABgP9AAeEEL0qFVtPNBN/5gNLNKXVwAvSSm7A0OAOTXavg+8I6XsC7ypfw1wL2AnpewFDACeFEIEXdfWNWFEV2/+fX8fnu5jd1Mmwm4Jz0SHcq2onG/2qSmX4k5Vdjea+/rA+kSH+5JdWMZRNclBq7A2Pg0Xe+tW+Vmp8uq4CHQS/vlLkrlDadeMORIcBCRLKc9KKcuAb4AptepMAb6UlfYA7kKIACnlJSnlIQApZT5wEqi6IEcCVcMv3YD0GuVOQghrwAEoA/Kub/MaJ4Tgrn4dzTLBsqkM6OLJoGBPPt9xlrIKy77pblxSBr4udvRohaN6I8N80Ah1V4nWoKRcy+YTV5hwS0Cr/vHbydORmSOCWXM4jSMXc8wdTrtlTBIMBGoeZqTyRyIzuo7+aK4fsFdf9DzwTyHEReADoKrbczVQCFwCUoAPpJTqoplGPB0VyqXcEn6Mt9zRZOVaHTtOZxEd7mu2uV0b4+FkS7/OHupu863Ab4kZFJRW3PQ7RlyPZ6JC8Xa2Ze76BDXrUAsx5iRYfd8otf8ajdYRQjgD3wPPSymrjuqeBl6QUn4vhLgPWAKMofLIUwt0ADyAHUKIrVJKgzPEQojZVHa94ufnR1xcnBGbUr+CgoIbam92UtLZRcP8jcfwzk9GcwNJoK3ui6SrWvJLKvCpyDBp/KbcH11sy1hzoZwfN/2Gu13bG5PWVj8btS05XIK7naDk4jHiUlv//5WJXWD5iWv885tfGRTQtsYttAXG7NFUoFON1x35o+uyyTpCCBsqE+BKKeWaGnVmAFUDZb4DvtA/fxD4RUpZDmQIIXYCAwGDJCilXAwsBhg4cKCMiooyYlPqFxcXx420bw0KPNP509eHKfOJYNwt1z/7RVvdF3s2JmKtOcuTd0aadLSfKfeHT1gua07/TplXN6IGdmq6QSvTVj8bNeUWl3Nsy1YeGRpETHTtoQ3Nc7P2x0idZM9/drAupYLnpo7E3qb1duG2Rcb8HN0PdBNCBAshbIFpwLpaddYB0/WjRIcAuVLKS6KyX2oJcFJKOb9Wm3QgUv88Bjitf54CxOjX5UTlgBo1mV4Txt/iTxcvRxbGnbHIbpO4pAxuDfJsdcPda+oR4Iqfq53qEjWjTccvU6bVtYmu0CpWGsGbE3uQeq2YpTvPmTucdqfJJCilrACeBTZRObBllZTyhBDiKSHEU/pqG6g8UksGPgee0ZcPBx6hMqnF6x8T9MueAP4lhDgC/A/6rk0qR6I6A8epTMDLpJRHb3A72z1rKw1PjgrlaGouu85kmzucmyo9p5jEy/lmvYGuMYQQRIf7suN0FuVayx7EZC5rj6QR7O3UKuaVbY5hXb0Z092PhbFnyMwvNXc47YpRJyaklBuklGFSylAp5bv6sk+llJ/qn0sp5Rz98l5SygP68t+llEJK2VtK2Vf/2FBj2QApZR8p5WAp5UF9eYGU8l4pZU8pZQ8p5T9bZtPbn3sGBOLrYsfCuGRzh3JTxSVVXhrRmqZKa0h0hC8FpRVqgmQzyMgrYdeZbCb16dAqB0815fUJEZSUa5m/RV0yYUpt7+y80iA7aytmjghmZ3K2RQ2pjk3KINDdga6+zuYOpUkjunpjYyVUl6gZ/HT0ElKa7+a5NyrEx5npQ4P4Zv9FEtJb5Koxi6SSYDvz0JAuuNpbzk13Syu07EzOIircp038uneys2ZwsBexSWoe0ZttXXwatwS6tokfSw358+huuDnYMO9ndcmEqYj2sCMHurjIAwMGGBbedx888wwUFcGECXUbPfpo5SMri5wxY3B3dzdc/vTTcP/9cPEiPPJI3fYvvQSTJkFSEjz5ZN3lb7wBY8ZAfDw8/3zd5f/zPzBsGOzaBa+/Xnf5hx9C376wdSvMm1d3+WefQXg4/PQT/OtfBosuXivi/hHP8OVbU+ka+zMsWlS3/erV4O0Ny5dXPvRycnIq98WGDeDoCAsXwqpVddtXDQ3/4ANYv95wmYMDbNxY+XzuXPj1V8PlXl7w/feVz197DXbvNlzesSN89VXl8+efr9yHNYWFweLFAFyeNp3zuw4T7u+CR9W8r337Vu4/gIcfhtRUw/ZDh8J771U+v+ceyK51DnX0aPh//w+A7MGD8XJwMFw+cSK8/HLl8/pGBzbx2ds+7A6m63qwc2YvAp+YXrd9K/3sVX82GvnsAbBiBXTqBN9+26zPXrUW+OwVl2s5cjEHv+BAgrf9Urn8Bj976RMn0qGg1s2STfjZY/x4KK41ZdrEiSwfeg9v/5TAod/exbP2XMfN+N5j6lTEtm0HpZQD61a0HOpIsB3yd7XH1krDp9va/9Fgek4xQghcHVrvqNDaqu5TueN0lpkjsRzZBZV3WmkLd4hpykNDuhDq40RKdhFqKloTkFK2+ceAAQPkjYiNjb2h9q3RW2uPy9DXfpZp14qa1a6t7YuYD2Llw1/sabH1t9T+iHz/Nzlj6d4WWXdLaWufjSo6nU5GfxAr7/9sl0nXa8798dvJK7LLK+vlFzvO3tB6gAOyFXyHm/OhjgTbqVkjgwH4vB3fdDclu4gzmYVtYlRobdERvuw+k01xmdbcobR7J9LzOJtZyJS+tWd7bLuiwn0Y2c2bj7ae4lqhup/ojVBJsJ3q6OHI5L4d+GbfRa620/8kcacqR1hGR7S9JBgT4UtphY5dZ1SXaEtbG5+GjZVg/C3+5g7FZIQQvHFHDwpKK/hw6ylzh9OmqSTYjj0dGUpxuZblu86bO5QWEZuYQZCXI8HeTuYOpdkGBXviaGtFbJK6VKIlaXWSdUfSiQzzbVM3zDZGuL8Lz8Z0o08n96YrKw1SSbAd6+bnwtgefvx313kKStvXTXdLyrXsOpPd6m6gayw7ayuGd/UmNjFTDXVvQfvOXeVKXmmbmiatOV68LYy7+3c0dxhtmkqC7dzTUaHkFpfz9d4Uc4diUrvPZlNaoWvVN0VtSkyEL2k5xZy6UtB0ZeW6rDuShqOtFWO6+5k7FKWVUkmwnevX2YOhIV588ftZSivazyCMuMQM7G00DAnxMnco160qgasu0ZZRWqFlw7HL3N7THwdbdecFpX4qCVqAZ6JDuZJXyg+H2sdNd6WUxCZlMizUu03fVibAzYHuAa7qbvMtZPupLHKLy5ncTrtCFdNQSdACjOjqzS2Brny2/SzadnB17bmsQlKuFhHdhrtCq8RE+HDwwjVyi8rNHUq7szY+DU8nW0Z09TZ3KEorppKgBRBC8ExUV85lFbLx+CVzh3PDqubdbKuDYmqKDvdFq5PsSFZziZpSQWkFW09e4Y5eAdhYqa85pWHq02Ehbu/pT4i3E4vawU1345Iy6OrrTCdPR3OHcsP6dfbA3dFGdYma2JaEy5SUt62b5yrmoZKghbDSCJ6MDOFEeh7b2/CclYWlFew9e7VddIVC5d8lMsyHbUmZ6NpBV3VrsTY+nUB3B/p39jB3KEorp5KgBbmrX0f8Xe1ZGNt2b7q760w2ZVpdm5wqrSHR4b5kF5ZxNC3X3KG0C9kFpew4ncXkvh3QaFr/7bUU81JJ0ILYWmuYNTKYveeucvDCNXOHc11ikzJwsrViYJCnuUMxmcgwH4RAdYmayIZjl9DqpOoKVYyikqCFeWBQZ9wdbdrkTXellMQlZjC8qze21u3no+vhZEu/Tu7qbvMmsjY+nXA/FyL8Xc0ditIGtJ9vEsUoTnbWzBgaxNaTV0i6nG/ucJrldEYB6bklbXLC7KbERPhyLC2XjPwSc4fSpl28WsSBC9fUtYGK0VQStECPDgvCwcaKz9rYTXerjpTa8lRpDalK7HFJ6lKJG/HT0XQAJvdRSVAxjkqCFsjDyZYHBnVm7ZF0Ll4tMnc4RotNyiDC34UANwdzh+wRAf8AACAASURBVGJyPQJc8XO1U12iN2hdfDoDuni0i8tnlJtDJUEL9cSoYDSi7dx0N6+knAPnr7XLrlConNAgOtyXHaezKNfqzB1Om5R4OY/Ey/lqQIzSLCoJWqgANwfu6hfIt/svklVQau5wmrTzdBYVOtmuLo2oLTrCl4LSCvafv2ruUNqkdfHpWGkEE3oFmDsUpQ1RSdCCPRkZSplWx7Kd58wdSpNikzJwsbemf+f2ewPR4V29sbESqkv0OkgpWRufzoiu3ng725k7HKUNUUnQgoX6ODOupz9f7r5AfknrncC56q4Ro7r5YN2O54F0trNmcLBX9dyoivEOpVwjLadYdYUqzdZ+v1EUozwdFUp+SQUrW/FNd0+k55GZX9ouR4XWFh3hS3JGQZsasNQarI1Px85aw9ie/uYORWljVBK0cL07ujOiqzdLfj9HSXnrvOnutlOVR0aRlpAE9duoZo8xXrlWx89HLzGmhx/OdtbmDkdpY1QSVHgmKpTM/FJWH0w1dyj1ik3MoFegG74u9uYOpcWF+DgT5OWo7jbfDDuTs8guLGOKujZQuQ4qCSoMDfWiTyd3FrfCm+7mFJVxKOVau7lrhDGiI3zZfSab4rLWeWTe2qyLT8fV3toiegoU01NJUEEIwdORoaRcLWL/5db1xbv9dBY6CVHt9PrA+kSH+1JaoWPXmbZ7y6ubpbhMy6YTl5nQKwA7aytzh6O0QSoJKgCM7eFHV19nvk0qY0FsMskZ+a3i5rtxiRl4ONrQp2P7vTSitsEhnjjaWqkuUSP8mniFwjKtmitUuW5GJUEhxDghRJIQIlkI8Wo9y4UQ4j/65UeFEP315Z2EELFCiJNCiBNCiD/XaNNXCLFHCBEvhDgghBikL39IX1b10Akh+ppqg5X6aTSCf9zdCw97wT83JTFm/nZi/rWN9zac5MD5q2bpJtXpJHGnMokM88HKgu4LZ2dtxfCu3sQmZraKHyKt2dr4dPxc7Rgc7GXuUJQ2qsmhVEIIK2ABcBuQCuwXQqyTUibUqDYe6KZ/DAYW6f+tAF6SUh4SQrgAB4UQW/Rt3wfekVJuFEJM0L+OklKuBFbq37sXsFZKGW+i7VUaMTDIkzeHOhDRbwhbTl5hS8IVlu48x2fbz+LtbMuY7n6M7enHsFBv7G1avuvpaFouVwvLiGrHs8Q0JDrcly0JVzh1pYBwfxdzh9Mq5RaVE5eUwYyhQRb1I0kxLWPGEw8CkqWUZwGEEN8AU4CaSXAK8KWs/Nm6RwjhLoQIkFJeAi4BSCnzhRAngUB9WwlU3fDLDUiv570fAL5u/mYpN8LfzZ5HhnThkSFdyCspZ1tSJpsTrrD+6CW+2X8RR1srIsN8GNvTj5hwP9wcbVokjtjEDISAUWGWN+AhOqJym2OTMlQSbMDG45co10qm9A00dyhKG2ZMEgwELtZ4nUrlUV5TdQLRJ0AAIUQQ0A/Yqy96HtgkhPiAym7ZYfW89/1UJtg6hBCzgdkAfn5+xMXFGbEp9SsoKLih9u1JffvCBbgnACb72ZKYbcXhDC27T19h4/HLaASEe2jo72tNPz8rvB1Md5r5pwPFhLhqOLp/l8nW2Vzm/Gx0ctHww55TRMiLTVe+CVrb/5P/7ivG31GQdfoQcck3/0iwte0P5foYkwTr+3TVPlHRaB0hhDPwPfC8lDJPX/w08IKU8nshxH3AEmBMjTaDgSIp5fH6gpJSLgYWAwwcOFBGRUUZsSn1i4uL40batydN7Yvb9P/qdJKjablsSbjM5hNXWJlYwMpE6NnBlbE9/Lmthx/dA1wQ4vq+nLIKSjm3aSsvjAkjKqrbda3DFMz52ZhUkshn28/Sb9DwFjvabo7W9P/kcm4JiZt+5c+juxEdHWaWGFrT/lCunzFJMBXoVON1R+p2XTZYRwhhQ2UCXCmlXFOjzgygaqDMd8AXtdY5DdUV2mppNIK+ndzp28mdv9wewbmswuqE+OGvp/j31lN09HCoToi3Bnk0a97P7acykZJ2fdeIpsRE+LIw7gw7kjOZ2FuNfqxp/dF0pFQ3z1VunDFJcD/QTQgRDKRRmZwerFVnHfCs/nzhYCBXSnlJVB4GLAFOSinn12qTDkQCcUAMcLpqgRBCA9wLjGr2FilmEeztxOxRocweVTn7zK/6gTVf7b3A0p3ncHe0YXSEH7f18GNUmDeOto1/9GKTMvF2tqNnB9dG67Vn/Tp74O5ow2+JGSoJ1rI2Pp3eHd0I8XE2dyhKG9dkEpRSVgghngU2AVbAUinlCSHEU/rlnwIbgAlAMlAEPKZvPhx4BDgmhKga4fm6lHID8ATwkRDCGihBf35PbxSQWjUYR2lbfFzsmDaoM9MGdaawtILtpzLZknCFrSev8P2hVOysNYzs5sPYHn6M7u6LV61b31RodWw/lcltPfzQWPCoPyuNYFQ3H7YlZaLTSYveFzWdySzgWFoub9zR3dyhKO2AUbPN6pPWhlpln9Z4LoE59bT7nfrPF1YtG9DAsjhgiDGxKa2bk50143sFML5XAOVaHfvPXWVzwpXqpKgRMLCLJ7f1qLz8oouXE/EXc8gtLrfortAqMRG+rDuSzv9uSiTczwV/V3t8Xe3xd7O32Mmi18WnIwRMUl2higlY5v8ixSxsrDQM6+rNsK7evDWpByfS86oT4rsbTvLuhpOE+7ngbG+NlUYwopu3uUM2u+hwXzq42fPZtrqdIk62Vvi52ePnUpkU/Vzt8XO1M0iUPs522Fq3n4mhpJSsO5LO0BAv/Fzb/4TqSstTSVAxCyEEtwS6cUugGy/eFsbFq0X6hHiZfeeuMryrN24O5h8RaW5ujjbsem00haUVXMkr4XJeCRl5pVzOK+FK9aOUfeeukpFfQrm27gwz3s62+FYnSjt9srTXJ8vKpOnhaNsqulullJRW6Cgp11JcrqW4TEtJuY7ici0l5VouZBdxLquQpyJDzB2q0k6oJKi0Cp08HZk5IpiZI4LJLSrHxtr8X8itiZOdNSE+zo0OBNHpJNeKyriSV1qdIC/rk2TV66OpOWQVlNVpa2Ml8HXRH0m62debNAvLJZn5pZToE1JlYtJVJ6vSisp/a5ZX160uN2xTUqGlpEabkgotTc0U52hrxbieATe6SxUFUElQaYVawzVxbZFGI/BytsPL2Y4ejYyqLavQkVmgT4y5VcmylAx90ky6nM/2U1kUlFbUbfzr1mbFZG+jwcHGCgcbK+z1DwdbK+xtNHg42vxRVlVurcHe9o/6f7TTVNfxd7VXnxHFZFQSVBQLY2utIdDdgUB3h0brFei7YKse+46cpEd4tz8SmXVVQjNMUlXldtaa654sQVFuFpUEFUWpl7OdNc4+zoTqu2A9cpOJGhpk3qAUxcTaz7AxRVEURWkmlQQVRVEUi6WSoKIoimKxVBJUFEVRLJZKgoqiKIrFUklQURRFsVgqCSqKoigWSyVBRVEUxWKpJKgoiqJYLJUEFUVRFIulkqCiKIpisVQSVBRFUSyWSoKKoiiKxVJJUFEURbFYKgkqiqIoFkslQUVRFMViqSSoKIqiWCyVBBVFURSLpZKgoiiKYrGElNLcMdywgQMHygMHDjRZLzc3l6ysLMrKygzKS0pKsLe3b6nw2hS1Lwyp/fEHtS8MtYf9kZaWVubj43PJ3HG0ECmEOF9eXv7+gAEDNjZUyfpmRmROJSUlXLlyhY4dO+Lg4IAQonpZfn4+Li4uZoyu9VD7wpDaH39Q+8JQe9gfWq224pZbbskydxwtQUpJYWGh3/nz5z86ePBg8oABA07XV89iukMzMzPx8fHB0dHRIAEqiqIo7Y8QAmdn5yJ/f3+NtbX1aw3Vs5gkWFJSgrOzs7nDUBRFUW4iV1fXAiFEn4aWW0wSrKiowNraYnp/FUVRFMDGxqZCSunR0HKLSYKA6gZVFEWxMPrv/QZznUUlQUVRFEWpyagkKIQYJ4RIEkIkCyFerWe5EEL8R7/8qBCiv768kxAiVghxUghxQgjx5xpt+goh9ggh4oUQB4QQg2os6y2E2K1vc0wI0bbHISuKoiitUpNJUAhhBSwAxgM9gAeEED1qVRsPdNM/ZgOL9OUVwEtSyu7AEGBOjbbvA+9IKfsCb+pfI4SwBr4CnpJS9gSigPLr3UBLsHz5coQQnD9/3tyhKIqitCnGHAkOApKllGellGXAN8CUWnWmAF/KSnsAdyFEgJTykpTyEICUMh84CQTq20jAVf/cDUjXPx8LHJVSHtG3y5ZSaq9z+xSlTVixYgW9e/fG3t6eLl268NZbb1Febvxvv1OnTjF58mTc3NxwdXVlypQpnDlzxqBOUVERixYtYty4cQQGBuLk5ESvXr149913KS4urrNOrVbLokWL6Nu3Ly4uLoSEhBATE8Mvv/xiUK/qR1hDjyeeeKK67ttvv91o3Xfffddg3enp6cyePZuQkBAcHBwICQnhySef5OLFi3XizcvL49lnn8Xf3x8HBweGDBnCli1bmtx3c+fORQhB165d610upWTZsmUMGjQIJycnPDw8GDZsGFu3bjWoFxUVVe821bfeoKCgeuuOGTOmTt2G9tWsWbPq1L127Rp/+ctfCAsLw8HBgc6dO/Pggw9y8uRJg3oFBQW8/fbbPPXUU3YeHh59hBADXnzxxQ71bf+gQYPChRADGnps3rzZqarukSNH7GbNmtVxwIAB4Q4ODv2EEAPWr19f52JKrVbLxx9/7HXbbbeFdujQoZeDg0O/4ODgnrNnz+6YlZVlVbNuUlKSbWPv36VLl1vqi9tYxgyXDARqfuJSgcFG1AkEqmciEEIEAf2Avfqi54FNQogPqEzGw/TlYVRe6b8J8AG+kVK+XzsoIcRsKo868fPzIy4urtGNcHNzIz8/v95lWq22wWVtQUlJCVD5wb7R7Wjr+8LUbsb+WLFiBXPmzGHs2LE88cQTHDt2jLlz53LhwgU+/vjjJttfunSJESNGYGdnx6uvvoqUkgULFjBy5Eh27tyJt7c3AAkJCcyZM4eRI0fy5JNP4uHhwZ49e3jzzTdZv349GzduxMrqj++fl19+mcWLFzN16lQee+wx8vPzWblyJePHj2flypVMmjQJgP79+7N48eI6cf34449s2LCByMjI6n14++23ExgYWKfuF198wb59+xg5cmR13dzcXIYMGUJxcTEzZ86kc+fOJCUlsXTpUjZs2MDevXurL1aXUjJhwgQOHjzIs88+S8eOHfnqq6+YMGEC69atY8SIEfXuuwsXLvDee+/h5OSETqer92/9wgsvsHz5cu69914eeeQRSkpKSEpKIjU11aC+VqvF19eXefPmGbR3cXGps14pJd27d+eFF14wKPf39683hpEjR/LII48YlIWEhBjUraioIDo6mtOnT/P4448THh5OSkoKS5Ys4eeff2b37t106tSpervfeecdfH19NT179szfuXOnKw147bXXLl26dKlOrnj11Vc7azQaRo0aVVRVtm3bNuelS5f6BQcHl4SFhRUfPXrUqXY7gIKCAs1zzz0X1KtXr8KHHnooKyAgoPzIkSMOy5cv9928ebP74cOHEzw8PHQAAQEBFQsWLDhXex3Hjx93WLRokX9UVFRuQ7EbRUrZ6AO4F/iixutHgI9r1fkZGFHj9a/AgBqvnYGDwN01yv4D3KN/fh+wVf/8ZeAc4A04AruB0Y3FOGDAANmUhISEBpfl5eU12b41W7ZsmQTkuXPnbnhdbX1fmFpL74/i4mLp7e0to6KipE6nqy7/29/+JoUQ8ujRo02u49lnn5U2NjYyKSmpuuzkyZPSyspKvvTSS9VlmZmZ9a5v7ty5EpBr1641iMvW1lbefffd1WV5eXkyOztb2tnZycmTJzcZV69evaSHh4csKSlptF5paan08vKSPXr0MChfsmSJBOS6desMyhcsWCABuWbNmuqy1atXS0AuW7bMYBtCQ0NlY98PkydPlrfddpuMjIyUoaGhdZavWbNGAnLVqlV1ltX+bDS0jvp06dJFjh492qi6gJw5c2aT9X799VcJyP/85z8G5T///LME5Pz586vLSkpKZFpamjx27FhhcnLyEUC+8MIL6VLKA8Y8tm/fngDIhx9+OKNm+eXLlw9nZ2cfklIeWLBgwVlA/vTTT0m12xcXFx/ctGnTydrlVW3mzp2b0lQM06dPvwLIuLi4hKbqxsfHn5cN5A9jukNTgU41Xnfkj67LJusIIWyA74GVUso1NerMAKpef0dlt2vVurZJKbOklEXABqC/EXEqtfz0008MGTIER0dH3N3dmTJlSp1ukcLCQl555RVCQ0Oru+KGDBnC6tWrG6zj5eVVp059zp8/jxCCefPmsWzZMsLDw3FwcGDYsGEcPXoUqDwK6t69O/b29vTv35/9+/fXWc+ZM2d48MEH8fHxwc7OjltuuYXPP//coE5ZWRlvvfUWgwYNwtPTEwcHB/r27cvy5cvrrC8oKIgxY8awf/9+hg8fjoODA506dWL+/PnG7lqTiY2NJSsrizlz5hhcwvPMM88gpWTVqlVNruO7775j7NixhIWFVZdFREQwevRovv322+oyb29vevXqVaf9PffcA1QeKVYpKSmhrKyMgIAAg7oeHh44ODjg6OjYaEzx8fEcO3aM+++/Hzs7u0brbtiwgezsbKZPn25QnpeXB1AnhqrXNWNYtWoV7u7uPPzww9Vl9vb2zJw5k4MHD9bpGq563w0bNvDRRx81GNsHH3zAwIEDuffee5FSUlBQ0Oi2QPN6D8rLy41aJ0BpaSlFRUUNLm/O/rKzs6NDh3p7P42ybNkyL4DHHnssu2a5n5+f1tPTU9dUe3t7ezl27NjC2uXTp0+/BpCQkNDoYMjS0lKxbt06z+Dg4JLIyMiGd4oRjEmC+4FuQohgIYQtMA1YV6vOOmC6fpToECBXSnlJVP6vXgKclFLW/oZJByL1z2OAqnndNgG9hRCO+kEykUACSrN8/fXXTJkyheLiYubNm8fzzz/P77//zrBhw0hOTq6u98wzz/Dvf/+bSZMm8cknn1SfT9i7d2+Ddf72t7/VqdOYNWvW8O677zJr1izefPNNTpw4wfjx41myZAnvvPMOjz32GG+99RZnzpxh6tSpVFRUVLc9deoUgwYNYv/+/bzwwgt8+OGHhISEMHv2bP7xj39U18vLy+PTTz9lyJAhzJ07l/feew93d3cee+yxOgkTICUlhUmTJjFs2DDmz59PaGgoL730Eps2bWpye4qKisjKyjLqodM1/n1w6NAhAAYNGmRQ3qFDBzp27Fi9vCFpaWlcuXKlTvuqdaamppKZmdnoOtLTK3/Tenl5VZe5u7vTp08fli1bxpdffklKSgqJiYnMmjWL8vLyOt14tX355ZcAdRJbQ3U1Gg0PPfSQQXlkZCRCCP70pz+xa9cu0tLS2Lp1K6+99hpDhgxh9OjR1XUPHTpEv3796kyIUbVfau/HkpISnnvuOZ599lm6d+9eb1z5+fns2bOHwYMH8+abb+Lu7o6LiwuBgYF88skn9bZJSUnB2dkZV1dXvLy8+NOf/tRgktu5cyeOjo64uLjQoUMH3nrrLYPPfk3ffPMNjo6OODk5ERISUm83+eDBg7G3t+eNN95g69atpKWlsWvXLp555hlCQ0OZNm1aveturvLyctauXevZpUuX0piYmDqJ7EZcuHDBFsDLy6vRcSCrV692zcnJsb7//vuzG6tnlIYOEWs+gAnAKeAM8Dd92VNUjuAEEFSOID0DHAMG6stHUDkA5igQr39MqLHsIHCEyvOENbtPHwZOAMeB95uK70a7Q8tHjJAyMtLwsWBB5cLCwrrLIiOlrOp2ycysf/k331QuT0mpf3lVF09i4h9l16l2d2hZWZn09/eXXbt2lfn5+dX1jhw5IjUajbz33nury9zd3eUzzzxT/bq+7r/adYx17tw5CUg3NzeZmZlZXb5w4UIJSE9PT5mVlVWnfOPGjdVlY8eOlWFhYbKgoMBg3Q888IB0dHSUOTk5UkopKyoq6u12GzNmjOzatatBWZcuXeq8T0lJifT19ZVTp041qFvf/njrrbek/nPd5KOpLuo5c+ZIQJaVldVZduutt8q+ffs22n7//v0SkIsXL66zrKrbMD4+vsH2Op1OjhkzRjo6OsrLly8bLEtMTJT9+vUz2B5/f3+5b9++RmOqqKio/vw15erVq9LW1rbBrsFPP/1Uuru7G8QwadIkWVhYaFDPyclJPvjgg3XanzhxQgLyww8/NCh/++23pa+vb/Xnp76uzMOHD0tAent7S29vb/nxxx/Lb7/9Vo4dO1YC8v333zeo/9hjj8k333xTrl69Wq5cuVI+/PDDEpDDhg2T5eXlBnUnTpwo33vvPfnDDz/I5cuXyzvuuEMCBv83qwwZMkT++9//lmvXrpWLFy+WQ4cOlYD8y1/+Uqfujz/+KP39/Q3215AhQ2RGRka9+/d6ukO//vrr0/r3T2usXmPdoQ097rvvvkwhhNyzZ8+JxuqNGzfuqhBCnjp16qgx622sO9SoecSklBuo7JasWfZpjecSmFNPu9/1CbK+df4ODGhg2VdUXiahXIeDBw9y+fJl/vWvfxnMl9q7d2/GjRvHxo0b0el0aDQa3N3d2bt3LxcvXqw+aV6bMXUac88991QPzgAYOnQoAHfeeafB0UdV+dmzZ4HKkW5btmzh9ddfp7i42GAE4/jx4/n666/Zs2cPt99+O1ZWVtWDOsrLy8nPz0en0xETE8Prr79Obm4ubm5u1e2DgoIYN25c9Ws7OzuGDBlS/d6NmT59eoMDLWrz9/dvdHlxcTFCCGxsbOoss7e3r+7iaqw9UG+XY9Vtfuob+Vll7ty5bN26lQ8//BA/Pz+DZc7Oztxyyy1ERUUxatQoUlJSWLhwIZMnT2bbtm0G3a81bd68mcuXL/POO+80GjtUHuGUlZU1eMQYEBDAiBEjGDt2LJ07d2bfvn3Mnz+f6dOn891331V3IRcXFxu9D86dO8c//vEPPvnkE4PPRG1VR3BZWVns3LmTYcMqx+7dc8899O3bl/fff58XX3yx+nO3dOlSg/YPPvggYWFhvPnmm/zf//2fwTb+9NNPBnVnzJjBzJkzWbp0Kdu3b2fUqFHVy3bv3m1Qd+bMmdx+++3Mnz+fp59+muDg4Opl3t7eDBo0iGHDhtG9e3dOnDjB+++/z1133cWWLVtwcHBocHuNtWLFCi/96NQbPwqr4ZNPPvFatWqV9+OPP54xePDgBj+0WVlZVr/99pv7oEGD8rt161bWUD2jNZQd29JDDYwxPBL8+uuvJSB//vnnOnVffvllCVT/6v/++++lo6OjFELIPn36yOeee04eOHDAoE3tOi+//HKdOvWpOhJ8++23m1U+b948KaWUe/fubfJI68svv6xuv3z5ctmrVy+p0Wjq1Ltw4UJ1vS5dusioqKg68c6YMUMGBQUZlLX0Z8OcR4LLli2TQgj5+OOP11mWn58vO3fubPA3ysvLkxkZGdLd3V3eeeedDcb0wAMPSCGEPHv2bKOxSynl0KFDpZOTU50jfSkrj2rs7OzkqVOnDMo///xzCcgff/yxuqw5R4ITJ06UAwcOlFqttrqsviPBAwcOSEAGBwfXWe+8efMkIE+cONHo9hUWFkqNRiMffvjhRutJWTmYCZBvvPFGk3U3btwoAfnFF19Ul+3fv1/a2NjIuLg4g7pbtmyp92hYyuYfCWZnZx+ys7PT3XrrrflN1W3OkeCaNWtO2djY6EaMGJFbUlJysLG6//znP88D8qOPPjrX1HqrHjc6MEZpR2Rld3P1L+i7776bc+fO8fnnn9O9e3dWrFjBrbfeynvvvVfdpnadpUuX1qnTmJrD7o0pr4qx6nzac889x5YtW+p9VF1X9e233/Loo4/SuXNnlixZwoYNG9iyZUv1uava5+aaeu/GFBQUcPnyZaMeWm3jl7hWDVqoOi9X06VLl5ocvNBUe6Dedfzwww/MmjWLKVOm1Ht5w/fff09KSgp33XWXQbmPjw8jR45kx44d9caTn5/Pjz/+yIgRIwyOUOqTnJzM7t27ufvuu3FyqjuS/sMPP6RHjx5069bNoPzuu+8GMIghICDAqH0QGxvL+vXrefHFF0lJSeH8+fOcP3+ekpISKioqOH/+PFlZWQZtah8h1yy7du1ao9vo6OiIl5cX2dlNHzR17twZ4LrrLliwAGdnZyIjIw3qjhkzBhcXlwb/Zs2xfPlyz9LSUvHggw+a7B6EW7dudXrooYdCu3fvXvTzzz+fsbOza/Q/4ddff+1lb2+vmzFjRuM730jqtgrtUFBQEACJiYlMmDDBYFliYiLOzs4G3ZO+vr7MnDmTmTNncuXKFaZNm8Zbb73Fyy+/XN1NV7NOUVERd9xxR506phYaGgpUJqz6LiKu6ZtvviE4OJiffvrJYJTlb7/9ZvK4PvjgA6O6+qCy663q71Gf/v0rBz7v37+fLl26VJenp6eTmprKo48+2uj6AwMD8fX1rXdU7d69ewkMDMTHx8egfMuWLTzwwAOMGjWKb775pt4fBFUJpb4kXlFR0eAAjtWrV1NcXGzUgJgVK1YADQ+eSU9Pr/fO7VXvXTOG/v37s3nz5jp3i6kavNWvXz+g8vo4qOyqrE9wcDAzZsxg+fLlBAQE0KFDB1JTU+vUqyqrvW9ry8vLIysrq8l6QPUI1uutm56eXu/fS0qJTqdr8G/WHFUJqGoU543avXu3w913390tMDCwbPPmzaddXV0bHUmWkJBge+jQIefJkydfdXNza3IUqjHUkWA7NHDgQPz9/fn0008pLPxj8Nbx48f55ZdfmDBhAhqNBq1WS26u4XWmjo6OhIeHU15eTmFhoVF1WoqPjw+jR49myZIl1V9eNdUc9ajRVH6Uax7xZWdn1zlPYwrTp09v8Mi09qOpc4LR0dF4eXmxYMECg6PQhQsXAnDvvfca1E9MIcg2qQAAHrRJREFUTCQlJcWgbOrUqWzevJlTp04Z1Pvtt9+47777DOru2rWLO++8k969e7N27doGL18IDw8H4KuvDE/Np6SksH37dgYMqPd0Pl9++SX29vZ14q7PV199RWBgIDExMQ3GcOLECQ4fPlynHWAQw9SpU8nJyTGIt6SkhKVLl9KvX7/qWVtiYmL44Ycf6jx69uyJv78/P/zwA88991z1OqZNm0ZqaiobN240WO+KFSsICgqqPkrNy8ujtLS0zja888471RfyV7l69WqdZKXT6fj73/8OYFC3vpG9ZWVlvPfee9jY2HDbbbcZ7K+8vLw65xt/+OEHCgsLG/ybGSspKcn24MGDzmPGjMkx5jKIphw9etRu4sSJYa6urtotW7ac8vPza3JmsCVLlngBzJgxw2TnI9WRYDtkbW3N/Pnzeeihhxg+fDgzZswgLy+Pjz/+GBcXl+qpqfLz8wkMDOSuu+6iT58+eHp6snfvXpYuXcr48eNxd3cnJyenTp3Dhw/zxRdfVNdpSQsXLmT48OH06dOHWbNmER4eTnZ2NvHx8fz444/Vs+VMmTKFNWvWMHHiRO68804yMjJYvHgxHTp04MqVKyaNKSQkhJCQEJOsy97envfee4/Zs2czadIk7rzzTo4cOcLChQt5/PHH6d27t0H97t27ExkZaTBD0uuvv853333HmDFjeOGFF5BSMn/+fHx9fXnllVeq6124cIE77riDiooKHnroIdauXWuw7tDQ0OrBSZMmTaJ3797Mnz+f1NRUoqOjSU1NZcmSJZSUlPDGG2/U2ZaLFy+ybds27rvvvkYHnAD8/vvvnD17lldeeaX6B0xtr7zyCr/88gsxMTHMmTOHTp06sW/fPpYvX06PHj0MEu0999zDiBEjePrpp0lOTqZTp04sX76c8+fPG0yd1rlz5+quxJo+/PBDSkpKuPPOOw3KX331VVatWsW9997LCy+8gI+PD19++SXnz5/nq6++qu51OHToENOmTWPatGl069aN8vJyNm7cyObNm7n99tsNYl23bh1///vfmTp1KiEhIeTl5bFmzRr27t3LE088weDBf0zItWDBAtasWcPkyZPp3LkzmZmZrFy5kpMnTzJv3jw6duxYXfe5557jv//9L/fddx9PP/00ERERJCQk8Omnn+Lv78+TTz5psG2ffPIJJ0+etNFqtX4Au3btcv7rX/8aADBr1qzssLAwg0EnS5Ys8ZJSNpqAsrOzrf73f//XF+DYsWMOAF9++aXn9u3bnQHef//9SwDXrl3TjBs3LuzatWvWjz32WPqGDRsMplYLCAiouOuuu+qMClu9erWXj49P+ZQpUxofMdYcDZ0sbEsPNTDGcGBMlbVr18pBgwZJe3t76erqKidPnmxwIr+0tFT+9a9/lf3795f/v737jo6y2B8//h4g2SSkQQolkEIxV4qE0Kwh1OMlGKp86R0xEkEUUfSigFcNKFWK5gpyUAQkiBcxiugNx3bgREFQSBB+EEoIxlBDSf/8/kiy5mELC3LvQnZe5+whmZlndnbysJ+nzDPj7+8vnp6e0rx5c5k5c6b50QprZSIjIw1lbKkc6PLKK6/8pfRjx47JuHHjJCQkRNzc3KR+/frStWtXWbp0qaHcggULpGnTpmIymaR58+ayaNEiq31ja7aOUaNGSVhYmCHtf7VvrF69Wlq2bCnu7u7SqFEjmTlzptXBMoB0tvI4TWZmpsTFxYmPj4/4+PjII488IocOHTKUSUtLszvIaNSoUYbyFy5ckBdeeEHuvvtu8fT0FF9fX+nevbvFwItKr732ms0BWdd67LHHHBpYsmfPHunbt680btxY3NzcJCQkRB5//HHDozWVzp8/LwkJCRIcHCweHh7SoUMH+eKLL67bFhH7s71kZWXJoEGDxN/fX0wmk3Tq1ElSU1MN+8aRI0fk0UcflYiICPH09BQPDw9p3bq1JCUlWfwdf/rpJ4mPj5dGjRqJyWSS2rVrS4cOHSQ5Odkwa5CIyJdffik9e/aUBg0aiLu7u/j4+EhMTIykpKRYbevhw4dl+PDhEhERIe7u7hIcHCzDhg2z+qhO5aNC1l7WBrOEh4dfDQwMLCouLrY5ACUzM3OfvX3M0XLWBt5s3749A5CJEyeetvX+tl72BsYocWAgwO2uffv28uOPP9otk5GRYfeh2Mo5CF2d7gsj3R9/0n1hVB3649dff73SqlWrjOuXvLPt3bs3sE2bNuHW8vQ9QU3TNM1l6SCoaZqmuSwdBDVN0zSXpYOgpmma5rJ0ENQ0TdNclg6CmqZpmsvSQVDTNE1zWToIapqmaS5LB0FN0zTNZekgqGmaprksHQQ1TdM0l6WDYDWxevVqlFJkZWU5uylms2bNMqztp2madrvRQVDTboHTp08zfPhwAgIC8Pb2pmvXrvz0008Oby8iLFy4kObNm2MymbjrrrtYsmSJxUr36enpTJ48mdatW+Pt7U3Dhg3p3bs3tiaQ/+233xg8eDCNGzfGy8uLu+66i+eee46zZ8/abc8rr7yCUsq8Dt+1tm/fTpcuXQgKCsLPz4+2bduybNkyq4u6VtW9e3eUUowfP96QfvbsWebPn09sbCz16tXD19eX6OhoVqxYYbXOt99+m8GDB9OkSROUUsTGxlp9v8zMTGbMmEF0dDR+fn4EBwfTtWtXw/JKmmvTQbCaGDFiBFevXjWsTq79b1y+fJkuXbqQmprK008/TVJSEjk5OXTp0oXMzEyH6njppZd4+umnuffee1m2bBkdOnRgypQpvPbaa4Zyc+fOZcOGDcTExLBw4UKeeuopMjIy6NSpE6mpqYayR48epUOHDvzwww8kJCSwePFiunbtyvz58+nRo4dhAeKqsrKyeP3116ldu7bV/JSUFHr27ElBQQEvvfQSSUlJ1K9fn8TERJ555hmbn3HDhg3s3LnTat4PP/zA888/j4+PD88//zzz5s0jLCyMJ554guHDh1uUT0pK4osvvqBp06b4+vrafM93332XFStWEBUVxdy5c5kxYwbnzp2jZ8+evPPOOza301yIrTWW7qSXq68neCtdvHhRysrK5PLly3+5rpdffrlyDbE7liP7xptvvimA/Oc//zGn5ebmir+/vwwYMOC622dnZ4u7u7vFen7Dhg0TT09Pyc3NNad9//33UlhYaCiXl5cnwcHB0rZtW0P6zJkzBZB9+/YZ0p999lkBZPfu3VbbEx8fLz169LBYY6+yLx566CFp2LChFBQUmPPKysqkY8eO4uvra7XO/Px8CQkJkVdffVUAGTdunCH/yJEjVte8GzdunACyd+9eQ3pWVpZ57b2mTZtaXWNRRCQ9Pd3ib1hQUCAtWrSQwMBAKS0ttbqdI6rD98Yvv/xyWW5wbb478WVvPUF9JlhNXHtPMDY21uolImv36SovT3388cc88MADeHh4kJyczL333kvLli2tvt/f//53QkNDbZ5N2LNq1SratGmDh4cHQUFBjBgxgpMnTxrK5ObmMnHiRMLCwjCZTAQFBREbG2tYUd2RMv8LH330ES1btqRLly7mtKCgIAYNGsTWrVu5cuWK3e3//e9/U1RURGJioiF90qRJXL16lS1btpjT7r//ftzd3Q3lAgICiI2N5cCBA4b0ixfLF99u0KCBIb3ydy8vL4u2pKamkpqayuLFi2229+LFi9SpUweTyWROU0pRv359q3UCzJ49G5PJZPNMMSIigvDwcIv0AQMGAFh8trCwMIfuN7dv395izT+TyUTv3r3Jy8sjNzf3unVo1ZsOghpQfjlqwoQJPPzww7z11ltERUUxevRoDhw4YHFv6/fff2f79u0MHz6cGjVubBdKSkpi3Lhx+Pr6Mm/ePMaOHUtKSgoPPPCA4T7Vo48+yvr16xk2bBjLly9n+vTpBAYGsmfPnhsqY0teXp5Dr0uXLtmtp6ysjH379tGxY0eLvI4dO1JYWMj+/fvt1rF7925q1apF27ZtDent2rWjRo0a7N69+7qf59SpUwQEBBjSKg+CxowZw+7duzl58iSffPIJc+fOZeDAgURGRhrKFxQUMHnyZBITE20uQF1Z7/79+5kxYwaHDh3i6NGjLFq0iM8++4wXX3zRovyBAwdYvHgxCxYsMAROR5w6dQrA4rP9VadOnaJWrVp2L6VqrqGWsxug3R4yMjJIT08nMjLSfOQcFRXF1KlTWbNmDe3atTOXXbt2LaWlpYwcOfKG3iMvL49Zs2bx4IMPkpaWRq1a5btfTEwMvXv3JikpiXnz5nHhwgW++eYb5s2bx7PPPmu1LkfK2BMUFORQuaFDh7J27Vqb+WfPnqWgoMDibAv+POOq/CK3JScnh+DgYGrWrGlId3d3JyAg4Lrbf/vtt3z//fdMnTrVkN63b19efvll3njjDbZu3WpOf+yxx1i+fLlFPXPnziU/P59Zs2bZfb9XXnmF7Oxs5s6dS1JSEgBubm4kJyczduxYi/KJiYl06dKFPn362K33WkVFRSxYsICGDRsSExNzQ9vac/jwYVJSUoiPj7d55qq5Dh0EgV4f9bL4AhrUchBPdHiCK8VX6LW2l8U2o6NGMzpqNHlX8hj40UCL/IT2Cfxfq//jxIUTjNg8wiL/mfue4ZHIRziYd5CJWycCsGP0jlvzgW5Cp06daN++Pfn5+eY0f39/+vTpw7p165g/f745aL3//vt06NCBv/3tbzf0Hl999RWFhYVMnTrVXBdAXFwcLVq0YOvWrcybNw9PT0/c3d3ZsWMHY8aMITAw0KIuR8rY4+joQD8/P7v5V69eBbB6huPh4WEoY68OW2dIHh4edrfPyclhyJAhhIaG8tJLL1nkh4WF0a1bN+Lj4wkICODrr7/m7bffpnbt2ixYsMBc7ujRoyQlJbF06dLrfmZ3d3eaNWvGsGHDiIuLo6ysjA8++ICJEyfi7+9P//79zWXXrVvHd999x759++zWaU1CQgIHDhxg8+bNN3wGaculS5cYMGAAJpPJ8Pk116WDoAZA06ZNraaPGTOGDRs2sG3bNuLi4vj111/5+eefWbp06Q2/R+X9SmvBszIIQvmX7Pz585k6dSr169enXbt2PPzwwwwdOtR8Cc+RMvZ0797doTZXPSiwxtPTE4DCwkKLvIKCAkMZe3VY276yDlvbX7hwgV69enHp0iW+/fZbi+C1ZMkSZs+ezaFDh6hbty4A/fr1w9/fn1dffZWRI0cSFRUFwOTJk2nVqhVjxoyx21aAIUOGcO7cOdLS0gxpMTExPPHEE8TFxWEymcjPz2fatGk8+eSTN3zANHv2bFatWsWcOXPo27fvDW1rS1FREQMGDCAzM5PPPvtMj6TWAB0EAUgdlGpx87ySl5uX3TO0QK9Au/mN/RrbzY8MjPyvnAEqpSyeMQNsPsdl64u2R48ehISEsGbNGuLi4lizZg1ubm4MHjz4lrZXRAwDHRITE4mPj2fLli18/fXXLFiwgNdff52VK1cyYsQIh8vYcvr0aYfaVVJSYnPfAKhbty4mk8nqJcucnBwAGjZsaPc9GjRowLZt2ygtLTVckSgqKuLMmTNWt79y5Qq9e/fm4MGDfPnll7Ru3dqizMKFC+ncubM5AFbq378/r776Kt999x1RUVGkpaWxdetWPvzwQ44fP24uV1BQQElJCVlZWXh7e2MymTh27BibN2+2GDijlKJfv34888wzHDp0iFatWvHGG2+Qn5/PyJEjLSZxuHTpEllZWQQGBuLt7W3Ie+utt5g1axaJiYnMnDnTbt85qqysjOHDh/PVV1+xbt06hw+CtOpPD4yppurUqcP58+ct0m90RpkaNWowYsQItmzZwrlz5/jwww+Ji4u7qYEKlaP/rD07l5mZaTE6MDQ0lMTERDZv3szx48dp0qSJxSU/R8pY06BBA4dezz33nN16atSoQZs2bUhPT7fI27VrFyaTiRYtWtitIzo6mpKSEosBPT/++CNlZWVER0cb0ouKiujfvz87d+5k48aNPPjgg1brPXXqlNWDnpKSEsO/x44dA8rvf0ZERJhfu3bt4tixY0RERDBt2jRznWD9YOraeo8fP05+fj5RUVGGeqH8mcGIiAhSUlIMdaxevZopU6YwbNgwlixZYqvLboiIMGHCBDZu3Mjy5csZNGjQLalXqx70mWA11axZMz777DN+//136tWrB0B2djaffPLJDdc1evRokpKSSEhIIDs7m7feeuum2tSjRw9MJhOLFy8mPj7efF/w888/Z//+/UyfPh0oP8tRShnOTuvUqUN4eDi7du1yuIw9t+qeIMDAgQOZPn06O3bsMI/I/OOPP9i4cSO9evUyPHReOeo0NDTUPCgjPj6eKVOmsHTpUlavXm0uu3z5cjw8PHjkkUfMaaWlpQwdOpTt27fz/vvvExcXZ7NdkZGRpKWlkZ2dTUhIiDn9gw8+ADAPduratSubN2+22P4f//gHZ86cYcWKFYSGhgLQvHlzatSowbp165g8ebL5zLWkpIT169fj5eVlvvQ5efJkq5cy+/XrR/fu3Zk0aZIhwG/atInx48cTFxdnfuTnVnj66adZtWoVSUlJTJw48ZbUqVUfOghWU+PHj2f+/Pn07NmTCRMmcP78eVasWEFkZOQNTecF5V+m9913Hxs2bCAgIMDuF689AQEBzJo1ixkzZtCtWzcGDhxIdnY2S5YsITQ01HzW9dtvv9GlSxcGDhxIixYt8Pb25ptvvmHbtm0kJCQ4XMaeW3VPEMoHcPzrX/+if//+TJs2DT8/P5YtW0ZxcTH//Oc/DWWXLl3K7NmzSUtLMwfMkJAQpk2bxuuvv05ZWRmdO3cmLS2NtWvXMmfOHIKDg83bT5s2jU2bNplnfKkMaJX69etnDrovvvgigwcPplOnTiQkJJgHxqSkpNCtWzceeughoPxsujLIVbVo0SIKCgrMgSw/P5/AwEAmTpzIihUruP/++xkyZAgiwocffsiePXuYM2eOeUBQdHS0xVlspbCwMEOATE9PZ+jQofj4+NCnTx/Wr19vKH/PPfdwzz33mH//9NNP2bt3LwDnzp2jtLTU3NcxMTHm0aSLFy9m0aJFREVFERISYtFfPXr0MB8kai7K1lP0VV/Aw8BB4DDwvJV8BSypyN8HRFekNwbSgAxgPzClyjZRwE7gZ+BHoGNFejhwtSL9Z+Dt67VPzxgj8t577wlgmHVjw4YN0rx5c3Fzc5O7775b1q9fb3UWF6rM4GGrL9555x0BZNKkSQ63ydaMMStXrpTWrVuLu7u7BAQEyLBhw+TEiRPm/Ly8PHnyySelZcuW4uPjI7Vr15bWrVvL/Pnzpbi42OEyt4Kj+0Z2drYMGTJE6tSpI15eXhIbGyvp6ekW5Sr7JC0tzZBeWloqb775pjRt2lTc3d2lWbNmsnDhQvOsKJU6d+4sgM3XtbOupKWlSY8ePaR+/fri5uYmTZo0keeee86hGYFszRhTUlIi7777rrRv3178/f3Fw8ND2rZtK8nJyQ71FVZmjKncf229Xn75ZUP5UaNGOVTWXjlrf4cbUR2+N/SMMYISK4MnqlJK1QR+A3oAJ4F0YIiIHKhSphfwJNAL6AQsFpFOSqkGQAMR2a2U8gF+AvqKyAGl1JfAQhH5vGL76SISq5QKB7aKSCu7Dauiffv2YmsC4UoZGRk2HwDOz8+3O/jhTrBy5UrGjx/PiRMnaNSo0U3XY6sv3nvvPcaOHcuuXbusPhheXVWHfeNW0X1hVB3649dff73SqlWrDGe3479t7969gW3atAm3lufIwJiOwGEROSIiRcB64NqnXvsAayoOLnYC/kqpBiKSIyK7AUQkn/IzwsqbEwJUTtfgB9h/Iliz69SpUyilLEYC3irJycm0aNHCpQKgpmnVnyP3BEOAE1V+P0n52d71yoQAOZUJFWd4bYHKUQtPAduUUm9SHozvr7J9hFJqD3AR+IeIfHtto5RSjwGPAdSrV++680X6+fnZvL9TWlrq0L2f21F2djaffvopy5cvp2PHjn/5s1Td/vLly3z++efs3LmTnTt3smzZsju2n27Wnbxv3Gq6L4x0f1QPjgRBa0O0rr2GareMUsob2AQ8JSIXK5ITgKkiskkpNQhYCXSnPHCGisgZpVQ74BOlVMsq25VXLpIMJEP55VBb64lVysjIsHnp4k6+rJGZmcns2bPp2LEjycnJf/lzVO2LM2fOMHbsWPz8/Jg8eTKPP/74Dc8Veqe7k/eNW033hZHuj+rBkSB4kvIBLpUaYXnp0mYZpZQb5QFwrYh8XKXMKGBKxc8bgXcBRKQQKKz4+Sel1P8D7qJ88Ix2jb59+153kuebFR4ebvWBe03TtOrCkcP6dKC5UipCKeUODAa2XFNmCzBSlbsXuCAiOar8QZ+VQIaIXDtR3ymgc8XPXYFDAEqpoIrBOCilmgDNgSM38dk0TdM0za7rngmKSIlSKhHYBtQEVonIfqXU4xX5bwOplI8MPQxcASonIHwAGAH8opT6uSLtBRFJBSYAi5VStYACKu7vATHAHKVUCVAKPC4if66xo2mapmm3iEMPy1cErdRr0t6u8rMAk6xs9x3W7xdW5rWzkr6J8sunt5xcMz+lpmmaVr2VlZUpwObq39VixpiDZw4SuzrWkHbtUkhTI6ciOUJN9/JpngK8Agj0CqS4tJjjV45Tq9DYFUG1g6jrWZeikiKOnj9q8Z71vOvh7+FPQXEBxy4cs8hv4NMAX5MvV4qvcOLCCYv8EN8QvN29uVR0ieyL2Rb5jf0a4+XmxcXCi+Tk51jkh/mF4eHmwfmC8/x+6XeL/Aj/CNxruXP26ln+uPyHRX6TOk1wq+lG3pU8zlw5Y04vKS2hVmEtmtVtRs0aNcm9nMu5q+csto8MLF+p4fSl01wouGDIU0pxV8BdAJzKP0V+oXEEXc0aNWlWtxkAJy+e5HLRZUO+W003mtRpAsDxC8e5WmxcSshUy0S4fzgAWeezKCwxrsDg6eZJqF/5DChHzh2huLTYkF/bvTaNfMufpTx89jClZcZ5MH1MPjT0KZ+0+uSVk9QoNN418PPwo753fQAO5h3kWnU86xBcO5jSslIOnz1skV913ztyzvJK/+2671XuG/+tfa/SnbLvnS44zalC4/CIW7nv/XbmN4t78v/tfa86unr1qodSyuaM+S4z1C81O5XcnFxKi0r1YA9N07RqrqysTF2+fNkzKyvLvaSkZLatctedMeZO4MiMMQAXL14kNzeX4mLj0VlBQYF5vkNXp/vCSPfHn3RfGFWH/sjOzi4KCgqyvNRUPZQppU6XlJTMjo6O3marULW4HOooX19ffH19LdJ37NhB27ZtndCi24/uCyPdH3/SfWFUHfqjRYsWv4hIe2e3w5lc5nKopmmapl1LB0FN0zTNZekgqGmaprksHQQ1TdM0l6WDoKZpmuaydBDUNE3TXJYOgpqmaZrLqhYPyyul/gAs549yXCCQd4uac6fTfWGk++NPui+MqkN/hIlIkLMb4UzVIgj+VUqpH139gdFKui+MdH/8SfeFke6P6kFfDtU0TdNclg6CmqZpmsvSQbBcsrMbcBvRfWGk++NPui+MdH9UA/qeoKZpmuay9Jmgpmma5rJ0ENQ0TdNclksHQaXUw0qpg0qpw0qp553dHmdSSjVWSqUppTKUUvuVUlOc3SZnU0rVVErtUUptdXZbnE0p5a+USlFKZVbsI/c5u03OpJSaWvH/5Fel1Dql1J29uq4Lc9kgqJSqCSwD/g60AIYopVo4t1VOVQI8IyJ3A/cCk1y8PwCmABnObsRtYjHwhYj8DWiDC/eLUioEmAy0F5FWQE1gsHNbpd0slw2CQEfgsIgcEZEiYD3Qx8ltchoRyRGR3RU/51P+JRfi3FY5j1KqERAHvOvstjibUsoXiAFWAohIkYicd26rnK4W4KmUqgV4Aaec3B7tJrlyEAwBTlT5/SQu/KVflVIqHGgL7HJuS5xqETAdKHN2Q24DTYA/gPcqLg+/q5Sq7exGOYuIZANvAseBHOCCiHzp3FZpN8uVg6Cykubyz4sopbyBTcBTInLR2e1xBqVUbyBXRH5ydltuE7WAaGCFiLQFLgMuew9dKVWH8qtGEUBDoLZSarhzW6XdLFcOgieBxlV+b4SLX9JQSrlRHgDXisjHzm6PEz0AxCulsii/TN5VKfWBc5vkVCeBkyJSeWUghfKg6Kq6A0dF5A8RKQY+Bu53cpu0m+TKQTAdaK6UilBKuVN+Y3uLk9vkNEopRfk9nwwRWeDs9jiTiMwQkUYiEk75fvEfEXHZI30ROQ2cUEpFViR1Aw44sUnOdhy4VynlVfH/phsuPFDoTlfL2Q1wFhEpUUolAtsoH921SkT2O7lZzvQAMAL4RSn1c0XaCyKS6sQ2abePJ4G1FQeMR4AxTm6P04jILqVUCrCb8lHVe9BTqN2x9LRpmqZpmsty5cuhmqZpmovTQVDTNE1zWToIapqmaS5LB0FN0zTNZekgqGmaprksHQQ1TdM0l6WDoKZpmuay/j+C0Uj32PUqSgAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_preds = []\nfor batch in tqdm(test_loader):\n    x, y = batch\n    foo = jury.predict(x, plot = False, sigmoid = True)\n    list_of_preds.append(foo)\n\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub_cp = submission\nsub_cp.to_csv('./submission_cp.csv', index=None, header=True)\n\nimport csv \na = list_of_preds  \nwith open('./submission_cp.csv', \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerows(a)\n\nfinal_sub = pd.read_csv('./submission_cp.csv', header = None)\n\nfinal_sub.columns = submission.columns[1:]\nfinal_sub[\"sig_id\"] = submission[\"sig_id\"]\n\ngood_cols = np.roll(final_sub.columns.values, 1)\nfinal_sub = final_sub[good_cols]","execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3982.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb7621b88abe4449bc09816f5b001af4"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.head()","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0  id_0004d9e33                     0.015194                0.015917   \n1  id_001897cda                     0.015002                0.015721   \n2  id_002429b5b                     0.014672                0.014762   \n3  id_00276f245                     0.015084                0.015793   \n4  id_0027f1083                     0.013877                0.014205   \n\n   acat_inhibitor  acetylcholine_receptor_agonist  \\\n0        0.015485                        0.018907   \n1        0.015557                        0.018712   \n2        0.015905                        0.015030   \n3        0.015470                        0.018872   \n4        0.015334                        0.012764   \n\n   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                           0.023239                        0.016816   \n1                           0.021169                        0.016743   \n2                           0.016099                        0.014116   \n3                           0.022525                        0.016786   \n4                           0.014661                        0.012300   \n\n   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                    0.016174                       0.016310   \n1                    0.015852                       0.016036   \n2                    0.014874                       0.016651   \n3                    0.016031                       0.016189   \n4                    0.013814                       0.017005   \n\n   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n0                    0.015095  ...                               0.014700   \n1                    0.015021  ...                               0.014707   \n2                    0.015205  ...                               0.015259   \n3                    0.015033  ...                               0.014672   \n4                    0.015093  ...                               0.015173   \n\n   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n0      0.015237         0.015754           0.015884   \n1      0.015497         0.015638           0.022640   \n2      0.014747         0.014536           0.011562   \n3      0.015272         0.015658           0.017571   \n4      0.013788         0.014368           0.005579   \n\n   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n0                   0.016530                               0.015190   \n1                   0.016745                               0.015194   \n2                   0.014111                               0.014804   \n3                   0.016538                               0.015153   \n4                   0.012934                               0.014348   \n\n   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n0         0.017941   0.015520                    0.015286       0.015403  \n1         0.018651   0.015458                    0.015498       0.015068  \n2         0.013534   0.015534                    0.015041       0.017019  \n3         0.018136   0.015435                    0.015333       0.015271  \n4         0.011609   0.015236                    0.014245       0.017491  \n\n[5 rows x 207 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0004d9e33</td>\n      <td>0.015194</td>\n      <td>0.015917</td>\n      <td>0.015485</td>\n      <td>0.018907</td>\n      <td>0.023239</td>\n      <td>0.016816</td>\n      <td>0.016174</td>\n      <td>0.016310</td>\n      <td>0.015095</td>\n      <td>...</td>\n      <td>0.014700</td>\n      <td>0.015237</td>\n      <td>0.015754</td>\n      <td>0.015884</td>\n      <td>0.016530</td>\n      <td>0.015190</td>\n      <td>0.017941</td>\n      <td>0.015520</td>\n      <td>0.015286</td>\n      <td>0.015403</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_001897cda</td>\n      <td>0.015002</td>\n      <td>0.015721</td>\n      <td>0.015557</td>\n      <td>0.018712</td>\n      <td>0.021169</td>\n      <td>0.016743</td>\n      <td>0.015852</td>\n      <td>0.016036</td>\n      <td>0.015021</td>\n      <td>...</td>\n      <td>0.014707</td>\n      <td>0.015497</td>\n      <td>0.015638</td>\n      <td>0.022640</td>\n      <td>0.016745</td>\n      <td>0.015194</td>\n      <td>0.018651</td>\n      <td>0.015458</td>\n      <td>0.015498</td>\n      <td>0.015068</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_002429b5b</td>\n      <td>0.014672</td>\n      <td>0.014762</td>\n      <td>0.015905</td>\n      <td>0.015030</td>\n      <td>0.016099</td>\n      <td>0.014116</td>\n      <td>0.014874</td>\n      <td>0.016651</td>\n      <td>0.015205</td>\n      <td>...</td>\n      <td>0.015259</td>\n      <td>0.014747</td>\n      <td>0.014536</td>\n      <td>0.011562</td>\n      <td>0.014111</td>\n      <td>0.014804</td>\n      <td>0.013534</td>\n      <td>0.015534</td>\n      <td>0.015041</td>\n      <td>0.017019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00276f245</td>\n      <td>0.015084</td>\n      <td>0.015793</td>\n      <td>0.015470</td>\n      <td>0.018872</td>\n      <td>0.022525</td>\n      <td>0.016786</td>\n      <td>0.016031</td>\n      <td>0.016189</td>\n      <td>0.015033</td>\n      <td>...</td>\n      <td>0.014672</td>\n      <td>0.015272</td>\n      <td>0.015658</td>\n      <td>0.017571</td>\n      <td>0.016538</td>\n      <td>0.015153</td>\n      <td>0.018136</td>\n      <td>0.015435</td>\n      <td>0.015333</td>\n      <td>0.015271</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0027f1083</td>\n      <td>0.013877</td>\n      <td>0.014205</td>\n      <td>0.015334</td>\n      <td>0.012764</td>\n      <td>0.014661</td>\n      <td>0.012300</td>\n      <td>0.013814</td>\n      <td>0.017005</td>\n      <td>0.015093</td>\n      <td>...</td>\n      <td>0.015173</td>\n      <td>0.013788</td>\n      <td>0.014368</td>\n      <td>0.005579</td>\n      <td>0.012934</td>\n      <td>0.014348</td>\n      <td>0.011609</td>\n      <td>0.015236</td>\n      <td>0.014245</td>\n      <td>0.017491</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 207 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [col for col in final_sub.columns]\nfinal_sub.loc[test_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\nfinal_sub.to_csv('submission.csv', index=False)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub[:30]","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"          sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0   id_0004d9e33                     0.015194                0.015917   \n1   id_001897cda                     0.015002                0.015721   \n2   id_002429b5b                     0.000000                0.000000   \n3   id_00276f245                     0.015084                0.015793   \n4   id_0027f1083                     0.013877                0.014205   \n5   id_0042c1364                     0.000000                0.000000   \n6   id_006fc47b8                     0.015002                0.015152   \n7   id_0071d65a2                     0.015272                0.016028   \n8   id_007a2159c                     0.014890                0.015484   \n9   id_009201382                     0.015225                0.015877   \n10  id_00a3f939a                     0.015106                0.015773   \n11  id_00c23789a                     0.015100                0.015798   \n12  id_00c8748de                     0.015231                0.015748   \n13  id_00dc49d16                     0.015245                0.015853   \n14  id_00ef20e0f                     0.014565                0.014647   \n15  id_00f7bf922                     0.015029                0.015492   \n16  id_0100497d9                     0.015111                0.015835   \n17  id_010b2717f                     0.015140                0.015626   \n18  id_0129cad3a                     0.015024                0.015625   \n19  id_0140840fb                     0.015292                0.016029   \n20  id_01412d166                     0.015168                0.015901   \n21  id_01501dd3d                     0.015244                0.016016   \n22  id_016adc400                     0.015153                0.015859   \n23  id_018c5b9b7                     0.015323                0.016074   \n24  id_01be13119                     0.014838                0.015506   \n25  id_01c7e0f02                     0.015162                0.015832   \n26  id_01d10f19e                     0.000000                0.000000   \n27  id_01d7e2151                     0.014499                0.014649   \n28  id_01dcdc16b                     0.015120                0.015822   \n29  id_01e7d3c23                     0.015178                0.015905   \n\n    acat_inhibitor  acetylcholine_receptor_agonist  \\\n0         0.015485                        0.018907   \n1         0.015557                        0.018712   \n2         0.000000                        0.000000   \n3         0.015470                        0.018872   \n4         0.015334                        0.012764   \n5         0.000000                        0.000000   \n6         0.015684                        0.018674   \n7         0.015501                        0.018908   \n8         0.015550                        0.017871   \n9         0.015459                        0.018976   \n10        0.015368                        0.018747   \n11        0.015453                        0.018857   \n12        0.015247                        0.017843   \n13        0.015313                        0.018762   \n14        0.016125                        0.014861   \n15        0.015613                        0.018766   \n16        0.015359                        0.018793   \n17        0.015250                        0.018278   \n18        0.015416                        0.018806   \n19        0.015498                        0.018920   \n20        0.015377                        0.018771   \n21        0.015592                        0.018976   \n22        0.015341                        0.018747   \n23        0.015799                        0.019271   \n24        0.015489                        0.018388   \n25        0.015460                        0.018931   \n26        0.000000                        0.000000   \n27        0.015883                        0.015587   \n28        0.015386                        0.018800   \n29        0.015459                        0.018861   \n\n    acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                            0.023239                        0.016816   \n1                            0.021169                        0.016743   \n2                            0.000000                        0.000000   \n3                            0.022525                        0.016786   \n4                            0.014661                        0.012300   \n5                            0.000000                        0.000000   \n6                            0.019522                        0.016676   \n7                            0.023884                        0.016868   \n8                            0.018365                        0.016310   \n9                            0.023321                        0.016869   \n10                           0.023287                        0.016687   \n11                           0.022694                        0.016770   \n12                           0.023900                        0.016430   \n13                           0.023873                        0.016739   \n14                           0.015004                        0.014250   \n15                           0.021260                        0.016541   \n16                           0.023419                        0.016698   \n17                           0.023184                        0.016429   \n18                           0.022675                        0.016789   \n19                           0.023984                        0.016865   \n20                           0.023729                        0.016733   \n21                           0.022919                        0.016917   \n22                           0.023770                        0.016680   \n23                           0.022406                        0.017097   \n24                           0.019437                        0.016481   \n25                           0.023049                        0.016860   \n26                           0.000000                        0.000000   \n27                           0.015206                        0.014109   \n28                           0.023250                        0.016692   \n29                           0.023361                        0.016815   \n\n    adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                     0.016174                       0.016310   \n1                     0.015852                       0.016036   \n2                     0.000000                       0.000000   \n3                     0.016031                       0.016189   \n4                     0.013814                       0.017005   \n5                     0.000000                       0.000000   \n6                     0.015706                       0.015916   \n7                     0.016300                       0.016448   \n8                     0.015484                       0.015504   \n9                     0.016055                       0.016414   \n10                    0.016036                       0.016230   \n11                    0.016058                       0.016207   \n12                    0.015636                       0.015906   \n13                    0.015980                       0.016372   \n14                    0.015080                       0.017649   \n15                    0.015852                       0.016405   \n16                    0.016095                       0.016250   \n17                    0.015754                       0.015950   \n18                    0.015799                       0.016219   \n19                    0.016299                       0.016449   \n20                    0.016167                       0.016308   \n21                    0.016252                       0.016345   \n22                    0.016128                       0.016264   \n23                    0.016291                       0.016422   \n24                    0.015601                       0.015607   \n25                    0.016013                       0.016359   \n26                    0.000000                       0.000000   \n27                    0.015302                       0.016941   \n28                    0.016090                       0.016223   \n29                    0.016150                       0.016331   \n\n    adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n0                     0.015095  ...                               0.014700   \n1                     0.015021  ...                               0.014707   \n2                     0.000000  ...                               0.000000   \n3                     0.015033  ...                               0.014672   \n4                     0.015093  ...                               0.015173   \n5                     0.000000  ...                               0.000000   \n6                     0.015584  ...                               0.015209   \n7                     0.015158  ...                               0.014722   \n8                     0.014888  ...                               0.014653   \n9                     0.015079  ...                               0.014716   \n10                    0.014933  ...                               0.014554   \n11                    0.015022  ...                               0.014657   \n12                    0.014752  ...                               0.014368   \n13                    0.014984  ...                               0.014606   \n14                    0.017139  ...                               0.016325   \n15                    0.015019  ...                               0.014881   \n16                    0.014992  ...                               0.014574   \n17                    0.014728  ...                               0.014398   \n18                    0.014942  ...                               0.014654   \n19                    0.015146  ...                               0.014718   \n20                    0.015025  ...                               0.014601   \n21                    0.015220  ...                               0.014817   \n22                    0.014975  ...                               0.014553   \n23                    0.015340  ...                               0.014980   \n24                    0.014896  ...                               0.014636   \n25                    0.015048  ...                               0.014685   \n26                    0.000000  ...                               0.000000   \n27                    0.016528  ...                               0.016109   \n28                    0.014988  ...                               0.014593   \n29                    0.015071  ...                               0.014670   \n\n    trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n0       0.015237         0.015754           0.015884   \n1       0.015497         0.015638           0.022640   \n2       0.000000         0.000000           0.000000   \n3       0.015272         0.015658           0.017571   \n4       0.013788         0.014368           0.005579   \n5       0.000000         0.000000           0.000000   \n6       0.015316         0.015109           0.022034   \n7       0.015192         0.015785           0.014462   \n8       0.015644         0.015595           0.032035   \n9       0.015296         0.015857           0.016099   \n10      0.015079         0.015626           0.015377   \n11      0.015247         0.015663           0.017014   \n12      0.014866         0.016033           0.014369   \n13      0.015136         0.015837           0.014831   \n14      0.014683         0.013989           0.009384   \n15      0.015602         0.015856           0.020429   \n16      0.015056         0.015621           0.014688   \n17      0.014924         0.015699           0.015500   \n18      0.015202         0.015647           0.017182   \n19      0.015181         0.015807           0.014314   \n20      0.015064         0.015652           0.014395   \n21      0.015442         0.015834           0.017542   \n22      0.015006         0.015614           0.014181   \n23      0.015687         0.015976           0.019730   \n24      0.015548         0.015600           0.028153   \n25      0.015297         0.015792           0.016697   \n26      0.000000         0.000000           0.000000   \n27      0.014719         0.014141           0.010063   \n28      0.015113         0.015640           0.015442   \n29      0.015178         0.015706           0.015395   \n\n    tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n0                    0.016530                               0.015190   \n1                    0.016745                               0.015194   \n2                    0.000000                               0.000000   \n3                    0.016538                               0.015153   \n4                    0.012934                               0.014348   \n5                    0.000000                               0.000000   \n6                    0.016000                               0.015200   \n7                    0.016506                               0.015227   \n8                    0.016865                               0.015099   \n9                    0.016488                               0.015210   \n10                   0.016296                               0.015032   \n11                   0.016488                               0.015142   \n12                   0.016114                               0.015092   \n13                   0.016358                               0.015146   \n14                   0.013744                               0.014954   \n15                   0.016312                               0.015212   \n16                   0.016382                               0.015072   \n17                   0.016137                               0.014946   \n18                   0.016219                               0.015047   \n19                   0.016482                               0.015217   \n20                   0.016374                               0.015103   \n21                   0.016780                               0.015323   \n22                   0.016320                               0.015059   \n23                   0.017008                               0.015483   \n24                   0.016778                               0.015078   \n25                   0.016485                               0.015180   \n26                   0.000000                               0.000000   \n27                   0.014077                               0.014965   \n28                   0.016396                               0.015085   \n29                   0.016459                               0.015162   \n\n    vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n0          0.017941   0.015520                    0.015286       0.015403  \n1          0.018651   0.015458                    0.015498       0.015068  \n2          0.000000   0.000000                    0.000000       0.000000  \n3          0.018136   0.015435                    0.015333       0.015271  \n4          0.011609   0.015236                    0.014245       0.017491  \n5          0.000000   0.000000                    0.000000       0.000000  \n6          0.017166   0.015478                    0.015788       0.015937  \n7          0.017782   0.015563                    0.015259       0.015478  \n8          0.019093   0.015324                    0.015447       0.014756  \n9          0.017851   0.015524                    0.015278       0.015463  \n10         0.017536   0.015385                    0.015153       0.015275  \n11         0.018035   0.015432                    0.015289       0.015294  \n12         0.016938   0.015784                    0.015379       0.015201  \n13         0.017466   0.015489                    0.015179       0.015405  \n14         0.013005   0.015423                    0.015577       0.018880  \n15         0.017457   0.015500                    0.015438       0.015842  \n16         0.017720   0.015437                    0.015136       0.015283  \n17         0.017102   0.015496                    0.015228       0.015083  \n18         0.017483   0.015330                    0.015332       0.015292  \n19         0.017677   0.015570                    0.015242       0.015495  \n20         0.017605   0.015436                    0.015149       0.015338  \n21         0.018382   0.015655                    0.015432       0.015465  \n22         0.017478   0.015416                    0.015117       0.015288  \n23         0.018847   0.015785                    0.015679       0.015562  \n24         0.019169   0.015329                    0.015327       0.015030  \n25         0.017947   0.015489                    0.015293       0.015409  \n26         0.000000   0.000000                    0.000000       0.000000  \n27         0.013879   0.015098                    0.015208       0.019092  \n28         0.017732   0.015430                    0.015185       0.015289  \n29         0.017823   0.015476                    0.015254       0.015370  \n\n[30 rows x 207 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0004d9e33</td>\n      <td>0.015194</td>\n      <td>0.015917</td>\n      <td>0.015485</td>\n      <td>0.018907</td>\n      <td>0.023239</td>\n      <td>0.016816</td>\n      <td>0.016174</td>\n      <td>0.016310</td>\n      <td>0.015095</td>\n      <td>...</td>\n      <td>0.014700</td>\n      <td>0.015237</td>\n      <td>0.015754</td>\n      <td>0.015884</td>\n      <td>0.016530</td>\n      <td>0.015190</td>\n      <td>0.017941</td>\n      <td>0.015520</td>\n      <td>0.015286</td>\n      <td>0.015403</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_001897cda</td>\n      <td>0.015002</td>\n      <td>0.015721</td>\n      <td>0.015557</td>\n      <td>0.018712</td>\n      <td>0.021169</td>\n      <td>0.016743</td>\n      <td>0.015852</td>\n      <td>0.016036</td>\n      <td>0.015021</td>\n      <td>...</td>\n      <td>0.014707</td>\n      <td>0.015497</td>\n      <td>0.015638</td>\n      <td>0.022640</td>\n      <td>0.016745</td>\n      <td>0.015194</td>\n      <td>0.018651</td>\n      <td>0.015458</td>\n      <td>0.015498</td>\n      <td>0.015068</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_002429b5b</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00276f245</td>\n      <td>0.015084</td>\n      <td>0.015793</td>\n      <td>0.015470</td>\n      <td>0.018872</td>\n      <td>0.022525</td>\n      <td>0.016786</td>\n      <td>0.016031</td>\n      <td>0.016189</td>\n      <td>0.015033</td>\n      <td>...</td>\n      <td>0.014672</td>\n      <td>0.015272</td>\n      <td>0.015658</td>\n      <td>0.017571</td>\n      <td>0.016538</td>\n      <td>0.015153</td>\n      <td>0.018136</td>\n      <td>0.015435</td>\n      <td>0.015333</td>\n      <td>0.015271</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0027f1083</td>\n      <td>0.013877</td>\n      <td>0.014205</td>\n      <td>0.015334</td>\n      <td>0.012764</td>\n      <td>0.014661</td>\n      <td>0.012300</td>\n      <td>0.013814</td>\n      <td>0.017005</td>\n      <td>0.015093</td>\n      <td>...</td>\n      <td>0.015173</td>\n      <td>0.013788</td>\n      <td>0.014368</td>\n      <td>0.005579</td>\n      <td>0.012934</td>\n      <td>0.014348</td>\n      <td>0.011609</td>\n      <td>0.015236</td>\n      <td>0.014245</td>\n      <td>0.017491</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_0042c1364</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>id_006fc47b8</td>\n      <td>0.015002</td>\n      <td>0.015152</td>\n      <td>0.015684</td>\n      <td>0.018674</td>\n      <td>0.019522</td>\n      <td>0.016676</td>\n      <td>0.015706</td>\n      <td>0.015916</td>\n      <td>0.015584</td>\n      <td>...</td>\n      <td>0.015209</td>\n      <td>0.015316</td>\n      <td>0.015109</td>\n      <td>0.022034</td>\n      <td>0.016000</td>\n      <td>0.015200</td>\n      <td>0.017166</td>\n      <td>0.015478</td>\n      <td>0.015788</td>\n      <td>0.015937</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>id_0071d65a2</td>\n      <td>0.015272</td>\n      <td>0.016028</td>\n      <td>0.015501</td>\n      <td>0.018908</td>\n      <td>0.023884</td>\n      <td>0.016868</td>\n      <td>0.016300</td>\n      <td>0.016448</td>\n      <td>0.015158</td>\n      <td>...</td>\n      <td>0.014722</td>\n      <td>0.015192</td>\n      <td>0.015785</td>\n      <td>0.014462</td>\n      <td>0.016506</td>\n      <td>0.015227</td>\n      <td>0.017782</td>\n      <td>0.015563</td>\n      <td>0.015259</td>\n      <td>0.015478</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>id_007a2159c</td>\n      <td>0.014890</td>\n      <td>0.015484</td>\n      <td>0.015550</td>\n      <td>0.017871</td>\n      <td>0.018365</td>\n      <td>0.016310</td>\n      <td>0.015484</td>\n      <td>0.015504</td>\n      <td>0.014888</td>\n      <td>...</td>\n      <td>0.014653</td>\n      <td>0.015644</td>\n      <td>0.015595</td>\n      <td>0.032035</td>\n      <td>0.016865</td>\n      <td>0.015099</td>\n      <td>0.019093</td>\n      <td>0.015324</td>\n      <td>0.015447</td>\n      <td>0.014756</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>id_009201382</td>\n      <td>0.015225</td>\n      <td>0.015877</td>\n      <td>0.015459</td>\n      <td>0.018976</td>\n      <td>0.023321</td>\n      <td>0.016869</td>\n      <td>0.016055</td>\n      <td>0.016414</td>\n      <td>0.015079</td>\n      <td>...</td>\n      <td>0.014716</td>\n      <td>0.015296</td>\n      <td>0.015857</td>\n      <td>0.016099</td>\n      <td>0.016488</td>\n      <td>0.015210</td>\n      <td>0.017851</td>\n      <td>0.015524</td>\n      <td>0.015278</td>\n      <td>0.015463</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>id_00a3f939a</td>\n      <td>0.015106</td>\n      <td>0.015773</td>\n      <td>0.015368</td>\n      <td>0.018747</td>\n      <td>0.023287</td>\n      <td>0.016687</td>\n      <td>0.016036</td>\n      <td>0.016230</td>\n      <td>0.014933</td>\n      <td>...</td>\n      <td>0.014554</td>\n      <td>0.015079</td>\n      <td>0.015626</td>\n      <td>0.015377</td>\n      <td>0.016296</td>\n      <td>0.015032</td>\n      <td>0.017536</td>\n      <td>0.015385</td>\n      <td>0.015153</td>\n      <td>0.015275</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>id_00c23789a</td>\n      <td>0.015100</td>\n      <td>0.015798</td>\n      <td>0.015453</td>\n      <td>0.018857</td>\n      <td>0.022694</td>\n      <td>0.016770</td>\n      <td>0.016058</td>\n      <td>0.016207</td>\n      <td>0.015022</td>\n      <td>...</td>\n      <td>0.014657</td>\n      <td>0.015247</td>\n      <td>0.015663</td>\n      <td>0.017014</td>\n      <td>0.016488</td>\n      <td>0.015142</td>\n      <td>0.018035</td>\n      <td>0.015432</td>\n      <td>0.015289</td>\n      <td>0.015294</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>id_00c8748de</td>\n      <td>0.015231</td>\n      <td>0.015748</td>\n      <td>0.015247</td>\n      <td>0.017843</td>\n      <td>0.023900</td>\n      <td>0.016430</td>\n      <td>0.015636</td>\n      <td>0.015906</td>\n      <td>0.014752</td>\n      <td>...</td>\n      <td>0.014368</td>\n      <td>0.014866</td>\n      <td>0.016033</td>\n      <td>0.014369</td>\n      <td>0.016114</td>\n      <td>0.015092</td>\n      <td>0.016938</td>\n      <td>0.015784</td>\n      <td>0.015379</td>\n      <td>0.015201</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>id_00dc49d16</td>\n      <td>0.015245</td>\n      <td>0.015853</td>\n      <td>0.015313</td>\n      <td>0.018762</td>\n      <td>0.023873</td>\n      <td>0.016739</td>\n      <td>0.015980</td>\n      <td>0.016372</td>\n      <td>0.014984</td>\n      <td>...</td>\n      <td>0.014606</td>\n      <td>0.015136</td>\n      <td>0.015837</td>\n      <td>0.014831</td>\n      <td>0.016358</td>\n      <td>0.015146</td>\n      <td>0.017466</td>\n      <td>0.015489</td>\n      <td>0.015179</td>\n      <td>0.015405</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>id_00ef20e0f</td>\n      <td>0.014565</td>\n      <td>0.014647</td>\n      <td>0.016125</td>\n      <td>0.014861</td>\n      <td>0.015004</td>\n      <td>0.014250</td>\n      <td>0.015080</td>\n      <td>0.017649</td>\n      <td>0.017139</td>\n      <td>...</td>\n      <td>0.016325</td>\n      <td>0.014683</td>\n      <td>0.013989</td>\n      <td>0.009384</td>\n      <td>0.013744</td>\n      <td>0.014954</td>\n      <td>0.013005</td>\n      <td>0.015423</td>\n      <td>0.015577</td>\n      <td>0.018880</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>id_00f7bf922</td>\n      <td>0.015029</td>\n      <td>0.015492</td>\n      <td>0.015613</td>\n      <td>0.018766</td>\n      <td>0.021260</td>\n      <td>0.016541</td>\n      <td>0.015852</td>\n      <td>0.016405</td>\n      <td>0.015019</td>\n      <td>...</td>\n      <td>0.014881</td>\n      <td>0.015602</td>\n      <td>0.015856</td>\n      <td>0.020429</td>\n      <td>0.016312</td>\n      <td>0.015212</td>\n      <td>0.017457</td>\n      <td>0.015500</td>\n      <td>0.015438</td>\n      <td>0.015842</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>id_0100497d9</td>\n      <td>0.015111</td>\n      <td>0.015835</td>\n      <td>0.015359</td>\n      <td>0.018793</td>\n      <td>0.023419</td>\n      <td>0.016698</td>\n      <td>0.016095</td>\n      <td>0.016250</td>\n      <td>0.014992</td>\n      <td>...</td>\n      <td>0.014574</td>\n      <td>0.015056</td>\n      <td>0.015621</td>\n      <td>0.014688</td>\n      <td>0.016382</td>\n      <td>0.015072</td>\n      <td>0.017720</td>\n      <td>0.015437</td>\n      <td>0.015136</td>\n      <td>0.015283</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>id_010b2717f</td>\n      <td>0.015140</td>\n      <td>0.015626</td>\n      <td>0.015250</td>\n      <td>0.018278</td>\n      <td>0.023184</td>\n      <td>0.016429</td>\n      <td>0.015754</td>\n      <td>0.015950</td>\n      <td>0.014728</td>\n      <td>...</td>\n      <td>0.014398</td>\n      <td>0.014924</td>\n      <td>0.015699</td>\n      <td>0.015500</td>\n      <td>0.016137</td>\n      <td>0.014946</td>\n      <td>0.017102</td>\n      <td>0.015496</td>\n      <td>0.015228</td>\n      <td>0.015083</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>id_0129cad3a</td>\n      <td>0.015024</td>\n      <td>0.015625</td>\n      <td>0.015416</td>\n      <td>0.018806</td>\n      <td>0.022675</td>\n      <td>0.016789</td>\n      <td>0.015799</td>\n      <td>0.016219</td>\n      <td>0.014942</td>\n      <td>...</td>\n      <td>0.014654</td>\n      <td>0.015202</td>\n      <td>0.015647</td>\n      <td>0.017182</td>\n      <td>0.016219</td>\n      <td>0.015047</td>\n      <td>0.017483</td>\n      <td>0.015330</td>\n      <td>0.015332</td>\n      <td>0.015292</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>id_0140840fb</td>\n      <td>0.015292</td>\n      <td>0.016029</td>\n      <td>0.015498</td>\n      <td>0.018920</td>\n      <td>0.023984</td>\n      <td>0.016865</td>\n      <td>0.016299</td>\n      <td>0.016449</td>\n      <td>0.015146</td>\n      <td>...</td>\n      <td>0.014718</td>\n      <td>0.015181</td>\n      <td>0.015807</td>\n      <td>0.014314</td>\n      <td>0.016482</td>\n      <td>0.015217</td>\n      <td>0.017677</td>\n      <td>0.015570</td>\n      <td>0.015242</td>\n      <td>0.015495</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>id_01412d166</td>\n      <td>0.015168</td>\n      <td>0.015901</td>\n      <td>0.015377</td>\n      <td>0.018771</td>\n      <td>0.023729</td>\n      <td>0.016733</td>\n      <td>0.016167</td>\n      <td>0.016308</td>\n      <td>0.015025</td>\n      <td>...</td>\n      <td>0.014601</td>\n      <td>0.015064</td>\n      <td>0.015652</td>\n      <td>0.014395</td>\n      <td>0.016374</td>\n      <td>0.015103</td>\n      <td>0.017605</td>\n      <td>0.015436</td>\n      <td>0.015149</td>\n      <td>0.015338</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>id_01501dd3d</td>\n      <td>0.015244</td>\n      <td>0.016016</td>\n      <td>0.015592</td>\n      <td>0.018976</td>\n      <td>0.022919</td>\n      <td>0.016917</td>\n      <td>0.016252</td>\n      <td>0.016345</td>\n      <td>0.015220</td>\n      <td>...</td>\n      <td>0.014817</td>\n      <td>0.015442</td>\n      <td>0.015834</td>\n      <td>0.017542</td>\n      <td>0.016780</td>\n      <td>0.015323</td>\n      <td>0.018382</td>\n      <td>0.015655</td>\n      <td>0.015432</td>\n      <td>0.015465</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>id_016adc400</td>\n      <td>0.015153</td>\n      <td>0.015859</td>\n      <td>0.015341</td>\n      <td>0.018747</td>\n      <td>0.023770</td>\n      <td>0.016680</td>\n      <td>0.016128</td>\n      <td>0.016264</td>\n      <td>0.014975</td>\n      <td>...</td>\n      <td>0.014553</td>\n      <td>0.015006</td>\n      <td>0.015614</td>\n      <td>0.014181</td>\n      <td>0.016320</td>\n      <td>0.015059</td>\n      <td>0.017478</td>\n      <td>0.015416</td>\n      <td>0.015117</td>\n      <td>0.015288</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>id_018c5b9b7</td>\n      <td>0.015323</td>\n      <td>0.016074</td>\n      <td>0.015799</td>\n      <td>0.019271</td>\n      <td>0.022406</td>\n      <td>0.017097</td>\n      <td>0.016291</td>\n      <td>0.016422</td>\n      <td>0.015340</td>\n      <td>...</td>\n      <td>0.014980</td>\n      <td>0.015687</td>\n      <td>0.015976</td>\n      <td>0.019730</td>\n      <td>0.017008</td>\n      <td>0.015483</td>\n      <td>0.018847</td>\n      <td>0.015785</td>\n      <td>0.015679</td>\n      <td>0.015562</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>id_01be13119</td>\n      <td>0.014838</td>\n      <td>0.015506</td>\n      <td>0.015489</td>\n      <td>0.018388</td>\n      <td>0.019437</td>\n      <td>0.016481</td>\n      <td>0.015601</td>\n      <td>0.015607</td>\n      <td>0.014896</td>\n      <td>...</td>\n      <td>0.014636</td>\n      <td>0.015548</td>\n      <td>0.015600</td>\n      <td>0.028153</td>\n      <td>0.016778</td>\n      <td>0.015078</td>\n      <td>0.019169</td>\n      <td>0.015329</td>\n      <td>0.015327</td>\n      <td>0.015030</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>id_01c7e0f02</td>\n      <td>0.015162</td>\n      <td>0.015832</td>\n      <td>0.015460</td>\n      <td>0.018931</td>\n      <td>0.023049</td>\n      <td>0.016860</td>\n      <td>0.016013</td>\n      <td>0.016359</td>\n      <td>0.015048</td>\n      <td>...</td>\n      <td>0.014685</td>\n      <td>0.015297</td>\n      <td>0.015792</td>\n      <td>0.016697</td>\n      <td>0.016485</td>\n      <td>0.015180</td>\n      <td>0.017947</td>\n      <td>0.015489</td>\n      <td>0.015293</td>\n      <td>0.015409</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>id_01d10f19e</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>id_01d7e2151</td>\n      <td>0.014499</td>\n      <td>0.014649</td>\n      <td>0.015883</td>\n      <td>0.015587</td>\n      <td>0.015206</td>\n      <td>0.014109</td>\n      <td>0.015302</td>\n      <td>0.016941</td>\n      <td>0.016528</td>\n      <td>...</td>\n      <td>0.016109</td>\n      <td>0.014719</td>\n      <td>0.014141</td>\n      <td>0.010063</td>\n      <td>0.014077</td>\n      <td>0.014965</td>\n      <td>0.013879</td>\n      <td>0.015098</td>\n      <td>0.015208</td>\n      <td>0.019092</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>id_01dcdc16b</td>\n      <td>0.015120</td>\n      <td>0.015822</td>\n      <td>0.015386</td>\n      <td>0.018800</td>\n      <td>0.023250</td>\n      <td>0.016692</td>\n      <td>0.016090</td>\n      <td>0.016223</td>\n      <td>0.014988</td>\n      <td>...</td>\n      <td>0.014593</td>\n      <td>0.015113</td>\n      <td>0.015640</td>\n      <td>0.015442</td>\n      <td>0.016396</td>\n      <td>0.015085</td>\n      <td>0.017732</td>\n      <td>0.015430</td>\n      <td>0.015185</td>\n      <td>0.015289</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>id_01e7d3c23</td>\n      <td>0.015178</td>\n      <td>0.015905</td>\n      <td>0.015459</td>\n      <td>0.018861</td>\n      <td>0.023361</td>\n      <td>0.016815</td>\n      <td>0.016150</td>\n      <td>0.016331</td>\n      <td>0.015071</td>\n      <td>...</td>\n      <td>0.014670</td>\n      <td>0.015178</td>\n      <td>0.015706</td>\n      <td>0.015395</td>\n      <td>0.016459</td>\n      <td>0.015162</td>\n      <td>0.017823</td>\n      <td>0.015476</td>\n      <td>0.015254</td>\n      <td>0.015370</td>\n    </tr>\n  </tbody>\n</table>\n<p>30 rows × 207 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}