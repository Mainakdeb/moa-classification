{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d   ## smoother\nfrom tqdm.notebook import tqdm\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.preprocessing import MinMaxScaler\n        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\nplt.rcParams['figure.figsize'] = 15, 7\n\n\n\ndef seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\nprint(\"done importing\")\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/test_features.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\ndone importing\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'\ndevice","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"'cuda'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]\n\ndrop_list =[]\nfor i in range(875):\n    if i not in top_features:\n        drop_list.append(i)\n        \nprint(len(drop_list), \"features to be dropped\")","execution_count":4,"outputs":[{"output_type":"stream","text":"90 features to be dropped\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features=train_features.drop(train_features.columns[drop_list], axis=1)\ntrain_features.head()\n","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"  cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3     g-5     g-7  \\\n0  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208 -1.0120 -0.0326   \n1  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604  0.5207  0.3372   \n2  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764  1.2390  0.2155   \n3  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288 -0.8095  0.1792   \n4  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919 -0.8244 -0.1498   \n\n     g-10  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n0  1.1830  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n1 -1.1520  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899  0.1522   \n2 -0.4797  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174 -0.6417   \n3 -0.8269  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880 -1.6210   \n4 -0.2219  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031  0.1094   \n\n     c-97    c-98    c-99  \n0  0.2139  0.3801  0.4176  \n1  0.1241  0.6077  0.7371  \n2 -0.2187 -1.4080  0.6931  \n3 -0.8784 -0.3876 -0.8154  \n4  0.2885 -0.3786  0.7125  \n\n[5 rows x 786 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-5</th>\n      <th>g-7</th>\n      <th>g-10</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-1.0120</td>\n      <td>-0.0326</td>\n      <td>1.1830</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>0.5207</td>\n      <td>0.3372</td>\n      <td>-1.1520</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>1.2390</td>\n      <td>0.2155</td>\n      <td>-0.4797</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>-0.8095</td>\n      <td>0.1792</td>\n      <td>-0.8269</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>-0.8244</td>\n      <td>-0.1498</td>\n      <td>-0.2219</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 786 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore_columns = ['sig_id', \"cp_type\"]\n\ntrain_columns = [x for x in train_features.columns if x not in ignore_columns]\n\ntrain = train_features[train_columns]\ntest = test_features[train_columns]\ntarget = train_targets_scored.iloc[:,1:].values","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = ColumnTransformer([\n                            ('o',OneHotEncoder(),[0,1]),\n                            ('s',Normalizer(),list(range(3,train.shape[1])))\n                        ])\n\n\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)\ntest.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(3982, 787)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, train,targets, noise ):\n        \n        self.features  = train\n        self.targets = targets\n        self.noise = noise\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float() \n        \n#         if self.noise == True:\n# #             print(\"noisy boi\")\n#             feature  = feature + torch.randn_like(feature)/200\n            \n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature,target","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef train_step(x, y, model, optimizer, criterion):\n    optimizer.zero_grad()\n    pred = model(x.to(device))\n    y = y.float()\n    loss = criterion(pred,y.to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('Linear') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        \n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(787)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(787, 1048))\n        \n        self.batch_norm2 = nn.BatchNorm1d(1048)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(1048, 2096))\n        \n        self.batch_norm3 = nn.BatchNorm1d(2096)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(2096, 2096))\n        \n        self.batch_norm4 = nn.BatchNorm1d(2096)\n        self.dropout4 = nn.Dropout(0.5)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(2096, 1048))\n        \n        self.batch_norm5 = nn.BatchNorm1d(1048)\n        #self.dropout5 = nn.Dropout(0.5)\n        self.dense5 = nn.utils.weight_norm(nn.Linear(1048, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n        \n        x = self.batch_norm5(x)\n        #x = self.dropout5(x)\n        x = (self.dense5(x))\n        \n        return x\n    ","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n    \n    losses = []\n    val_losses = []\n    learning_rates = []    \n    best_loss = 1000000\n\n    for epoch in range(num_epochs):\n   \n        if train == True:\n            model.train()\n            losses_temp = []\n            for batch in train_loader:\n                (x_batch, y_batch) = batch\n                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n                losses_temp.append(loss)\n            losses.append(torch.mean(torch.tensor(losses_temp)))\n            scheduler.step(1.)   ## lr decay caller \n            learning_rates.append(get_lr(optimizer))\n            \n\n        if validate == True:\n            with torch.no_grad():\n                model.eval()\n                val_losses_temp = []\n                for x_val, y_val in val_loader:\n                    yhat =model(x_val.to(device))  # pred \n                    val_loss = criterion(yhat.to(device), y_val.to(device))\n                    val_losses_temp.append(val_loss.item())  ## metrics \n                val_losses.append(torch.mean(torch.tensor(val_losses_temp)).item())  ## metrics \n\n        if train == True:\n            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n\n            if val_losses[-1] <= best_loss:\n\n                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n                best_loss = val_losses[-1]\n\n                name = \"./model_\" + str(fold_number)+\".pth\"\n\n                print(\"saving model as: \", name)\n\n                torch.save(model.state_dict(), name)\n\n            else: \n                print(\"showing no improvements, best loss yet:\", best_loss)\n\n        if show_plots == True:\n\n            show_lr(learning_rates)\n            plt.plot(val_losses, label = \"val\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n            plt.plot(losses, label = \"train\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n        \n    return losses, val_losses","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_counts = np.sum(target, axis = 1)\nprint(np.unique(target_counts, return_counts = True))\ntarget_counts[target_counts == 7] = 5\nprint(np.unique(target_counts, return_counts = True))","execution_count":13,"outputs":[{"output_type":"stream","text":"(array([0, 1, 2, 3, 4, 5, 7]), array([ 9367, 12532,  1538,   303,    55,    13,     6]))\n(array([0, 1, 2, 3, 4, 5]), array([ 9367, 12532,  1538,   303,    55,    19]))\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building folds \n\nCGREEN  = '\\33[32m'\nCBLUE =  '\\033[34m'\nCRED = '\\033[1;31m'\nCEND  = '\\33[0m'\n\n\nNFOLDS =10\nnum_epochs = 30\n\n\nkfold = StratifiedKFold(NFOLDS,shuffle=True,random_state=42)\nfold_train_losses = list()\nfold_valid_losses = list()\n\n\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(train,target_counts)):\n\n    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n\n    input_size = x_train.shape[1]\n    output_size = target.shape[1]\n    \n    \n    train_dataset = TrainDataset(x_train, y_train, noise = False)\n    valid_dataset = TrainDataset(x_valid, y_valid, noise = False)\n    \n    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers = 8)\n\n    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle = True, num_workers = 8)\n    \n    model = Model()\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.4e-3)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     mode='min', \n                                                     factor=0.5, \n                                                     patience=5, \n                                                     eps=1e-5, \n                                                     verbose=True)\n    criterion = nn.BCEWithLogitsLoss()\n    print(CRED ,\"fold \", str(k+1), CEND)\n\n    train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = k+1)\n    \nprint(CBLUE, \"Training complete\", CEND)","execution_count":14,"outputs":[{"output_type":"stream","text":"\u001b[1;31m fold  1 \u001b[0m\nepoch  2  out of  30      >\u001b[32m Val loss decreased from: 0.027822013944387436  to  0.020573485642671585 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  3  out of  30      >\u001b[32m Val loss decreased from: 0.020573485642671585  to  0.018965452909469604 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  4  out of  30      >\u001b[32m Val loss decreased from: 0.018965452909469604  to  0.017822932451963425 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  5  out of  30      >\u001b[32m Val loss decreased from: 0.017822932451963425  to  0.017340239137411118 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  6  out of  30      >\u001b[32m Val loss decreased from: 0.017340239137411118  to  0.016833476722240448 \u001b[0m   >saving model as:  ./model_1.pth\nEpoch     7: reducing learning rate of group 0 to 2.0000e-04.\nepoch  7  out of  30      >\u001b[32m Val loss decreased from: 0.016833476722240448  to  0.01662253588438034 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  8  out of  30      >\u001b[32m Val loss decreased from: 0.01662253588438034  to  0.016296688467264175 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  9  out of  30      >\u001b[32m Val loss decreased from: 0.016296688467264175  to  0.016215352341532707 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  10  out of  30      >showing no improvements, best loss yet: 0.016215352341532707\nepoch  11  out of  30      >\u001b[32m Val loss decreased from: 0.016215352341532707  to  0.015726005658507347 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  12  out of  30      >showing no improvements, best loss yet: 0.015726005658507347\nEpoch    13: reducing learning rate of group 0 to 1.0000e-04.\nepoch  13  out of  30      >showing no improvements, best loss yet: 0.015726005658507347\nepoch  14  out of  30      >showing no improvements, best loss yet: 0.015726005658507347\nepoch  15  out of  30      >\u001b[32m Val loss decreased from: 0.015726005658507347  to  0.01538520585745573 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  16  out of  30      >\u001b[32m Val loss decreased from: 0.01538520585745573  to  0.015369278378784657 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  17  out of  30      >showing no improvements, best loss yet: 0.015369278378784657\nepoch  18  out of  30      >showing no improvements, best loss yet: 0.015369278378784657\nEpoch    19: reducing learning rate of group 0 to 5.0000e-05.\nepoch  19  out of  30      >showing no improvements, best loss yet: 0.015369278378784657\nepoch  20  out of  30      >\u001b[32m Val loss decreased from: 0.015369278378784657  to  0.015259196981787682 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  21  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  22  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  23  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  24  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nEpoch    25: reducing learning rate of group 0 to 2.5000e-05.\nepoch  25  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  26  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  27  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  28  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  29  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\nepoch  30  out of  30      >showing no improvements, best loss yet: 0.015259196981787682\n\u001b[1;31m fold  2 \u001b[0m\nepoch  1  out of  30      >\u001b[32m Val loss decreased from: 1000000  to  0.02799588441848755 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  2  out of  30      >\u001b[32m Val loss decreased from: 0.02799588441848755  to  0.02079864591360092 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  3  out of  30      >\u001b[32m Val loss decreased from: 0.02079864591360092  to  0.019335126504302025 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  4  out of  30      >\u001b[32m Val loss decreased from: 0.019335126504302025  to  0.01785159856081009 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  5  out of  30      >\u001b[32m Val loss decreased from: 0.01785159856081009  to  0.01735578663647175 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  6  out of  30      >\u001b[32m Val loss decreased from: 0.01735578663647175  to  0.01690846122801304 \u001b[0m   >saving model as:  ./model_2.pth\nEpoch     7: reducing learning rate of group 0 to 2.0000e-04.\nepoch  7  out of  30      >\u001b[32m Val loss decreased from: 0.01690846122801304  to  0.016713758930563927 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  8  out of  30      >\u001b[32m Val loss decreased from: 0.016713758930563927  to  0.016223585233092308 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  9  out of  30      >\u001b[32m Val loss decreased from: 0.016223585233092308  to  0.015681659802794456 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  10  out of  30      >showing no improvements, best loss yet: 0.015681659802794456\nepoch  11  out of  30      >showing no improvements, best loss yet: 0.015681659802794456\nepoch  12  out of  30      >showing no improvements, best loss yet: 0.015681659802794456\nEpoch    13: reducing learning rate of group 0 to 1.0000e-04.\nepoch  13  out of  30      >showing no improvements, best loss yet: 0.015681659802794456\nepoch  14  out of  30      >\u001b[32m Val loss decreased from: 0.015681659802794456  to  0.015610625967383385 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  15  out of  30      >\u001b[32m Val loss decreased from: 0.015610625967383385  to  0.015466565266251564 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  16  out of  30      >showing no improvements, best loss yet: 0.015466565266251564\nepoch  17  out of  30      >\u001b[32m Val loss decreased from: 0.015466565266251564  to  0.015465142205357552 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  18  out of  30      >showing no improvements, best loss yet: 0.015465142205357552\nEpoch    19: reducing learning rate of group 0 to 5.0000e-05.\nepoch  19  out of  30      >showing no improvements, best loss yet: 0.015465142205357552\nepoch  20  out of  30      >\u001b[32m Val loss decreased from: 0.015465142205357552  to  0.01543066930025816 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  21  out of  30      >\u001b[32m Val loss decreased from: 0.01543066930025816  to  0.015264046378433704 \u001b[0m   >saving model as:  ./model_2.pth\nepoch  22  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  23  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  24  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nEpoch    25: reducing learning rate of group 0 to 2.5000e-05.\nepoch  25  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  26  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  27  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  28  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  29  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\nepoch  30  out of  30      >showing no improvements, best loss yet: 0.015264046378433704\n\u001b[1;31m fold  3 \u001b[0m\nepoch  1  out of  30      >\u001b[32m Val loss decreased from: 1000000  to  0.02792004868388176 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  2  out of  30      >\u001b[32m Val loss decreased from: 0.02792004868388176  to  0.020791970193386078 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  3  out of  30      >\u001b[32m Val loss decreased from: 0.020791970193386078  to  0.019262325018644333 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  4  out of  30      >\u001b[32m Val loss decreased from: 0.019262325018644333  to  0.017978256568312645 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  5  out of  30      >\u001b[32m Val loss decreased from: 0.017978256568312645  to  0.017808791249990463 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  6  out of  30      >\u001b[32m Val loss decreased from: 0.017808791249990463  to  0.017230603843927383 \u001b[0m   >saving model as:  ./model_3.pth\nEpoch     7: reducing learning rate of group 0 to 2.0000e-04.\nepoch  7  out of  30      >\u001b[32m Val loss decreased from: 0.017230603843927383  to  0.016813017427921295 \u001b[0m   >saving model as:  ./model_3.pth\n","name":"stdout"},{"output_type":"stream","text":"epoch  8  out of  30      >\u001b[32m Val loss decreased from: 0.016813017427921295  to  0.01644838973879814 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  9  out of  30      >\u001b[32m Val loss decreased from: 0.01644838973879814  to  0.0161411352455616 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  10  out of  30      >\u001b[32m Val loss decreased from: 0.0161411352455616  to  0.015890488401055336 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  11  out of  30      >showing no improvements, best loss yet: 0.015890488401055336\nepoch  12  out of  30      >\u001b[32m Val loss decreased from: 0.015890488401055336  to  0.015879439190030098 \u001b[0m   >saving model as:  ./model_3.pth\nEpoch    13: reducing learning rate of group 0 to 1.0000e-04.\nepoch  13  out of  30      >\u001b[32m Val loss decreased from: 0.015879439190030098  to  0.015739746391773224 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  14  out of  30      >showing no improvements, best loss yet: 0.015739746391773224\nepoch  15  out of  30      >\u001b[32m Val loss decreased from: 0.015739746391773224  to  0.015361905097961426 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  16  out of  30      >showing no improvements, best loss yet: 0.015361905097961426\nepoch  17  out of  30      >showing no improvements, best loss yet: 0.015361905097961426\nepoch  18  out of  30      >showing no improvements, best loss yet: 0.015361905097961426\nEpoch    19: reducing learning rate of group 0 to 5.0000e-05.\nepoch  19  out of  30      >showing no improvements, best loss yet: 0.015361905097961426\nepoch  20  out of  30      >showing no improvements, best loss yet: 0.015361905097961426\nepoch  21  out of  30      >\u001b[32m Val loss decreased from: 0.015361905097961426  to  0.015335681848227978 \u001b[0m   >saving model as:  ./model_3.pth\nepoch  22  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  23  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  24  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nEpoch    25: reducing learning rate of group 0 to 2.5000e-05.\nepoch  25  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  26  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  27  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  28  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  29  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\nepoch  30  out of  30      >showing no improvements, best loss yet: 0.015335681848227978\n\u001b[1;31m fold  4 \u001b[0m\nepoch  1  out of  30      >\u001b[32m Val loss decreased from: 1000000  to  0.028292233124375343 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  2  out of  30      >\u001b[32m Val loss decreased from: 0.028292233124375343  to  0.02101985178887844 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  3  out of  30      >\u001b[32m Val loss decreased from: 0.02101985178887844  to  0.019413892179727554 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  4  out of  30      >\u001b[32m Val loss decreased from: 0.019413892179727554  to  0.017911000177264214 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  5  out of  30      >\u001b[32m Val loss decreased from: 0.017911000177264214  to  0.017511168494820595 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  6  out of  30      >\u001b[32m Val loss decreased from: 0.017511168494820595  to  0.01705225743353367 \u001b[0m   >saving model as:  ./model_4.pth\nEpoch     7: reducing learning rate of group 0 to 2.0000e-04.\nepoch  7  out of  30      >showing no improvements, best loss yet: 0.01705225743353367\nepoch  8  out of  30      >\u001b[32m Val loss decreased from: 0.01705225743353367  to  0.016156144440174103 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  9  out of  30      >\u001b[32m Val loss decreased from: 0.016156144440174103  to  0.01609862968325615 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  10  out of  30      >\u001b[32m Val loss decreased from: 0.01609862968325615  to  0.015936346724629402 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  11  out of  30      >\u001b[32m Val loss decreased from: 0.015936346724629402  to  0.0156683512032032 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  12  out of  30      >showing no improvements, best loss yet: 0.0156683512032032\nEpoch    13: reducing learning rate of group 0 to 1.0000e-04.\nepoch  13  out of  30      >showing no improvements, best loss yet: 0.0156683512032032\nepoch  14  out of  30      >\u001b[32m Val loss decreased from: 0.0156683512032032  to  0.015335175208747387 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  15  out of  30      >showing no improvements, best loss yet: 0.015335175208747387\nepoch  16  out of  30      >\u001b[32m Val loss decreased from: 0.015335175208747387  to  0.015183672308921814 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  17  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  18  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nEpoch    19: reducing learning rate of group 0 to 5.0000e-05.\nepoch  19  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  20  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  21  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  22  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  23  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nepoch  24  out of  30      >showing no improvements, best loss yet: 0.015183672308921814\nEpoch    25: reducing learning rate of group 0 to 2.5000e-05.\nepoch  25  out of  30      >\u001b[32m Val loss decreased from: 0.015183672308921814  to  0.015091109089553356 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  26  out of  30      >\u001b[32m Val loss decreased from: 0.015091109089553356  to  0.015052000992000103 \u001b[0m   >saving model as:  ./model_4.pth\nepoch  27  out of  30      >showing no improvements, best loss yet: 0.015052000992000103\nepoch  28  out of  30      >showing no improvements, best loss yet: 0.015052000992000103\nepoch  29  out of  30      >showing no improvements, best loss yet: 0.015052000992000103\nepoch  30  out of  30      >showing no improvements, best loss yet: 0.015052000992000103\n\u001b[1;31m fold  5 \u001b[0m\nepoch  1  out of  30      >\u001b[32m Val loss decreased from: 1000000  to  0.027988657355308533 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  2  out of  30      >\u001b[32m Val loss decreased from: 0.027988657355308533  to  0.02077152580022812 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  3  out of  30      >\u001b[32m Val loss decreased from: 0.02077152580022812  to  0.019427239894866943 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  4  out of  30      >\u001b[32m Val loss decreased from: 0.019427239894866943  to  0.0182151161134243 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  5  out of  30      >\u001b[32m Val loss decreased from: 0.0182151161134243  to  0.017763447016477585 \u001b[0m   >saving model as:  ./model_5.pth\nepoch  6  out of  30      >\u001b[32m Val loss decreased from: 0.017763447016477585  to  0.017128445208072662 \u001b[0m   >saving model as:  ./model_5.pth\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-b1b24aa8b5ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCRED\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"fold \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrain_one_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBLUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-9b46123c8abf>\u001b[0m in \u001b[0;36mtrain_one_fold\u001b[0;34m(model, num_epochs, train_loader, val_loader, optimizer, scheduler, criterion, fold_number, show_plots, train, validate)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mlosses_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-90560f624f30>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [Model() for i in range (4)]\n\nfor i in range (len(all_models)):\n    \n    name = \"./model_\" + str(i + 1) + \".pth\"\n    all_models[i].load_state_dict(torch.load(name))\n    all_models[i].to(device)\n    print(\"Loaded: \", name)\n\n","execution_count":15,"outputs":[{"output_type":"stream","text":"Loaded:  ./model_1.pth\nLoaded:  ./model_2.pth\nLoaded:  ./model_3.pth\nLoaded:  ./model_4.pth\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_val_losses = []\nfor i in tqdm(range(4)):\n    losses, val_losses = train_one_fold(all_models[i],5 , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 0, train = False, validate = True)\n    all_val_losses.append(np.mean(np.array(val_losses)))\nall_val_losses = np.array(all_val_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class model_jury(object):   ## only works for dataloaders for batch size 1 \n    def __init__(self, all_models):\n        self.all_models = all_models\n        \n    def predict(self, x, plot = False, sigmoid = False):\n        \n        with torch.no_grad():\n            \n            if sigmoid == False:\n                preds = [self.all_models[i](x.to(device)).view(-1).cpu().tolist() for i in range(len(self.all_models))]\n            else:\n                preds = [self.all_models[i](x.to(device)).view(-1).cpu().sigmoid().tolist() for i in range(len(self.all_models))]\n\n        if plot == True:\n            for pred in preds:\n                plt.plot(pred)\n            plt.show()\n            \n        preds = np.array(preds)\n        mean = np.mean(preds, axis = 0)\n        return mean.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jury = model_jury(all_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TrainDataset(test, target, noise = False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers = 8)\nval_loader_test_jury = DataLoader(dataset= valid_dataset, batch_size=1, shuffle=False, num_workers = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"benchmarking...\")\nwith torch.no_grad():\n    benchmark_losses = []\n    criterion = nn.BCEWithLogitsLoss()\n    for batch in tqdm(val_loader_test_jury):\n        x, y = batch\n        pred = jury.predict(x, plot = False, sigmoid = False)\n        pred = torch.tensor(pred).view(1,-1)\n\n        benchmark_losses.append(criterion(pred, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_val_losses)\nplt.axhline(y = all_val_losses.mean(), label = \"loss mean = \" + str(all_val_losses.mean()), c = \"r\", linestyle = \"--\")\nplt.axhline(y = np.array(benchmark_losses).mean(), label = \"jury loss = \" + str(np.array(benchmark_losses).mean()), c = \"g\", linestyle = \"--\")\nplt.legend(fontsize = 17)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_val_losses)\nplt.axhline(y = all_val_losses.mean(), label = \"loss mean = \" + str(all_val_losses.mean()), c = \"r\", linestyle = \"--\")\nplt.axhline(y = np.array(benchmark_losses).mean(), label = \"jury loss = \" + str(np.array(benchmark_losses).mean()), c = \"g\", linestyle = \"--\")\nplt.legend(fontsize = 17)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"making preds...\")\nlist_of_preds = []\nfor batch in tqdm(test_loader):\n    x, y = batch\n    foo = jury.predict(x, plot = False, sigmoid = True)\n    list_of_preds.append(foo)\n\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub_cp = submission\nsub_cp.to_csv('./submission_cp.csv', index=None, header=True)\n\nimport csv \na = list_of_preds  \nwith open('./submission_cp.csv', \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerows(a)\n\nfinal_sub = pd.read_csv('./submission_cp.csv', header = None)\n\nfinal_sub.columns = submission.columns[1:]\nfinal_sub[\"sig_id\"] = submission[\"sig_id\"]\n\ngood_cols = np.roll(final_sub.columns.values, 1)\nfinal_sub = final_sub[good_cols]\n\nfinal_sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [col for col in final_sub.columns]\nfinal_sub.loc[test_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\nfinal_sub.to_csv('submission.csv', index=False)\nprint(\"end\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}