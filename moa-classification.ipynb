{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.preprocessing import MinMaxScaler\n\n        \n        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d   ## smoother\nfrom tqdm.notebook import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_categorical(category, df):\n    s = str(category)\n    df = pd.get_dummies(df, columns=[s])\n    return df\n\ndef encode_df(df):\n    \n    df_encoded = convert_categorical('cp_type', df)\n    df_encoded1 = convert_categorical('cp_dose', df_encoded)\n    return(df_encoded1)\n\ntrain_features_encoded = encode_df(train_features)\n\nif 'cp_dose_D1'and'cp_type_trt_cp' in train_features_encoded.columns:\n    print(\"encoded columns successfully\")\nelse:\n    print(\"Nope\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_encoded.shape, train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = train_features_encoded.columns[1:]\ntarget_columns = train_targets_scored.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat = train_features_encoded.merge(train_targets_scored, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df = train_cat.loc[:, train_cat.columns != 'sig_id']\ndf_float = dummy_df.astype(float)\nscaler = MinMaxScaler()\ndf_float_scaled = pd.DataFrame(scaler.fit_transform(df_float), columns = df_float.columns)\ndf_float_scaled['sig_id'] = train_features_encoded['sig_id']\ndf_float_scaled.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df, feature_columns, target_columns):\n        \n        self.features  = df[feature_columns].values\n        self.targets = df[target_columns].values\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float()\n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature,target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_dataset = TrainDataset(df_float_scaled, feature_columns, target_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(0.9 * len(full_dataset))  ## 90/10 split\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers = 8)\n\nval_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle = True, num_workers = 8)\n\nprint(len(train_loader), \"batches \")\nprint(len(val_loader), \" batches \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \ndef show_deltas(deltas):\n    deltas = gaussian_filter1d(deltas, sigma=3)\n    plt.plot(deltas, \"r\", label = \"Absolute error from label \")\n    plt.ylabel(\"Error\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \ndef train_step(x, y, model, optimizer, criterion):\n        optimizer.zero_grad()\n        pred = model(x.to(device))\n        y = y.float()\n        loss = criterion(pred,y)\n        loss.backward()\n        optimizer.step()\n        return loss.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc1 = nn.Linear(877, 1024)\n        self.fc2 = nn.Linear(1024, 2048)\n        self.fc3 = nn.Linear(2048, 2048)\n        self.fc4 = nn.Linear(2048, 2048)\n        self.fc5 = nn.Linear(2048, 2048)\n\n        self.fc6 = nn.Linear(2048, 1024)\n\n        self.fc7 = nn.Linear(1024, 512)\n        self.fc8 = nn.Linear(512, 206)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = self.dropout(F.relu(self.fc4(x)))\n        x = self.dropout(F.relu(self.fc5(x)))\n        x = self.dropout(F.relu(self.fc6(x)))\n\n\n        x = F.relu(self.fc7(x))\n\n        return torch.round(torch.sigmoid(self.fc8(x)))\n    \nmodel = Model()\nprint(model)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = []\nval_losses = []\nlearning_rates = []\naverage_deltas = []\nval_corr=[]\n\noptimizer = optim.Adam(model.parameters(), lr = 0.003) \n\ncriterion = nn.BCEWithLogitsLoss()\n\nscheduler = ReduceLROnPlateau(optimizer, \n                                mode='min', \n                                factor=0.5, \n                                patience=8, \n                                verbose=False, \n                                threshold=0.0001, \n                                threshold_mode='rel', \n                                cooldown=0, \n                                min_lr=0, \n                                eps=1e-08)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs=10\nfor epoch in range(num_epochs):\n    \n    clear_output(wait = True)\n    show_lr(learning_rates)\n    show_deltas(average_deltas)\n    \n    total_loss = 0\n    total_correct = 0\n    total_loss2 = 0\n    total_correct2 = 0\n    \n    print (\"epoch \", epoch+1, \" out of \", num_epochs )\n\n    \n    with torch.no_grad():\n        model.eval()\n\n        for x_val, y_val in tqdm(val_loader, desc = \"running on test set --\"):\n            yhat =model(x_val.to(device))  # pred \n            yhat = torch.round(torch.sigmoid(yhat))\n            val_loss = criterion(yhat.to(device), y_val.to(device))\n            val_losses.append(val_loss.item())  ## metrics \n            average_deltas.append(torch.mean(torch.abs(yhat.cpu()-y_val.cpu())).item())\n\n\n\n    model.train()\n    for batch in tqdm(train_loader, desc = \" Training batches : \"):\n        (x_batch, y_batch) = batch\n        loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n        losses.append(loss)\n    scheduler.step(1.)   ## lr decay caller \n    learning_rates.append(get_lr(optimizer))\n\n    clear_output(wait = True)\n\n    show_lr(learning_rates)\n    show_deltas(average_deltas)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}