{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"        \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm, tnrange\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nplt.rcParams['figure.figsize'] = 15, 7\n\nCGREEN  = '\\33[32m'\nCBLUE =  '\\033[34m'\nCRED = '\\033[1;31m'\nCEND  = '\\33[0m'\n\ndef seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'\n    \ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\n\ntarget = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns = train.columns\ntrain=train[all_columns[top_features]]\ntest = test[all_columns[top_features]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.values\ntarget = target.values\ntest = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, train,targets, noise ):\n        \n        self.features  = train\n        self.targets = targets\n        self.noise = noise\n        \n    def sizes(self):\n        print(\"features size = \", self.features.shape[1])\n        print(\"targets size = \", self.targets.shape[1])\n\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx]).float()\n        \n#         if self.noise == True:\n# #             print(\"noisy boi\")\n#             feature  = feature + torch.randn_like(feature)/150\n            \n        target = torch.tensor(self.targets[idx]).float()\n        \n        return feature, target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \ndef show_lr(learning_rates):\n    plt.plot(learning_rates, label = \"learning rate\")\n    plt.ylabel(\"Learning rate\", fontsize = 15)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef train_step(x, y, model, optimizer, criterion):\n    optimizer.zero_grad()\n    pred = model(x.to(device))\n    y = y.float()\n    loss = criterion(pred,y.to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train test split for testing "},{"metadata":{"trusted":true},"cell_type":"code","source":"y1 = torch.tensor([-4.,1.,5.,0.])\ny2 = torch.tensor([1.,0.,1.,0.])\nnn.BCEWithLogitsLoss()(y1,y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_fn(model, val_loader, criterion, device):\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            x, y = batch \n            pred = model(x.to(device))\n            loss = criterion(pred.cpu(),y).item()\n            losses.append(loss)\n    return np.array(losses).mean()\n\ndef create_weights_from_val_losses(val_losses_np):\n    w = 1/val_losses_np\n    w_norm = w/w.max()\n    return w_norm\n\n\ndef train_one_fold(model,num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1, show_plots = False, train = True, validate = True):\n    \n    losses = []\n    val_losses = []\n    learning_rates = []    \n    best_loss = 1000000\n\n    for epoch in range(num_epochs):\n\n            \n        if train == True:\n            model.train()\n            losses_temp = []\n            for batch in train_loader:\n                (x_batch, y_batch) = batch\n                loss = train_step(x_batch.to(device), y_batch.to(device), model, optimizer, criterion)\n                losses_temp.append(loss)\n            losses.append(torch.mean(torch.tensor(losses_temp)))\n            \n            if scheduler is not None:\n                scheduler.step(1.)   ## lr decay caller \n\n            learning_rates.append(get_lr(optimizer))\n            \n\n        if validate == True:\n            val_losses.append(validate_fn(model, val_loader, criterion, device))\n            \n\n        \n        if train == True:\n            print (\"epoch \", epoch+1, \" out of \", num_epochs, end = \"      >\" )\n\n            if val_losses[-1] <= best_loss:\n\n                print(CGREEN, \"Val loss decreased from:\", best_loss, \" to \", val_losses[-1], CEND, end = \"   >\")\n                best_loss = val_losses[-1]\n                name = \"./model_\" + str(fold_number)+\".pth\"\n                print(\"saving model as: \", name)\n                torch.save(model.state_dict(), name)\n\n            else: \n                print(\"showing no improvements, best loss yet:\", best_loss)\n\n        if show_plots == True:\n\n            show_lr(learning_rates)\n            plt.plot(val_losses, label = \"val\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(val_losses[4:], label = \"val after main drop\", c = \"g\")\n            plt.axhline(min(val_losses), linestyle = \"--\", c = \"r\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n\n\n            plt.plot(losses, label = \"train\")\n            plt.legend()\n            plt.grid()\n            plt.show()\n        \n    if train == True:\n        return min(val_losses), name\n    else:\n        return min(val_losses)","execution_count":472,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfull_dataset = TrainDataset(train, target, noise = False)\n\ntrain_size = int(0.9* len(full_dataset))  ## 80/20 split\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n\nval_loader = DataLoader(dataset=test_dataset, batch_size= 512, shuffle = False)\n\nprint(len(train_loader), \"batches \")\nprint(len(val_loader), \" batches \")","execution_count":347,"outputs":[{"output_type":"stream","text":"155 batches \n5  batches \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(785)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(785, 2048))\n        \n        self.batch_norm2 = nn.BatchNorm1d(2048)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n        \n        self.batch_norm3 = nn.BatchNorm1d(1048)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":454,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_filenames = []\nval_losses = []","execution_count":455,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 training "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 45 ## changes here \n\nmodel = Model()\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr = 0.004299882049752947, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                 mode='min', \n                                                 factor=0.1, ## wooo hoo\n                                                 patience=7, ## was 3 for 158 \n                                                 eps=1e-4, \n                                                 verbose=True)\n\n\ncriterion = nn.BCEWithLogitsLoss()\n\nval_loss, filename = train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 1)\n\nval_losses.append(val_loss)\nmodel_filenames.append(filename)\n    \nprint(CBLUE, \"Training complete\", CEND)","execution_count":456,"outputs":[{"output_type":"stream","text":"epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020748450234532357 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020748450234532357  to  0.019001370295882225 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019001370295882225  to  0.01833629608154297 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01833629608154297  to  0.017614253237843512 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017614253237843512  to  0.017354720458388328 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017354720458388328  to  0.01735384836792946 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.01735384836792946  to  0.017228595912456512 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017228595912456512  to  0.017123056948184966 \u001b[0m   >saving model as:  ./model_1.pth\nEpoch     9: reducing learning rate of group 0 to 4.2999e-04.\nepoch  9  out of  45      >showing no improvements, best loss yet: 0.017123056948184966\nepoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.017123056948184966  to  0.016626890748739243 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016626890748739243  to  0.016500462591648103 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.016500462591648103  to  0.016404110938310623 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016404110938310623  to  0.016352223232388496 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016352223232388496  to  0.016240422055125235 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.016240422055125235  to  0.01623081974685192 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01623081974685192  to  0.01621958538889885 \u001b[0m   >saving model as:  ./model_1.pth\nEpoch    17: reducing learning rate of group 0 to 4.2999e-05.\nepoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01621958538889885  to  0.016159237548708915 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.016159237548708915  to  0.016105377301573753 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.016105377301573753  to  0.016066128946840764 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  20  out of  45      >\u001b[32m Val loss decreased from: 0.016066128946840764  to  0.016040627844631672 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  21  out of  45      >showing no improvements, best loss yet: 0.016040627844631672\nepoch  22  out of  45      >showing no improvements, best loss yet: 0.016040627844631672\nepoch  23  out of  45      >showing no improvements, best loss yet: 0.016040627844631672\nepoch  24  out of  45      >showing no improvements, best loss yet: 0.016040627844631672\nepoch  25  out of  45      >showing no improvements, best loss yet: 0.016040627844631672\nepoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.016040627844631672  to  0.01601264551281929 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.01601264551281929  to  0.015982404910027982 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  28  out of  45      >showing no improvements, best loss yet: 0.015982404910027982\nepoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.015982404910027982  to  0.015972630679607393 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  30  out of  45      >showing no improvements, best loss yet: 0.015972630679607393\nepoch  31  out of  45      >showing no improvements, best loss yet: 0.015972630679607393\nepoch  32  out of  45      >showing no improvements, best loss yet: 0.015972630679607393\nepoch  33  out of  45      >\u001b[32m Val loss decreased from: 0.015972630679607393  to  0.015971332043409347 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  34  out of  45      >showing no improvements, best loss yet: 0.015971332043409347\nepoch  35  out of  45      >\u001b[32m Val loss decreased from: 0.015971332043409347  to  0.015936505794525147 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  36  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  37  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  38  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  39  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  40  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  41  out of  45      >showing no improvements, best loss yet: 0.015936505794525147\nepoch  42  out of  45      >\u001b[32m Val loss decreased from: 0.015936505794525147  to  0.015921371430158614 \u001b[0m   >saving model as:  ./model_1.pth\nepoch  43  out of  45      >showing no improvements, best loss yet: 0.015921371430158614\nepoch  44  out of  45      >showing no improvements, best loss yet: 0.015921371430158614\nepoch  45  out of  45      >showing no improvements, best loss yet: 0.015921371430158614\n\u001b[34m Training complete \u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_2(nn.Module):\n    def __init__(self):\n        super(Model_2, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(785)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(785, 512))\n        \n        self.batch_norm4 = nn.BatchNorm1d(512)\n        self.dropout4 = nn.Dropout(0.5)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(512, 206))\n    \n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x","execution_count":457,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 training"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 45 ## changes here \n\nmodel = Model_2()\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr = 4e-3, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                 mode='min', \n                                                 factor=0.1, ## wooo hoo\n                                                 patience=7, ## was 3 for 158 \n                                                 eps=1e-4, \n                                                 verbose=True)\n\n\ncriterion = nn.BCEWithLogitsLoss()\n\nval_loss, filename = train_one_fold(model, num_epochs , train_loader,val_loader, optimizer, scheduler, criterion, fold_number = 100)\n\n\nval_losses.append(val_loss)\nmodel_filenames.append(filename)\n    \n    \nprint(CBLUE, \"Training complete\", CEND)","execution_count":458,"outputs":[{"output_type":"stream","text":"epoch  1  out of  45      >\u001b[32m Val loss decreased from: 1000000  to  0.020921602472662927 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  2  out of  45      >\u001b[32m Val loss decreased from: 0.020921602472662927  to  0.019058291241526602 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  3  out of  45      >\u001b[32m Val loss decreased from: 0.019058291241526602  to  0.01854160465300083 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  4  out of  45      >\u001b[32m Val loss decreased from: 0.01854160465300083  to  0.017702825367450714 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  5  out of  45      >\u001b[32m Val loss decreased from: 0.017702825367450714  to  0.017415288463234903 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  6  out of  45      >\u001b[32m Val loss decreased from: 0.017415288463234903  to  0.017212071269750596 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  7  out of  45      >\u001b[32m Val loss decreased from: 0.017212071269750596  to  0.017006976157426835 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  8  out of  45      >\u001b[32m Val loss decreased from: 0.017006976157426835  to  0.016899461299180983 \u001b[0m   >saving model as:  ./model_100.pth\nEpoch     9: reducing learning rate of group 0 to 4.0000e-04.\nepoch  9  out of  45      >\u001b[32m Val loss decreased from: 0.016899461299180983  to  0.016884162649512292 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  10  out of  45      >\u001b[32m Val loss decreased from: 0.016884162649512292  to  0.016523374244570732 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  11  out of  45      >\u001b[32m Val loss decreased from: 0.016523374244570732  to  0.01641211248934269 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  12  out of  45      >\u001b[32m Val loss decreased from: 0.01641211248934269  to  0.016340957954525948 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  13  out of  45      >\u001b[32m Val loss decreased from: 0.016340957954525948  to  0.016297558695077895 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  14  out of  45      >\u001b[32m Val loss decreased from: 0.016297558695077895  to  0.01627260223031044 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  15  out of  45      >\u001b[32m Val loss decreased from: 0.01627260223031044  to  0.01622239239513874 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  16  out of  45      >\u001b[32m Val loss decreased from: 0.01622239239513874  to  0.01620594710111618 \u001b[0m   >saving model as:  ./model_100.pth\nEpoch    17: reducing learning rate of group 0 to 4.0000e-05.\nepoch  17  out of  45      >\u001b[32m Val loss decreased from: 0.01620594710111618  to  0.016198165714740753 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  18  out of  45      >\u001b[32m Val loss decreased from: 0.016198165714740753  to  0.01617981996387243 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  19  out of  45      >\u001b[32m Val loss decreased from: 0.01617981996387243  to  0.016134187020361422 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  20  out of  45      >showing no improvements, best loss yet: 0.016134187020361422\nepoch  21  out of  45      >showing no improvements, best loss yet: 0.016134187020361422\nepoch  22  out of  45      >\u001b[32m Val loss decreased from: 0.016134187020361422  to  0.016124280728399755 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  23  out of  45      >\u001b[32m Val loss decreased from: 0.016124280728399755  to  0.016116378456354143 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  24  out of  45      >\u001b[32m Val loss decreased from: 0.016116378456354143  to  0.016109581291675567 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  25  out of  45      >showing no improvements, best loss yet: 0.016109581291675567\nepoch  26  out of  45      >\u001b[32m Val loss decreased from: 0.016109581291675567  to  0.01609619464725256 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  27  out of  45      >\u001b[32m Val loss decreased from: 0.01609619464725256  to  0.016089870780706405 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  28  out of  45      >showing no improvements, best loss yet: 0.016089870780706405\nepoch  29  out of  45      >\u001b[32m Val loss decreased from: 0.016089870780706405  to  0.016080285981297494 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  30  out of  45      >showing no improvements, best loss yet: 0.016080285981297494\nepoch  31  out of  45      >showing no improvements, best loss yet: 0.016080285981297494\nepoch  32  out of  45      >showing no improvements, best loss yet: 0.016080285981297494\nepoch  33  out of  45      >showing no improvements, best loss yet: 0.016080285981297494\nepoch  34  out of  45      >showing no improvements, best loss yet: 0.016080285981297494\nepoch  35  out of  45      >\u001b[32m Val loss decreased from: 0.016080285981297494  to  0.016073044575750827 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  36  out of  45      >showing no improvements, best loss yet: 0.016073044575750827\nepoch  37  out of  45      >\u001b[32m Val loss decreased from: 0.016073044575750827  to  0.016069399751722814 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  38  out of  45      >\u001b[32m Val loss decreased from: 0.016069399751722814  to  0.01604740172624588 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  39  out of  45      >showing no improvements, best loss yet: 0.01604740172624588\nepoch  40  out of  45      >showing no improvements, best loss yet: 0.01604740172624588\nepoch  41  out of  45      >showing no improvements, best loss yet: 0.01604740172624588\nepoch  42  out of  45      >showing no improvements, best loss yet: 0.01604740172624588\nepoch  43  out of  45      >showing no improvements, best loss yet: 0.01604740172624588\nepoch  44  out of  45      >\u001b[32m Val loss decreased from: 0.01604740172624588  to  0.01604318991303444 \u001b[0m   >saving model as:  ./model_100.pth\nepoch  45  out of  45      >showing no improvements, best loss yet: 0.01604318991303444\n\u001b[34m Training complete \u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class blend(nn.Module):\n    def __init__(self,weights, model_list ,model_filenames, device = device):\n        super(blend, self).__init__()\n        print(\"loading models...\")\n        self.model_filenames = model_filenames\n        self.model_list = model_list\n        self.weights = weights \n\n        for i in range(len(self.model_filenames)):\n            self.model_list[i].load_state_dict(torch.load(self.model_filenames[i]))\n            self.model_list[i].to(device)\n            self.model_list[i].eval()\n            \n        print(\"done loading from\", self.model_filenames)\n\n    def forward(self, x):\n        \n        x_list = [self.model_list[i](x).detach().cpu() for i in range(len(self.model_list))]\n        \n        final_pred = torch.zeros_like(x_list[0])\n        for i in range(len(x_list)):\n            final_pred += x_list[i] * self.weights[i]\n            \n        final_pred = final_pred/self.weights.sum()\n        \n        return final_pred.cpu()","execution_count":459,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = create_weights_from_val_losses(np.array(val_losses))\nbb = blend(\n    weights = weights, \n    model_list = [Model(), Model_2()],\n    model_filenames = model_filenames\n)","execution_count":475,"outputs":[{"output_type":"stream","text":"loading models...\ndone loading from ['./model_1.pth', './model_100.pth']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_filenames","execution_count":461,"outputs":[{"output_type":"execute_result","execution_count":461,"data":{"text/plain":"['./model_1.pth', './model_100.pth']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [Model(), Model_2()]\ntest_val_losses = {}\n\nfor i in range(len(model_filenames)):\n    all_models[i].load_state_dict(torch.load(model_filenames[i]))\n    all_models[i].to(device)\n    all_models[i].eval()\n    \nall_val_losses = []\n\nwith torch.no_grad():\n    for i in range(len(all_models)):\n        val_loss = validate_fn(all_models[i], val_loader, criterion, device)\n        test_val_losses[\"model_\" + str(i+1)] = val_loss\n    print(\"done validating\")\n\ntest_val_losses","execution_count":473,"outputs":[{"output_type":"stream","text":"done validating\n","name":"stdout"},{"output_type":"execute_result","execution_count":473,"data":{"text/plain":"{'model_1': 0.015921371430158614, 'model_2': 0.01604318991303444}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss = validate_fn(bb, val_loader, criterion, device)\ntest_val_losses[\"model_blend\"] = val_loss\ntest_val_losses","execution_count":476,"outputs":[{"output_type":"execute_result","execution_count":476,"data":{"text/plain":"{'model_1': 0.015921371430158614,\n 'model_2': 0.01604318991303444,\n 'model_blend': 0.015878767147660254}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 15})\n\nnames = test_val_losses.keys()\nvals = test_val_losses.values()\n\nplt.bar(names, vals)\nplt.ylim(min(vals)- .0002, max(vals) + 0.0002)\nplt.ylabel(\"loss on validation set\", fontsize = 18)\nplt.xlabel(\"models ->\", fontsize = 15)\nplt.axhline(y = min(vals), linestyle = \"--\", c = \"r\", linewidth = 5)\nplt.show()","execution_count":477,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x504 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(vals)","execution_count":478,"outputs":[{"output_type":"execute_result","execution_count":478,"data":{"text/plain":"0.015878767147660254"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}